{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195a2ae5-1bd3-4e04-bbe2-04760d4f025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e8c871-540b-4e82-aff3-fc5bc340f64e",
   "metadata": {},
   "source": [
    "# 1.Overview\n",
    "\n",
    "In this notebook, we want to provide an tutorial how to train a wdl model using HugeCTR High-level python API. We will use original Criteo dataset as training data\n",
    "\n",
    "1. Overview\n",
    "2. Dataset Preprocessing\n",
    "3. WDL Model Training\n",
    "4. Save the Model Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f37d076-b23e-4bb6-876b-180699ab5ea2",
   "metadata": {},
   "source": [
    "# 2. Dataset Preprocessing\n",
    "## 2.1 Generate training and validation data folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07eb0150-c1ec-425f-8768-d307030c57c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some data folder to store the original and preprocessed data\n",
    "# Standard Libraries\n",
    "import os\n",
    "from time import time\n",
    "import re\n",
    "import shutil\n",
    "import glob\n",
    "import warnings\n",
    "BASE_DIR = \"/wdl_train\"\n",
    "train_path  = os.path.join(BASE_DIR, \"train\")\n",
    "val_path = os.path.join(BASE_DIR, \"val\")\n",
    "CUDA_VISIBLE_DEVICES = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
    "n_workers = len(CUDA_VISIBLE_DEVICES.split(\",\"))\n",
    "frac_size = 0.15\n",
    "allow_multi_gpu = False\n",
    "use_rmm_pool = False\n",
    "max_day = None  # (Optional) -- Limit the dataset to day 0-max_day for debugging\n",
    "\n",
    "if os.path.isdir(train_path):\n",
    "    shutil.rmtree(train_path)\n",
    "os.makedirs(train_path)\n",
    "\n",
    "if os.path.isdir(val_path):\n",
    "    shutil.rmtree(val_path)\n",
    "os.makedirs(val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea33738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n"
     ]
    }
   ],
   "source": [
    "ls -l $train_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3943a3c-7ae9-46da-98ec-7c3a0d479feb",
   "metadata": {},
   "source": [
    "## 2.2 Download the Original Criteo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0213bcaf-6c13-4cdd-ba2c-01722f334579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "wget is already the newest version (1.20.3-1ubuntu1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac0abe2-032a-4f1b-b32b-99738d513cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-05 03:27:50--  http://azuremlsampleexperiments.blob.core.windows.net/criteo/day_0.gz\n",
      "Resolving azuremlsampleexperiments.blob.core.windows.net (azuremlsampleexperiments.blob.core.windows.net)... 20.60.140.36\n",
      "Connecting to azuremlsampleexperiments.blob.core.windows.net (azuremlsampleexperiments.blob.core.windows.net)|20.60.140.36|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16309554343 (15G) [application/octet-stream]\n",
      "Saving to: ‘wdl_train/train/day_0.gz’\n",
      "\n",
      "day_0.gz               100%[===================================================>]   15G  --.-KB/s    in  6m 12s \n",
      "2021-07-05 03:34:04 (79.2 MB/s) - 'day_0.gz' saved [16309554343/16309554343]"
     ]
    }
   ],
   "source": [
    "!wget -P $train_path http://azuremlsampleexperiments.blob.core.windows.net/criteo/day_0.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72fff32e-6ad6-43c2-aecd-500c59d7471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the split data set to training and validation \n",
    "#!gzip -d -c $train_path/day_0.gz > day_0\n",
    "!head -n 45840617 day_0 > $train_path/train.txt\n",
    "!tail -n 2000000 day_0 > $val_path/test.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33165dda-ca3c-4721-a125-24b9745d3699",
   "metadata": {},
   "source": [
    "## 2.3 Preprocessing by NVTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b7644f3-e4b6-40a0-bd35-cd056618481d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /wdl_train/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /wdl_train/preprocess.py\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import glob\n",
    "import time\n",
    "from cudf.io.parquet import ParquetWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures as cf\n",
    "from concurrent.futures import as_completed\n",
    "import shutil\n",
    "\n",
    "import dask_cudf\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "from dask.utils import parse_bytes\n",
    "from dask.delayed import delayed\n",
    "\n",
    "import cudf\n",
    "import rmm\n",
    "import nvtabular as nvt\n",
    "from nvtabular.io import Shuffle\n",
    "from nvtabular.utils import device_mem_size\n",
    "from nvtabular.ops import Categorify, Clip, FillMissing, HashBucket, LambdaOp, Normalize, Rename, Operator, get_embedding_sizes\n",
    "#%load_ext memory_profiler\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(message)s')\n",
    "logging.root.setLevel(logging.NOTSET)\n",
    "logging.getLogger('numba').setLevel(logging.WARNING)\n",
    "logging.getLogger('asyncio').setLevel(logging.WARNING)\n",
    "\n",
    "# define dataset schema\n",
    "CATEGORICAL_COLUMNS=[\"C\" + str(x) for x in range(1, 27)]\n",
    "CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "LABEL_COLUMNS = ['label']\n",
    "COLUMNS =  LABEL_COLUMNS + CONTINUOUS_COLUMNS +  CATEGORICAL_COLUMNS\n",
    "#/samples/criteo mode doesn't have dense features\n",
    "criteo_COLUMN=LABEL_COLUMNS +  CATEGORICAL_COLUMNS\n",
    "#For new feature cross columns\n",
    "CROSS_COLUMNS = []\n",
    "\n",
    "\n",
    "NUM_INTEGER_COLUMNS = 13\n",
    "NUM_CATEGORICAL_COLUMNS = 26\n",
    "NUM_TOTAL_COLUMNS = 1 + NUM_INTEGER_COLUMNS + NUM_CATEGORICAL_COLUMNS\n",
    "\n",
    "\n",
    "# Initialize RMM pool on ALL workers\n",
    "def setup_rmm_pool(client, pool_size):\n",
    "    client.run(rmm.reinitialize, pool_allocator=True, initial_pool_size=pool_size)\n",
    "    return None\n",
    "\n",
    "#compute the partition size with GB\n",
    "def bytesto(bytes, to, bsize=1024):\n",
    "    a = {'k' : 1, 'm': 2, 'g' : 3, 't' : 4, 'p' : 5, 'e' : 6 }\n",
    "    r = float(bytes)\n",
    "    return bytes / (bsize ** a[to])\n",
    "\n",
    "class FeatureCross(Operator):\n",
    "    def __init__(self, dependency):\n",
    "        self.dependency = dependency\n",
    "\n",
    "    def transform(self, columns, gdf):\n",
    "        new_df = type(gdf)()\n",
    "        for col in columns.names:\n",
    "            new_df[col] = gdf[col] + gdf[self.dependency]\n",
    "        return new_df\n",
    "\n",
    "    def dependencies(self):\n",
    "        return [self.dependency]\n",
    "\n",
    "#process the data with NVTabular\n",
    "def process_NVT(args):\n",
    "\n",
    "    if args.feature_cross_list:\n",
    "        feature_pairs = [pair.split(\"_\") for pair in args.feature_cross_list.split(\",\")]\n",
    "        for pair in feature_pairs:\n",
    "            CROSS_COLUMNS.append(pair[0]+'_'+pair[1])\n",
    "\n",
    "\n",
    "    logging.info('NVTabular processing')\n",
    "    train_output = os.path.join(args.out_path, \"train\")\n",
    "    print(\"Training output data: \"+train_output)\n",
    "    val_output = os.path.join(args.out_path, \"val\")\n",
    "    print(\"Validation output data: \"+val_output)\n",
    "    train_input = os.path.join(args.data_path, \"train/train.txt\")\n",
    "    print(\"Training dataset: \"+train_input)\n",
    "    val_input = os.path.join(args.data_path, \"val/test.txt\")\n",
    "    PREPROCESS_DIR_temp_train = os.path.join(args.out_path, 'train/temp-parquet-after-conversion')  \n",
    "    PREPROCESS_DIR_temp_val = os.path.join(args.out_path, \"val/temp-parquet-after-conversion\")\n",
    "    if not os.path.exists(PREPROCESS_DIR_temp_train):\n",
    "        os.makedirs(PREPROCESS_DIR_temp_train)\n",
    "    \n",
    "    if not os.path.exists(PREPROCESS_DIR_temp_val):\n",
    "        os.makedirs(PREPROCESS_DIR_temp_val)\n",
    "    \n",
    "    PREPROCESS_DIR_temp = [PREPROCESS_DIR_temp_train, PREPROCESS_DIR_temp_val]\n",
    "    \n",
    "    \n",
    "\n",
    "    # Make sure we have a clean parquet space for cudf conversion\n",
    "    for one_path in PREPROCESS_DIR_temp:\n",
    "        if os.path.exists(one_path):\n",
    "            shutil.rmtree(one_path)\n",
    "        os.mkdir(one_path)\n",
    "\n",
    "\n",
    "    ## Get Dask Client\n",
    "\n",
    "    # Deploy a Single-Machine Multi-GPU Cluster\n",
    "    device_size = device_mem_size(kind=\"total\")\n",
    "    cluster = None\n",
    "    if args.protocol == \"ucx\":\n",
    "        UCX_TLS = os.environ.get(\"UCX_TLS\", \"tcp,cuda_copy,cuda_ipc,sockcm\")\n",
    "        os.environ[\"UCX_TLS\"] = UCX_TLS\n",
    "        cluster = LocalCUDACluster(\n",
    "            protocol = args.protocol,\n",
    "            CUDA_VISIBLE_DEVICES = args.devices,\n",
    "            n_workers = len(args.devices.split(\",\")),\n",
    "            enable_nvlink=True,\n",
    "            device_memory_limit = int(device_size * args.device_limit_frac),\n",
    "            dashboard_address=\":\" + args.dashboard_port\n",
    "        )\n",
    "    else:\n",
    "        cluster = LocalCUDACluster(\n",
    "            protocol = args.protocol,\n",
    "            n_workers = len(args.devices.split(\",\")),\n",
    "            CUDA_VISIBLE_DEVICES = args.devices,\n",
    "            device_memory_limit = int(device_size * args.device_limit_frac),\n",
    "            dashboard_address=\":\" + args.dashboard_port\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # Create the distributed client\n",
    "    client = Client(cluster)\n",
    "    if args.device_pool_frac > 0.01:\n",
    "        setup_rmm_pool(client, int(args.device_pool_frac*device_size))\n",
    "\n",
    "\n",
    "    #calculate the total processing time\n",
    "    runtime = time.time()\n",
    "\n",
    "    #test dataset without the label feature\n",
    "    if args.dataset_type == 'test':\n",
    "        global LABEL_COLUMNS\n",
    "        LABEL_COLUMNS = []\n",
    "\n",
    "    ##-----------------------------------##\n",
    "    # Dask rapids converts txt to parquet\n",
    "    # Dask cudf dataframe = ddf\n",
    "\n",
    "    ## train/valid txt to parquet\n",
    "    train_valid_paths = [(train_input,PREPROCESS_DIR_temp_train),(val_input,PREPROCESS_DIR_temp_val)]\n",
    "\n",
    "    for input, temp_output in train_valid_paths:\n",
    "\n",
    "        ddf = dask_cudf.read_csv(input,sep='\\t',names=LABEL_COLUMNS + CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS)\n",
    "\n",
    "        ## Convert label col to FP32\n",
    "        if args.parquet_format and args.dataset_type == 'train':\n",
    "            ddf[\"label\"] = ddf['label'].astype('float32')\n",
    "\n",
    "        # Save it as parquet format for better memory usage\n",
    "        ddf.to_parquet(temp_output,header=True)\n",
    "        ##-----------------------------------##\n",
    "\n",
    "    COLUMNS =  LABEL_COLUMNS + CONTINUOUS_COLUMNS + CROSS_COLUMNS + CATEGORICAL_COLUMNS\n",
    "    train_paths = glob.glob(os.path.join(PREPROCESS_DIR_temp_train, \"*.parquet\"))\n",
    "    valid_paths = glob.glob(os.path.join(PREPROCESS_DIR_temp_val, \"*.parquet\"))\n",
    "\n",
    "    categorify_op = Categorify(freq_threshold=args.freq_limit)\n",
    "    cat_features = CATEGORICAL_COLUMNS >> categorify_op\n",
    "    cont_features = CONTINUOUS_COLUMNS >> FillMissing() >> Clip(min_value=0) >> Normalize()\n",
    "    cross_cat_op = Categorify(freq_threshold=args.freq_limit)\n",
    "\n",
    "    features = LABEL_COLUMNS\n",
    "    \n",
    "    if args.criteo_mode == 0:\n",
    "        features += cont_features\n",
    "        if args.feature_cross_list:\n",
    "            feature_pairs = [pair.split(\"_\") for pair in args.feature_cross_list.split(\",\")]\n",
    "            for pair in feature_pairs:\n",
    "                col0 = pair[0]\n",
    "                col1 = pair[1]\n",
    "                features += col0 >> FeatureCross(col1)  >> Rename(postfix=\"_\"+col1) >> cross_cat_op\n",
    "            \n",
    "    features += cat_features\n",
    "\n",
    "    workflow = nvt.Workflow(features, client=client)\n",
    "\n",
    "    logging.info(\"Preprocessing\")\n",
    "\n",
    "    output_format = 'hugectr'\n",
    "    if args.parquet_format:\n",
    "        output_format = 'parquet'\n",
    "\n",
    "    # just for /samples/criteo model\n",
    "    train_ds_iterator = nvt.Dataset(train_paths, engine='parquet', part_size=int(args.part_mem_frac * device_size))\n",
    "    valid_ds_iterator = nvt.Dataset(valid_paths, engine='parquet', part_size=int(args.part_mem_frac * device_size))\n",
    "\n",
    "    shuffle = None\n",
    "    if args.shuffle == \"PER_WORKER\":\n",
    "        shuffle = nvt.io.Shuffle.PER_WORKER\n",
    "    elif args.shuffle == \"PER_PARTITION\":\n",
    "        shuffle = nvt.io.Shuffle.PER_PARTITION\n",
    "\n",
    "    logging.info('Train Datasets Preprocessing.....')\n",
    "\n",
    "    dict_dtypes = {}\n",
    "    for col in CATEGORICAL_COLUMNS:\n",
    "        dict_dtypes[col] = np.int64\n",
    "    if not args.criteo_mode:\n",
    "        for col in CONTINUOUS_COLUMNS:\n",
    "            dict_dtypes[col] = np.float32\n",
    "    for col in CROSS_COLUMNS:\n",
    "        dict_dtypes[col] = np.int64\n",
    "    for col in LABEL_COLUMNS:\n",
    "        dict_dtypes[col] = np.float32\n",
    "    \n",
    "    conts = CONTINUOUS_COLUMNS if not args.criteo_mode else []\n",
    "    \n",
    "    workflow.fit(train_ds_iterator)\n",
    "    \n",
    "    if output_format == 'hugectr':\n",
    "        workflow.transform(train_ds_iterator).to_hugectr(\n",
    "                cats=CATEGORICAL_COLUMNS + CROSS_COLUMNS,\n",
    "                conts=conts,\n",
    "                labels=LABEL_COLUMNS,\n",
    "                output_path=train_output,\n",
    "                shuffle=shuffle,\n",
    "                out_files_per_proc=args.out_files_per_proc,\n",
    "                num_threads=args.num_io_threads)\n",
    "    else:\n",
    "        workflow.transform(train_ds_iterator).to_parquet(\n",
    "                output_path=train_output,\n",
    "                dtypes=dict_dtypes,\n",
    "                cats=CATEGORICAL_COLUMNS + CROSS_COLUMNS,\n",
    "                conts=conts,\n",
    "                labels=LABEL_COLUMNS,\n",
    "                shuffle=shuffle,\n",
    "                out_files_per_proc=args.out_files_per_proc,\n",
    "                num_threads=args.num_io_threads)\n",
    "        \n",
    "        \n",
    "        \n",
    "    ###Getting slot size###    \n",
    "    #--------------------##\n",
    "    embeddings_dict_cat = categorify_op.get_embedding_sizes(CATEGORICAL_COLUMNS)\n",
    "    embeddings_dict_cross = cross_cat_op.get_embedding_sizes(CROSS_COLUMNS)\n",
    "    embeddings = [embeddings_dict_cat[c][0] for c in CATEGORICAL_COLUMNS] + [embeddings_dict_cross[c][0] for c in CROSS_COLUMNS]\n",
    "    \n",
    "    print(embeddings)\n",
    "    ##--------------------##\n",
    "\n",
    "    logging.info('Valid Datasets Preprocessing.....')\n",
    "\n",
    "    if output_format == 'hugectr':\n",
    "        workflow.transform(valid_ds_iterator).to_hugectr(\n",
    "                cats=CATEGORICAL_COLUMNS + CROSS_COLUMNS,\n",
    "                conts=conts,\n",
    "                labels=LABEL_COLUMNS,\n",
    "                output_path=val_output,\n",
    "                shuffle=shuffle,\n",
    "                out_files_per_proc=args.out_files_per_proc,\n",
    "                num_threads=args.num_io_threads)\n",
    "    else:\n",
    "        workflow.transform(valid_ds_iterator).to_parquet(\n",
    "                output_path=val_output,\n",
    "                dtypes=dict_dtypes,\n",
    "                cats=CATEGORICAL_COLUMNS + CROSS_COLUMNS,\n",
    "                conts=conts,\n",
    "                labels=LABEL_COLUMNS,\n",
    "                shuffle=shuffle,\n",
    "                out_files_per_proc=args.out_files_per_proc,\n",
    "                num_threads=args.num_io_threads)\n",
    "\n",
    "    embeddings_dict_cat = categorify_op.get_embedding_sizes(CATEGORICAL_COLUMNS)\n",
    "    embeddings_dict_cross = cross_cat_op.get_embedding_sizes(CROSS_COLUMNS)\n",
    "    embeddings = [embeddings_dict_cat[c][0] for c in CATEGORICAL_COLUMNS] + [embeddings_dict_cross[c][0] for c in CROSS_COLUMNS]\n",
    "    \n",
    "    print(embeddings)\n",
    "    ##--------------------##\n",
    "\n",
    "    ## Shutdown clusters\n",
    "    client.close()\n",
    "    logging.info('NVTabular processing done')\n",
    "\n",
    "    runtime = time.time() - runtime\n",
    "\n",
    "    print(\"\\nDask-NVTabular Criteo Preprocessing\")\n",
    "    print(\"--------------------------------------\")\n",
    "    print(f\"data_path          | {args.data_path}\")\n",
    "    print(f\"output_path        | {args.out_path}\")\n",
    "    print(f\"partition size     | {'%.2f GB'%bytesto(int(args.part_mem_frac * device_size),'g')}\")\n",
    "    print(f\"protocol           | {args.protocol}\")\n",
    "    print(f\"device(s)          | {args.devices}\")\n",
    "    print(f\"rmm-pool-frac      | {(args.device_pool_frac)}\")\n",
    "    print(f\"out-files-per-proc | {args.out_files_per_proc}\")\n",
    "    print(f\"num_io_threads     | {args.num_io_threads}\")\n",
    "    print(f\"shuffle            | {args.shuffle}\")\n",
    "    print(\"======================================\")\n",
    "    print(f\"Runtime[s]         | {runtime}\")\n",
    "    print(\"======================================\\n\")\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=(\"Multi-GPU Criteo Preprocessing\"))\n",
    "\n",
    "    #\n",
    "    # System Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\"--data_path\", type=str, help=\"Input dataset path (Required)\")\n",
    "    parser.add_argument(\"--out_path\", type=str, help=\"Directory path to write output (Required)\")\n",
    "    parser.add_argument(\n",
    "        \"-d\",\n",
    "        \"--devices\",\n",
    "        default=os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\"),\n",
    "        type=str,\n",
    "        help='Comma-separated list of visible devices (e.g. \"0,1,2,3\"). '\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-p\",\n",
    "        \"--protocol\",\n",
    "        choices=[\"tcp\", \"ucx\"],\n",
    "        default=\"tcp\",\n",
    "        type=str,\n",
    "        help=\"Communication protocol to use (Default 'tcp')\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device_limit_frac\",\n",
    "        default=0.5,\n",
    "        type=float,\n",
    "        help=\"Worker device-memory limit as a fraction of GPU capacity (Default 0.8). \"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device_pool_frac\",\n",
    "        default=0.9,\n",
    "        type=float,\n",
    "        help=\"RMM pool size for each worker  as a fraction of GPU capacity (Default 0.9). \"\n",
    "        \"The RMM pool frac is the same for all GPUs, make sure each one has enough memory size\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_io_threads\",\n",
    "        default=0,\n",
    "        type=int,\n",
    "        help=\"Number of threads to use when writing output data (Default 0). \"\n",
    "        \"If 0 is specified, multi-threading will not be used for IO.\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Data-Decomposition Parameters\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--part_mem_frac\",\n",
    "        default=0.125,\n",
    "        type=float,\n",
    "        help=\"Maximum size desired for dataset partitions as a fraction \"\n",
    "        \"of GPU capacity (Default 0.125)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out_files_per_proc\",\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=\"Number of output files to write on each worker (Default 1)\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Preprocessing Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"-f\",\n",
    "        \"--freq_limit\",\n",
    "        default=0,\n",
    "        type=int,\n",
    "        help=\"Frequency limit for categorical encoding (Default 0)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-s\",\n",
    "        \"--shuffle\",\n",
    "        choices=[\"PER_WORKER\", \"PER_PARTITION\", \"NONE\"],\n",
    "        default=\"PER_PARTITION\",\n",
    "        help=\"Shuffle algorithm to use when writing output data to disk (Default PER_PARTITION)\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--feature_cross_list\", default=None, type=str, help=\"List of feature crossing cols (e.g. C1_C2, C3_C4)\"\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Diagnostics Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--profile\",\n",
    "        metavar=\"PATH\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"Specify a file path to export a Dask profile report (E.g. dask-report.html).\"\n",
    "        \"If this option is excluded from the command, not profile will be exported\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dashboard_port\",\n",
    "        default=\"8787\",\n",
    "        type=str,\n",
    "        help=\"Specify the desired port of Dask's diagnostics-dashboard (Default `3787`). \"\n",
    "        \"The dashboard will be hosted at http://<IP>:<PORT>/status\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Format\n",
    "    #\n",
    "\n",
    "    parser.add_argument('--criteo_mode', type=int, default=0)\n",
    "    parser.add_argument('--parquet_format', type=int, default=1)\n",
    "    parser.add_argument('--dataset_type', type=str, default='train')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.n_workers = len(args.devices.split(\",\"))\n",
    "    return args\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args = parse_args()\n",
    "\n",
    "    process_NVT(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97aae996-9677-40ae-a976-6edb51fcc61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d57560b-c6cf-418e-b45b-3b6722d29a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-29 05:26:15,779 NVTabular processing\n",
      "Training output data: /wdl_train/train\n",
      "Validation output data: /wdl_train/val\n",
      "Training dataset: /wdl_train/train/train.txt\n",
      "distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2021-11-29 05:26:33,989 Preprocessing\n",
      "2021-11-29 05:26:34,194 Train Datasets Preprocessing.....\n",
      "/usr/local/lib/python3.8/dist-packages/numba/cuda/compiler.py:865: NumbaPerformanceWarning: Grid size (1) < 2 * SM count (160) will likely result in GPU under utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "[249058, 19561, 14212, 6890, 18592, 4, 6356, 1254, 52, 226170, 80508, 72308, 11, 2169, 7597, 61, 4, 923, 15, 249619, 168974, 243480, 68212, 9169, 75, 34, 278018, 415262]\n",
      "2021-11-29 05:27:42,216 Valid Datasets Preprocessing.....\n",
      "[249058, 19561, 14212, 6890, 18592, 4, 6356, 1254, 52, 226170, 80508, 72308, 11, 2169, 7597, 61, 4, 923, 15, 249619, 168974, 243480, 68212, 9169, 75, 34, 278018, 415262]\n",
      "2021-11-29 05:27:44,134 NVTabular processing done\n",
      "\n",
      "Dask-NVTabular Criteo Preprocessing\n",
      "--------------------------------------\n",
      "data_path          | /wdl_train/\n",
      "output_path        | /wdl_train/\n",
      "partition size     | 1.97 GB\n",
      "protocol           | tcp\n",
      "device(s)          | 0\n",
      "rmm-pool-frac      | 0.5\n",
      "out-files-per-proc | 1\n",
      "num_io_threads     | 2\n",
      "shuffle            | PER_PARTITION\n",
      "======================================\n",
      "Runtime[s]         | 85.47145938873291\n",
      "======================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 /wdl_train/preprocess.py --data_path /wdl_train/ --out_path /wdl_train/ --freq_limit 6 --feature_cross_list C1_C2,C3_C4 --device_pool_frac 0.5  --devices \"0\" --num_io_threads 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20924837",
   "metadata": {},
   "source": [
    "### 2.4 Checke the preprocessed training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc6e4663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 14542626\r\n",
      "-rw-r--r-- 1 root root          34 Nov 29 05:27 _file_list.txt\r\n",
      "-rw-r--r-- 1 root root     8554464 Nov 29 05:27 _hugectr.keyset\r\n",
      "-rw-r--r-- 1 root root      471772 Nov 29 05:27 _metadata\r\n",
      "-rw-r--r-- 1 root root        1510 Nov 29 05:27 _metadata.json\r\n",
      "-rw-r--r-- 1 root root  3332773998 Nov 29 05:27 part_0.parquet\r\n",
      "-rw-r--r-- 1 root root       21459 Nov 29 05:27 schema.pbtxt\r\n",
      "drwxr-xr-x 2 root root        4096 Nov 29 05:26 temp-parquet-after-conversion\r\n",
      "-rw-r--r-- 1 root root 11549710546 Nov 29 03:50 train.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ll /wdl_train/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0455ab5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>...</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "      <th>C22</th>\n",
       "      <th>C23</th>\n",
       "      <th>C24</th>\n",
       "      <th>C25</th>\n",
       "      <th>C26</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.031058</td>\n",
       "      <td>0.350474</td>\n",
       "      <td>0.264160</td>\n",
       "      <td>-0.091675</td>\n",
       "      <td>0.023207</td>\n",
       "      <td>0.068484</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.224423</td>\n",
       "      <td>-0.760031</td>\n",
       "      <td>1.386036</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.059432</td>\n",
       "      <td>-0.217361</td>\n",
       "      <td>-0.594327</td>\n",
       "      <td>-0.157301</td>\n",
       "      <td>-0.147269</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.279201</td>\n",
       "      <td>-0.760031</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>210</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         I1        I2        I3        I4        I5        I6        I7  \\\n",
       "0 -0.031058  0.350474  0.264160 -0.091675  0.023207  0.068484 -0.064249   \n",
       "1 -0.059432 -0.217361 -0.594327 -0.157301 -0.147269 -0.206385 -0.064249   \n",
       "\n",
       "         I8        I9       I10  ...  C18  C19  C20  C21  C22  C23  C24  C25  \\\n",
       "0 -0.224423 -0.760031  1.386036  ...    1    2    1    1    1    1   25    1   \n",
       "1 -0.279201 -0.760031 -0.470383  ...    2    2    1    1    1    1  210    1   \n",
       "\n",
       "   C26  label  \n",
       "0    2    0.0  \n",
       "1    2    0.0  \n",
       "\n",
       "[2 rows x 42 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"/wdl_train/train/part_0.parquet\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c7026-017e-46a2-ae74-bc820e60cc6e",
   "metadata": {},
   "source": [
    "## 3. WDL Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d3d01115-b0eb-4b7f-82d2-1d4a457d3170",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile './model.py'\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 4000,\n",
    "                              batchsize_eval = 2720,\n",
    "                              batchsize = 2720,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[2]],\n",
    "                              repeat_dataset = True,\n",
    "                              i64_input_key = True)\n",
    "\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Parquet,\n",
    "                                  source = [\"./train/_file_list.txt\"],\n",
    "                                  eval_source = \"./val/_file_list.txt\",\n",
    "                                  check_type = hugectr.Check_t.Non,\n",
    "                                  slot_size_array = [249058, 19561, 14212, 6890, 18592, 4, 6356, 1254, 52, 226170, 80508, 72308, 11, 2169, 7597, 61, 4, 923, 15, 249619, 168974, 243480, 68212, 9169, 75, 34, 278018, 415262])\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam,\n",
    "                                    update_type = hugectr.Update_t.Global,\n",
    "                                    beta1 = 0.9,\n",
    "                                    beta2 = 0.999,\n",
    "                                    epsilon = 0.0000001)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"wide_data\", 1, True, 2),\n",
    "                        hugectr.DataReaderSparseParam(\"deep_data\", 2, False, 26)]))\n",
    "\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 8,\n",
    "                            embedding_vec_size = 1,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding2\",\n",
    "                            bottom_name = \"wide_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 135,\n",
    "                            embedding_vec_size = 16,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"deep_data\",\n",
    "                            optimizer = optimizer))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=416))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding2\"],\n",
    "                            top_names = [\"reshape2\"],\n",
    "                            leading_dim=2))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReduceSum,\n",
    "                            bottom_names = [\"reshape2\"],\n",
    "                            top_names = [\"wide_redn\"],\n",
    "                            axis = 1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"dense\"],\n",
    "                            top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"dropout2\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Add,\n",
    "                            bottom_names = [\"fc3\", \"wide_redn\"],\n",
    "                            top_names = [\"add1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"add1\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.fit(max_iter = 21000, display = 1000, eval_interval = 4000, snapshot = 20000, snapshot_prefix = \"wdl\")\n",
    "model.graph_to_json(graph_config_file = \"wdl.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fc7ee08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "By default, for Open MPI 4.0 and later, infiniband ports on a device\n",
      "are not used by default.  The intent is to use UCX for these devices.\n",
      "You can override this policy by setting the btl_openib_allow_ib MCA parameter\n",
      "to true.\n",
      "\n",
      "  Local host:              prm-dgx-09\n",
      "  Local adapter:           mlx5_0\n",
      "  Local port:              1\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "WARNING: There was an error initializing an OpenFabrics device.\n",
      "\n",
      "  Local host:   prm-dgx-09\n",
      "  Local device: mlx5_1\n",
      "--------------------------------------------------------------------------\n",
      "HugeCTR Version: 3.2.0\n",
      "====================================================Model Init=====================================================\n",
      "[29d05h47m25s][HUGECTR][INFO]: Global seed is 2913349173\n",
      "[29d05h47m25s][HUGECTR][INFO]: Device to NUMA mapping:\n",
      "  GPU 2 ->  node 0\n",
      "\n",
      "[29d05h47m27s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "[29d05h47m27s][HUGECTR][INFO]: Start all2all warmup\n",
      "[29d05h47m27s][HUGECTR][INFO]: End all2all warmup\n",
      "[29d05h47m27s][HUGECTR][INFO]: Using All-reduce algorithm NCCL\n",
      "Device 2: Tesla V100-SXM2-16GB\n",
      "[29d05h47m27s][HUGECTR][INFO]: num of DataReader workers: 1\n",
      "[29d05h47m27s][HUGECTR][INFO]: Vocabulary size: 2138588\n",
      "[29d05h47m27s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=2097152\n",
      "[29d05h47m27s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=2211840\n",
      "===================================================Model Compile===================================================\n",
      "[prm-dgx-09:00405] 1 more process has sent help message help-mpi-btl-openib.txt / ib port not selected\n",
      "[prm-dgx-09:00405] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\n",
      "[29d05h47m35s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[29d05h47m35s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[29d05h47m35s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[29d05h47m35s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[29d05h47m35s][HUGECTR][INFO]: Starting AUC NCCL warm-up\n",
      "[29d05h47m35s][HUGECTR][INFO]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          wide_data,deep_data           \n",
      "(None, 1)                               (None, 13)                              \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      wide_data                     sparse_embedding2             (None, 2, 1)                  \n",
      "DistributedSlotSparseEmbeddingHash      deep_data                     sparse_embedding1             (None, 26, 16)                \n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 416)                   \n",
      "Reshape                                 sparse_embedding2             reshape2                      (None, 2)                     \n",
      "ReduceSum                               reshape2                      wide_redn                     (None, 1)                     \n",
      "Concat                                  reshape1,dense                concat1                       (None, 429)                   \n",
      "InnerProduct                            concat1                       fc1                           (None, 1024)                  \n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "InnerProduct                            dropout1                      fc2                           (None, 1024)                  \n",
      "ReLU                                    fc2                           relu2                         (None, 1024)                  \n",
      "Dropout                                 relu2                         dropout2                      (None, 1024)                  \n",
      "InnerProduct                            dropout2                      fc3                           (None, 1)                     \n",
      "Add                                     fc3,wide_redn                 add1                          (None, 1)                     \n",
      "BinaryCrossEntropyLoss                  add1,label                    loss                                                        \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[29d50h47m35s][HUGECTR][INFO]: Use non-epoch mode with number of iterations: 21000\n",
      "[29d50h47m35s][HUGECTR][INFO]: Training batchsize: 2720, evaluation batchsize: 2720\n",
      "[29d50h47m35s][HUGECTR][INFO]: Evaluation interval: 4000, snapshot interval: 20000\n",
      "[29d50h47m35s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[29d50h47m35s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[29d50h47m35s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[29d50h47m35s][HUGECTR][INFO]: Training source file: ./train/_file_list.txt\n",
      "[29d50h47m35s][HUGECTR][INFO]: Evaluation source file: ./val/_file_list.txt\n",
      "[29d50h47m40s][HUGECTR][INFO]: Iter: 1000 Time(1000 iters): 4.780970s Loss: 0.106841 lr:0.001000\n",
      "[29d50h47m45s][HUGECTR][INFO]: Iter: 2000 Time(1000 iters): 4.705520s Loss: 0.116291 lr:0.001000\n",
      "[29d50h47m50s][HUGECTR][INFO]: Iter: 3000 Time(1000 iters): 4.691297s Loss: 0.137555 lr:0.001000\n",
      "[29d50h47m54s][HUGECTR][INFO]: Iter: 4000 Time(1000 iters): 4.697687s Loss: 0.117297 lr:0.001000\n",
      "[29d50h47m59s][HUGECTR][INFO]: Evaluation, AUC: 0.761770\n",
      "[29d50h47m59s][HUGECTR][INFO]: Eval Time for 4000 iters: 4.655788s\n",
      "[29d50h48m40s][HUGECTR][INFO]: Iter: 5000 Time(1000 iters): 9.348719s Loss: 0.132989 lr:0.001000\n",
      "[29d50h48m80s][HUGECTR][INFO]: Iter: 6000 Time(1000 iters): 4.700791s Loss: 0.106546 lr:0.001000\n",
      "[29d50h48m13s][HUGECTR][INFO]: Iter: 7000 Time(1000 iters): 4.695415s Loss: 0.125986 lr:0.001000\n",
      "[29d50h48m18s][HUGECTR][INFO]: Iter: 8000 Time(1000 iters): 4.721910s Loss: 0.134764 lr:0.001000\n",
      "[29d50h48m22s][HUGECTR][INFO]: Evaluation, AUC: 0.766361\n",
      "[29d50h48m22s][HUGECTR][INFO]: Eval Time for 4000 iters: 4.664713s\n",
      "[29d50h48m27s][HUGECTR][INFO]: Iter: 9000 Time(1000 iters): 9.371010s Loss: 0.120176 lr:0.001000\n",
      "[29d50h48m32s][HUGECTR][INFO]: Iter: 10000 Time(1000 iters): 4.692971s Loss: 0.123762 lr:0.001000\n",
      "[29d50h48m37s][HUGECTR][INFO]: Iter: 11000 Time(1000 iters): 4.702503s Loss: 0.123353 lr:0.001000\n",
      "[29d50h48m41s][HUGECTR][INFO]: Iter: 12000 Time(1000 iters): 4.688909s Loss: 0.139235 lr:0.001000\n",
      "[29d50h48m46s][HUGECTR][INFO]: Evaluation, AUC: 0.767879\n",
      "[29d50h48m46s][HUGECTR][INFO]: Eval Time for 4000 iters: 4.667014s\n",
      "[29d50h48m51s][HUGECTR][INFO]: Iter: 13000 Time(1000 iters): 9.372777s Loss: 0.115001 lr:0.001000\n",
      "[29d50h48m55s][HUGECTR][INFO]: Iter: 14000 Time(1000 iters): 4.691976s Loss: 0.132797 lr:0.001000\n",
      "[29d50h49m00s][HUGECTR][INFO]: Iter: 15000 Time(1000 iters): 4.705240s Loss: 0.124456 lr:0.001000\n",
      "[29d50h49m50s][HUGECTR][INFO]: Iter: 16000 Time(1000 iters): 4.689947s Loss: 0.151871 lr:0.001000\n",
      "[29d50h49m90s][HUGECTR][INFO]: Evaluation, AUC: 0.769498\n",
      "[29d50h49m90s][HUGECTR][INFO]: Eval Time for 4000 iters: 4.664924s\n",
      "[29d50h49m14s][HUGECTR][INFO]: Iter: 17000 Time(1000 iters): 9.415263s Loss: 0.117839 lr:0.001000\n",
      "[29d50h49m19s][HUGECTR][INFO]: Iter: 18000 Time(1000 iters): 4.698764s Loss: 0.115465 lr:0.001000\n",
      "[29d50h49m24s][HUGECTR][INFO]: Iter: 19000 Time(1000 iters): 4.717870s Loss: 0.118373 lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29d50h49m28s][HUGECTR][INFO]: Iter: 20000 Time(1000 iters): 4.700434s Loss: 0.128879 lr:0.001000\n",
      "[29d50h49m33s][HUGECTR][INFO]: Evaluation, AUC: 0.766710\n",
      "[29d50h49m33s][HUGECTR][INFO]: Eval Time for 4000 iters: 4.639850s\n",
      "[29d50h49m33s][HUGECTR][INFO]: Rank0: Write hash table to file\n",
      "[29d50h49m33s][HUGECTR][INFO]: Rank0: Write hash table to file\n",
      "[29d50h49m33s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[29d50h49m33s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[29d50h49m33s][HUGECTR][INFO]: Done\n",
      "[29d50h49m33s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[29d50h49m33s][HUGECTR][INFO]: Done\n",
      "[29d50h49m33s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[29d50h49m33s][HUGECTR][INFO]: Done\n",
      "[29d50h49m33s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[29d50h49m33s][HUGECTR][INFO]: Done\n",
      "[29d50h49m33s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[29d50h49m33s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[29d50h49m33s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[29d50h49m33s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "Finish 21000 iterations with batchsize: 2720 in 122.57s\n",
      "[29d50h49m38s][HUGECTR][INFO]: Save the model graph to wdl.json, successful\n"
     ]
    }
   ],
   "source": [
    "!python ./model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f40e50c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 301620\r\n",
      "-rw-rw-r-- 1 1025 1025     49824 Jul  6 07:00 HugeCTR_WDL_Training.ipynb\r\n",
      "drwxr-xr-x 2 root root      4096 Jul  5 05:43 categories\r\n",
      "drwxr-xr-x 3 root root      4096 Jul  5 05:44 dask-worker-space\r\n",
      "-rw-r--r-- 1 root root      5539 Jul  6 07:00 model.py\r\n",
      "-rw-r--r-- 1 root root     14265 Jul  6 06:59 preprocess.py\r\n",
      "drwxr-xr-x 3 root root      4096 Jul  5 23:34 train\r\n",
      "drwxr-xr-x 3 root root      4096 Jul  5 05:44 val\r\n",
      "-rw-r--r-- 1 root root  17108704 Jul  6 03:28 wdl0_opt_sparse_20000.model\r\n",
      "drwxr-xr-x 2 root root      4096 Jul  5 06:32 wdl0_sparse_20000.model\r\n",
      "-rw-r--r-- 1 root root 273739264 Jul  6 03:28 wdl1_opt_sparse_20000.model\r\n",
      "drwxr-xr-x 2 root root      4096 Jul  5 06:32 wdl1_sparse_20000.model\r\n",
      "-rw-r--r-- 1 root root   5963780 Jul  6 03:28 wdl_dense_20000.model\r\n",
      "-rw-r--r-- 1 root root      3158 Jul  6 03:28 wdl_infer.json\r\n",
      "-rw-r--r-- 1 root root  11927560 Jul  6 03:28 wdl_opt_dense_20000.model\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a300e4",
   "metadata": {},
   "source": [
    "## 4. Inference Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80c436c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 645762\r\n",
      "-rw-r--r-- 1 root root        32 Nov 29 05:27 _file_list.txt\r\n",
      "-rw-r--r-- 1 root root   8554464 Nov 29 05:27 _hugectr.keyset\r\n",
      "-rw-r--r-- 1 root root     22726 Nov 29 05:27 _metadata\r\n",
      "-rw-r--r-- 1 root root      1509 Nov 29 05:27 _metadata.json\r\n",
      "-rw-r--r-- 1 root root 142825257 Nov 29 05:27 part_0.parquet\r\n",
      "-rw-r--r-- 1 root root     21459 Nov 29 05:27 schema.pbtxt\r\n",
      "drwxr-xr-x 2 root root      4096 Nov 29 05:26 temp-parquet-after-conversion\r\n",
      "-rw-r--r-- 1 root root 509766965 Nov 29 03:50 test.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /wdl_train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44ff2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"/wdl_train/val/part_0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5998dcc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>...</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "      <th>C22</th>\n",
       "      <th>C23</th>\n",
       "      <th>C24</th>\n",
       "      <th>C25</th>\n",
       "      <th>C26</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.055886</td>\n",
       "      <td>-0.548824</td>\n",
       "      <td>-0.272394</td>\n",
       "      <td>-0.157301</td>\n",
       "      <td>-0.224758</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>0.096421</td>\n",
       "      <td>-0.543133</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3856</td>\n",
       "      <td>4891</td>\n",
       "      <td>4119</td>\n",
       "      <td>143</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.059432</td>\n",
       "      <td>-0.380376</td>\n",
       "      <td>-0.272394</td>\n",
       "      <td>5.629719</td>\n",
       "      <td>-0.224758</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.279201</td>\n",
       "      <td>-0.253935</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>327</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.059432</td>\n",
       "      <td>-0.539315</td>\n",
       "      <td>-0.594327</td>\n",
       "      <td>-0.142386</td>\n",
       "      <td>-0.193763</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.023569</td>\n",
       "      <td>-0.687732</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2439</td>\n",
       "      <td>41980</td>\n",
       "      <td>349</td>\n",
       "      <td>3549</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.059432</td>\n",
       "      <td>-0.463242</td>\n",
       "      <td>-0.594327</td>\n",
       "      <td>-0.097641</td>\n",
       "      <td>-0.209261</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.219206</td>\n",
       "      <td>-0.687732</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4024</td>\n",
       "      <td>3677</td>\n",
       "      <td>4287</td>\n",
       "      <td>565</td>\n",
       "      <td>306</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022145</td>\n",
       "      <td>-0.509429</td>\n",
       "      <td>-0.379705</td>\n",
       "      <td>-0.151335</td>\n",
       "      <td>-0.162767</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.281810</td>\n",
       "      <td>-0.470833</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>40847</td>\n",
       "      <td>3862</td>\n",
       "      <td>41562</td>\n",
       "      <td>1066</td>\n",
       "      <td>132</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         I1        I2        I3        I4        I5        I6        I7  \\\n",
       "0 -0.055886 -0.548824 -0.272394 -0.157301 -0.224758 -0.206385 -0.064249   \n",
       "1 -0.059432 -0.380376 -0.272394  5.629719 -0.224758 -0.206385 -0.064249   \n",
       "2 -0.059432 -0.539315 -0.594327 -0.142386 -0.193763 -0.206385 -0.064249   \n",
       "3 -0.059432 -0.463242 -0.594327 -0.097641 -0.209261 -0.206385 -0.064249   \n",
       "4  0.022145 -0.509429 -0.379705 -0.151335 -0.162767 -0.206385 -0.064249   \n",
       "\n",
       "         I8        I9       I10  ...  C18  C19    C20   C21    C22   C23  \\\n",
       "0  0.096421 -0.543133 -0.470383  ...    1    1   3856  4891   4119   143   \n",
       "1 -0.279201 -0.253935 -0.470383  ...    2    1      2     2      2     0   \n",
       "2 -0.023569 -0.687732 -0.470383  ...    1    1      0  2439  41980   349   \n",
       "3 -0.219206 -0.687732 -0.470383  ...    1    1   4024  3677   4287   565   \n",
       "4 -0.281810 -0.470833 -0.470383  ...    2    3  40847  3862  41562  1066   \n",
       "\n",
       "    C24  C25  C26  label  \n",
       "0    50    1    1    0.0  \n",
       "1   327    2    1    0.0  \n",
       "2  3549    6    1    1.0  \n",
       "3   306    4    1    0.0  \n",
       "4   132    2    1    0.0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74d1d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10).to_csv('/wdl_train/infer_test.csv', sep=',', index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59a8b677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /wdl_train/wdl2predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /wdl_train/wdl2predict.py\n",
    "from hugectr.inference import InferenceParams, CreateInferenceSession\n",
    "import hugectr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "def wdl_inference(model_name, network_file, dense_file, embedding_file_list, data_file,enable_cache):\n",
    "    CATEGORICAL_COLUMNS=[\"C\" + str(x) for x in range(1, 27)]+[\"C1_C2\",\"C3_C4\"]\n",
    "    CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "    LABEL_COLUMNS = ['label']\n",
    "    emb_size = [249058, 19561, 14212, 6890, 18592, 4, 6356, 1254, 52, 226170, 80508, 72308, 11, 2169, 7597, 61, 4, 923, 15, 249619, 168974, 243480, 68212, 9169, 75, 34, 278018, 415262]\n",
    "    shift = np.insert(np.cumsum(emb_size), 0, 0)[:-1]\n",
    "    test_df=pd.read_csv(data_file,sep=',')\n",
    "    config_file = network_file\n",
    "    row_ptrs = list(range(0,21))+list(range(0,261))\n",
    "    dense_features =  list(test_df[CONTINUOUS_COLUMNS].values.flatten())\n",
    "    test_df[CATEGORICAL_COLUMNS].astype(np.int64)\n",
    "    embedding_columns = list((test_df[CATEGORICAL_COLUMNS]+shift).values.flatten())\n",
    "\n",
    "    # create parameter server, embedding cache and inference session\n",
    "    inference_params = InferenceParams(model_name = model_name,\n",
    "                                max_batchsize = 64,\n",
    "                                hit_rate_threshold = 0.9,\n",
    "                                dense_model_file = dense_file,\n",
    "                                sparse_model_files = embedding_file_list,\n",
    "                                device_id = 2,\n",
    "                                use_gpu_embedding_cache = enable_cache,\n",
    "                                cache_size_percentage = 0.9,\n",
    "                                i64_input_key = True,\n",
    "                                use_mixed_precision = False\n",
    "                                )\n",
    "    inference_session = CreateInferenceSession(config_file, inference_params)\n",
    "    output = inference_session.predict(dense_features, embedding_columns, row_ptrs)\n",
    "    print(\"WDL multi-embedding table inference result is {}\".format(output))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    model_name = sys.argv[1]\n",
    "    network_file = sys.argv[2]\n",
    "    dense_file = sys.argv[3]\n",
    "    embedding_file_list = str(sys.argv[4]).split(',')\n",
    "    print(embedding_file_list)\n",
    "    data_file = sys.argv[5]\n",
    "  \n",
    "\n",
    "    #wdl_inference(model_name, network_file, dense_file, embedding_file_list, data_file, True,hugectr.Database_t.Redis)\n",
    "    wdl_inference(model_name, network_file, dense_file, embedding_file_list, data_file, True)\n",
    "    #wdl_inference(model_name, network_file, dense_file, embedding_file_list, data_file, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebceb5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "By default, for Open MPI 4.0 and later, infiniband ports on a device\n",
      "are not used by default.  The intent is to use UCX for these devices.\n",
      "You can override this policy by setting the btl_openib_allow_ib MCA parameter\n",
      "to true.\n",
      "\n",
      "  Local host:              prm-dgx-09\n",
      "  Local adapter:           mlx5_0\n",
      "  Local port:              1\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "WARNING: There is at least non-excluded one OpenFabrics device found,\n",
      "but there are no active ports detected (or Open MPI was unable to use\n",
      "them).  This is most certainly not what you wanted.  Check your\n",
      "cables, subnet manager configuration, etc.  The openib BTL will be\n",
      "ignored for this job.\n",
      "\n",
      "  Local host: prm-dgx-09\n",
      "--------------------------------------------------------------------------\n",
      "['/wdl_train/wdl0_sparse_20000.model', '/wdl_train/wdl1_sparse_20000.model']\n",
      "[29d06h08m23s][HUGECTR][INFO]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[29d06h08m23s][HUGECTR][INFO]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[29d06h08m25s][HUGECTR][INFO]: Global seed is 4042209424\n",
      "[29d06h08m25s][HUGECTR][INFO]: Device to NUMA mapping:\n",
      "  GPU 2 ->  node 0\n",
      "\n",
      "[29d06h08m26s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "[29d06h08m26s][HUGECTR][INFO]: Start all2all warmup\n",
      "[29d06h08m26s][HUGECTR][INFO]: End all2all warmup\n",
      "[29d06h08m26s][HUGECTR][INFO]: Use mixed precision: 0\n",
      "[29d06h08m26s][HUGECTR][INFO]: start create embedding for inference\n",
      "[29d06h08m26s][HUGECTR][INFO]: sparse_input name wide_data\n",
      "[29d06h08m26s][HUGECTR][INFO]: sparse_input name deep_data\n",
      "[29d06h08m26s][HUGECTR][INFO]: create embedding for inference success\n",
      "[29d06h08m26s][HUGECTR][INFO]: Inference stage skip BinaryCrossEntropyLoss layer, replaced by Sigmoid layer\n",
      "WDL multi-embedding table inference result is [0.027012677863240242, 0.005466009955853224, 0.044732414186000824, 0.0037094708532094955, 0.012819402851164341, 0.05307499319314957, 0.003067328128963709, 0.010899372398853302, 0.017959514632821083, 0.03563397750258446]\n",
      "[prm-dgx-09:01096] 3 more processes have sent help message help-mpi-btl-openib.txt / ib port not selected\n",
      "[prm-dgx-09:01096] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\n"
     ]
    }
   ],
   "source": [
    "!python /wdl_train/wdl2predict.py \"wdl\" \"/wdl_train/wdl.json\" \"/wdl_train/wdl_dense_20000.model\" \"/wdl_train/wdl0_sparse_20000.model,/wdl_train/wdl1_sparse_20000.model\" \"/wdl_train/infer_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8341ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
