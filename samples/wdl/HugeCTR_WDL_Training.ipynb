{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "195a2ae5-1bd3-4e04-bbe2-04760d4f025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e8c871-540b-4e82-aff3-fc5bc340f64e",
   "metadata": {},
   "source": [
    "# 1.Overview\n",
    "\n",
    "In this notebook, we want to provide an tutorial how to train a wdl model using HugeCTR High-level python API. We will use original Criteo dataset as training data\n",
    "\n",
    "1. Overview\n",
    "2. Dataset Preprocessing\n",
    "3. WDL Model Training\n",
    "4. Save the Model Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f37d076-b23e-4bb6-876b-180699ab5ea2",
   "metadata": {},
   "source": [
    "# 2. Dataset Preprocessing\n",
    "## 2.1 Generate training and validation data folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07eb0150-c1ec-425f-8768-d307030c57c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some data folder to store the original and preprocessed data\n",
    "# Standard Libraries\n",
    "import os\n",
    "from time import time\n",
    "import re\n",
    "import shutil\n",
    "import glob\n",
    "import warnings\n",
    "BASE_DIR = \"/wdl_train\"\n",
    "train_path  = os.path.join(BASE_DIR, \"train\")\n",
    "val_path = os.path.join(BASE_DIR, \"val\")\n",
    "CUDA_VISIBLE_DEVICES = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
    "n_workers = len(CUDA_VISIBLE_DEVICES.split(\",\"))\n",
    "frac_size = 0.15\n",
    "allow_multi_gpu = False\n",
    "use_rmm_pool = False\n",
    "max_day = None  # (Optional) -- Limit the dataset to day 0-max_day for debugging\n",
    "\n",
    "if os.path.isdir(train_path):\n",
    "    shutil.rmtree(train_path)\n",
    "os.makedirs(train_path)\n",
    "\n",
    "if os.path.isdir(val_path):\n",
    "    shutil.rmtree(val_path)\n",
    "os.makedirs(val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cea33738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n"
     ]
    }
   ],
   "source": [
    "ls -l $train_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3943a3c-7ae9-46da-98ec-7c3a0d479feb",
   "metadata": {},
   "source": [
    "## 2.2 Download the Original Criteo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0213bcaf-6c13-4cdd-ba2c-01722f334579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "wget is already the newest version (1.20.3-1ubuntu1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac0abe2-032a-4f1b-b32b-99738d513cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P https://storage.googleapis.com/criteo-cail-datasets/day_0.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72fff32e-6ad6-43c2-aecd-500c59d7471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip the split data set to training and validation \n",
    "!gzip -d -c day_0.gz > day_0\n",
    "!head -n 45840617 day_0 > $train_path/train.txt\n",
    "!tail -n 2000000 day_0 > $val_path/test.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33165dda-ca3c-4721-a125-24b9745d3699",
   "metadata": {},
   "source": [
    "## 2.3 Preprocessing by NVTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7644f3-e4b6-40a0-bd35-cd056618481d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /wdl_train/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /wdl_train/preprocess.py\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "import dask_cudf\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "import cudf\n",
    "import rmm\n",
    "import nvtabular as nvt\n",
    "from nvtabular.utils import device_mem_size\n",
    "from nvtabular.ops import Categorify, Clip, FillMissing, LambdaOp, Normalize, Rename, Operator, get_embedding_sizes\n",
    "#%load_ext memory_profiler\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(message)s')\n",
    "logging.root.setLevel(logging.NOTSET)\n",
    "logging.getLogger('numba').setLevel(logging.WARNING)\n",
    "logging.getLogger('asyncio').setLevel(logging.WARNING)\n",
    "\n",
    "# define dataset schema\n",
    "CATEGORICAL_COLUMNS=[\"C\" + str(x) for x in range(1, 27)]\n",
    "CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "LABEL_COLUMNS = ['label']\n",
    "COLUMNS =  LABEL_COLUMNS + CONTINUOUS_COLUMNS +  CATEGORICAL_COLUMNS\n",
    "#/samples/criteo mode doesn't have dense features\n",
    "criteo_COLUMN=LABEL_COLUMNS +  CATEGORICAL_COLUMNS\n",
    "#For new feature cross columns\n",
    "CROSS_COLUMNS = []\n",
    "\n",
    "\n",
    "NUM_INTEGER_COLUMNS = 13\n",
    "NUM_CATEGORICAL_COLUMNS = 26\n",
    "NUM_TOTAL_COLUMNS = 1 + NUM_INTEGER_COLUMNS + NUM_CATEGORICAL_COLUMNS\n",
    "\n",
    "\n",
    "# Initialize RMM pool on ALL workers\n",
    "def setup_rmm_pool(client, pool_size):\n",
    "    client.run(rmm.reinitialize, pool_allocator=True, initial_pool_size=pool_size)\n",
    "    return None\n",
    "\n",
    "#compute the partition size with GB\n",
    "def bytesto(bytes, to, bsize=1024):\n",
    "    a = {'k' : 1, 'm': 2, 'g' : 3, 't' : 4, 'p' : 5, 'e' : 6 }\n",
    "    r = float(bytes)\n",
    "    return bytes / (bsize ** a[to])\n",
    "\n",
    "#process the data with NVTabular\n",
    "def process_NVT(args):\n",
    "\n",
    "    if args.feature_cross_list:\n",
    "        feature_pairs = [pair.split(\"_\") for pair in args.feature_cross_list.split(\",\")]\n",
    "        for pair in feature_pairs:\n",
    "            CROSS_COLUMNS.append(pair[0]+'_'+pair[1])\n",
    "\n",
    "\n",
    "    logging.info('NVTabular processing')\n",
    "    train_output = os.path.join(args.out_path, \"train\")\n",
    "    print(\"Training output data: \"+train_output)\n",
    "    val_output = os.path.join(args.out_path, \"val\")\n",
    "    print(\"Validation output data: \"+val_output)\n",
    "    train_input = os.path.join(args.data_path, \"train/train.txt\")\n",
    "    print(\"Training dataset: \"+train_input)\n",
    "    val_input = os.path.join(args.data_path, \"val/test.txt\")\n",
    "    PREPROCESS_DIR_temp_train = os.path.join(args.out_path, 'train/temp-parquet-after-conversion')  \n",
    "    PREPROCESS_DIR_temp_val = os.path.join(args.out_path, \"val/temp-parquet-after-conversion\")\n",
    "    if not os.path.exists(PREPROCESS_DIR_temp_train):\n",
    "        os.makedirs(PREPROCESS_DIR_temp_train)\n",
    "    \n",
    "    if not os.path.exists(PREPROCESS_DIR_temp_val):\n",
    "        os.makedirs(PREPROCESS_DIR_temp_val)\n",
    "    \n",
    "    PREPROCESS_DIR_temp = [PREPROCESS_DIR_temp_train, PREPROCESS_DIR_temp_val]\n",
    "    \n",
    "    \n",
    "\n",
    "    # Make sure we have a clean parquet space for cudf conversion\n",
    "    for one_path in PREPROCESS_DIR_temp:\n",
    "        if os.path.exists(one_path):\n",
    "            shutil.rmtree(one_path)\n",
    "        os.mkdir(one_path)\n",
    "\n",
    "\n",
    "    ## Get Dask Client\n",
    "\n",
    "    # Deploy a Single-Machine Multi-GPU Cluster\n",
    "    device_size = device_mem_size(kind=\"total\")\n",
    "    cluster = None\n",
    "    if args.protocol == \"ucx\":\n",
    "        UCX_TLS = os.environ.get(\"UCX_TLS\", \"tcp,cuda_copy,cuda_ipc,sockcm\")\n",
    "        os.environ[\"UCX_TLS\"] = UCX_TLS\n",
    "        cluster = LocalCUDACluster(\n",
    "            protocol = args.protocol,\n",
    "            CUDA_VISIBLE_DEVICES = args.devices,\n",
    "            n_workers = len(args.devices.split(\",\")),\n",
    "            enable_nvlink=True,\n",
    "            device_memory_limit = int(device_size * args.device_limit_frac),\n",
    "            dashboard_address=\":\" + args.dashboard_port\n",
    "        )\n",
    "    else:\n",
    "        cluster = LocalCUDACluster(\n",
    "            protocol = args.protocol,\n",
    "            n_workers = len(args.devices.split(\",\")),\n",
    "            CUDA_VISIBLE_DEVICES = args.devices,\n",
    "            device_memory_limit = int(device_size * args.device_limit_frac),\n",
    "            dashboard_address=\":\" + args.dashboard_port\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # Create the distributed client\n",
    "    client = Client(cluster)\n",
    "    if args.device_pool_frac > 0.01:\n",
    "        setup_rmm_pool(client, int(args.device_pool_frac*device_size))\n",
    "\n",
    "\n",
    "    #calculate the total processing time\n",
    "    runtime = time.time()\n",
    "\n",
    "    #test dataset without the label feature\n",
    "    if args.dataset_type == 'test':\n",
    "        global LABEL_COLUMNS\n",
    "        LABEL_COLUMNS = []\n",
    "\n",
    "    ##-----------------------------------##\n",
    "    # Dask rapids converts txt to parquet\n",
    "    # Dask cudf dataframe = ddf\n",
    "\n",
    "    ## train/valid txt to parquet\n",
    "    train_valid_paths = [(train_input,PREPROCESS_DIR_temp_train),(val_input,PREPROCESS_DIR_temp_val)]\n",
    "\n",
    "    for input, temp_output in train_valid_paths:\n",
    "\n",
    "        ddf = dask_cudf.read_csv(input,sep='\\t',names=LABEL_COLUMNS + CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS)\n",
    "\n",
    "        ## Convert label col to FP32\n",
    "        if args.parquet_format and args.dataset_type == 'train':\n",
    "            ddf[\"label\"] = ddf['label'].astype('float32')\n",
    "\n",
    "        # Save it as parquet format for better memory usage\n",
    "        ddf.to_parquet(temp_output,header=True)\n",
    "        ##-----------------------------------##\n",
    "\n",
    "    COLUMNS =  LABEL_COLUMNS + CONTINUOUS_COLUMNS + CROSS_COLUMNS + CATEGORICAL_COLUMNS\n",
    "    train_paths = glob.glob(os.path.join(PREPROCESS_DIR_temp_train, \"*.parquet\"))\n",
    "    valid_paths = glob.glob(os.path.join(PREPROCESS_DIR_temp_val, \"*.parquet\"))\n",
    "\n",
    "    categorify_op = Categorify(freq_threshold=args.freq_limit)\n",
    "    cat_features = CATEGORICAL_COLUMNS >> categorify_op\n",
    "    cont_features = CONTINUOUS_COLUMNS >> FillMissing() >> Clip(min_value=0) >> Normalize()\n",
    "    cross_cat_op = Categorify(encode_type=\"combo\", freq_threshold=args.freq_limit)\n",
    "\n",
    "    features = LABEL_COLUMNS\n",
    "    \n",
    "    if args.criteo_mode == 0:\n",
    "        features += cont_features\n",
    "        if args.feature_cross_list:\n",
    "            feature_pairs = [pair.split(\"_\") for pair in args.feature_cross_list.split(\",\")]\n",
    "            for pair in feature_pairs:\n",
    "                features += [pair] >> cross_cat_op\n",
    "            \n",
    "    features += cat_features\n",
    "\n",
    "    workflow = nvt.Workflow(features, client=client)\n",
    "\n",
    "    logging.info(\"Preprocessing\")\n",
    "\n",
    "    output_format = 'hugectr'\n",
    "    if args.parquet_format:\n",
    "        output_format = 'parquet'\n",
    "\n",
    "    # just for /samples/criteo model\n",
    "    train_ds_iterator = nvt.Dataset(train_paths, engine='parquet', part_size=int(args.part_mem_frac * device_size))\n",
    "    valid_ds_iterator = nvt.Dataset(valid_paths, engine='parquet', part_size=int(args.part_mem_frac * device_size))\n",
    "\n",
    "    shuffle = None\n",
    "    if args.shuffle == \"PER_WORKER\":\n",
    "        shuffle = nvt.io.Shuffle.PER_WORKER\n",
    "    elif args.shuffle == \"PER_PARTITION\":\n",
    "        shuffle = nvt.io.Shuffle.PER_PARTITION\n",
    "\n",
    "    logging.info('Train Datasets Preprocessing.....')\n",
    "\n",
    "    dict_dtypes = {}\n",
    "    for col in CATEGORICAL_COLUMNS:\n",
    "        dict_dtypes[col] = np.int64\n",
    "    if not args.criteo_mode:\n",
    "        for col in CONTINUOUS_COLUMNS:\n",
    "            dict_dtypes[col] = np.float32\n",
    "    for col in CROSS_COLUMNS:\n",
    "        dict_dtypes[col] = np.int64\n",
    "    for col in LABEL_COLUMNS:\n",
    "        dict_dtypes[col] = np.float32\n",
    "    \n",
    "    conts = CONTINUOUS_COLUMNS if not args.criteo_mode else []\n",
    "    \n",
    "    workflow.fit(train_ds_iterator)\n",
    "    \n",
    "    if output_format == 'hugectr':\n",
    "        workflow.transform(train_ds_iterator).to_hugectr(\n",
    "                cats=CATEGORICAL_COLUMNS + CROSS_COLUMNS,\n",
    "                conts=conts,\n",
    "                labels=LABEL_COLUMNS,\n",
    "                output_path=train_output,\n",
    "                shuffle=shuffle,\n",
    "                out_files_per_proc=args.out_files_per_proc,\n",
    "                num_threads=args.num_io_threads)\n",
    "    else:\n",
    "        workflow.transform(train_ds_iterator).to_parquet(\n",
    "                output_path=train_output,\n",
    "                dtypes=dict_dtypes,\n",
    "                cats=CATEGORICAL_COLUMNS + CROSS_COLUMNS,\n",
    "                conts=conts,\n",
    "                labels=LABEL_COLUMNS,\n",
    "                shuffle=shuffle,\n",
    "                out_files_per_proc=args.out_files_per_proc,\n",
    "                num_threads=args.num_io_threads)\n",
    "        \n",
    "        \n",
    "        \n",
    "    ###Getting slot size###    \n",
    "    #--------------------##\n",
    "    embeddings_dict_cat = categorify_op.get_embedding_sizes(CATEGORICAL_COLUMNS)\n",
    "    embeddings_dict_cross = cross_cat_op.get_embedding_sizes(CROSS_COLUMNS)\n",
    "    embeddings = [embeddings_dict_cat[c][0] for c in CATEGORICAL_COLUMNS] + [embeddings_dict_cross[c][0] for c in CROSS_COLUMNS]\n",
    "    \n",
    "    print(embeddings)\n",
    "    ##--------------------##\n",
    "\n",
    "    logging.info('Valid Datasets Preprocessing.....')\n",
    "\n",
    "    if output_format == 'hugectr':\n",
    "        workflow.transform(valid_ds_iterator).to_hugectr(\n",
    "                cats=CATEGORICAL_COLUMNS + CROSS_COLUMNS,\n",
    "                conts=conts,\n",
    "                labels=LABEL_COLUMNS,\n",
    "                output_path=val_output,\n",
    "                shuffle=shuffle,\n",
    "                out_files_per_proc=args.out_files_per_proc,\n",
    "                num_threads=args.num_io_threads)\n",
    "    else:\n",
    "        workflow.transform(valid_ds_iterator).to_parquet(\n",
    "                output_path=val_output,\n",
    "                dtypes=dict_dtypes,\n",
    "                cats=CATEGORICAL_COLUMNS + CROSS_COLUMNS,\n",
    "                conts=conts,\n",
    "                labels=LABEL_COLUMNS,\n",
    "                shuffle=shuffle,\n",
    "                out_files_per_proc=args.out_files_per_proc,\n",
    "                num_threads=args.num_io_threads)\n",
    "\n",
    "    embeddings_dict_cat = categorify_op.get_embedding_sizes(CATEGORICAL_COLUMNS)\n",
    "    embeddings_dict_cross = cross_cat_op.get_embedding_sizes(CROSS_COLUMNS)\n",
    "    embeddings = [embeddings_dict_cat[c][0] for c in CATEGORICAL_COLUMNS] + [embeddings_dict_cross[c][0] for c in CROSS_COLUMNS]\n",
    "    \n",
    "    print(embeddings)\n",
    "    ##--------------------##\n",
    "\n",
    "    ## Shutdown clusters\n",
    "    client.close()\n",
    "    logging.info('NVTabular processing done')\n",
    "\n",
    "    runtime = time.time() - runtime\n",
    "\n",
    "    print(\"\\nDask-NVTabular Criteo Preprocessing\")\n",
    "    print(\"--------------------------------------\")\n",
    "    print(f\"data_path          | {args.data_path}\")\n",
    "    print(f\"output_path        | {args.out_path}\")\n",
    "    print(f\"partition size     | {'%.2f GB'%bytesto(int(args.part_mem_frac * device_size),'g')}\")\n",
    "    print(f\"protocol           | {args.protocol}\")\n",
    "    print(f\"device(s)          | {args.devices}\")\n",
    "    print(f\"rmm-pool-frac      | {(args.device_pool_frac)}\")\n",
    "    print(f\"out-files-per-proc | {args.out_files_per_proc}\")\n",
    "    print(f\"num_io_threads     | {args.num_io_threads}\")\n",
    "    print(f\"shuffle            | {args.shuffle}\")\n",
    "    print(\"======================================\")\n",
    "    print(f\"Runtime[s]         | {runtime}\")\n",
    "    print(\"======================================\\n\")\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=(\"Multi-GPU Criteo Preprocessing\"))\n",
    "\n",
    "    #\n",
    "    # System Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\"--data_path\", type=str, help=\"Input dataset path (Required)\")\n",
    "    parser.add_argument(\"--out_path\", type=str, help=\"Directory path to write output (Required)\")\n",
    "    parser.add_argument(\n",
    "        \"-d\",\n",
    "        \"--devices\",\n",
    "        default=os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\"),\n",
    "        type=str,\n",
    "        help='Comma-separated list of visible devices (e.g. \"0,1,2,3\"). '\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-p\",\n",
    "        \"--protocol\",\n",
    "        choices=[\"tcp\", \"ucx\"],\n",
    "        default=\"tcp\",\n",
    "        type=str,\n",
    "        help=\"Communication protocol to use (Default 'tcp')\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device_limit_frac\",\n",
    "        default=0.5,\n",
    "        type=float,\n",
    "        help=\"Worker device-memory limit as a fraction of GPU capacity (Default 0.8). \"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device_pool_frac\",\n",
    "        default=0.9,\n",
    "        type=float,\n",
    "        help=\"RMM pool size for each worker  as a fraction of GPU capacity (Default 0.9). \"\n",
    "        \"The RMM pool frac is the same for all GPUs, make sure each one has enough memory size\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_io_threads\",\n",
    "        default=0,\n",
    "        type=int,\n",
    "        help=\"Number of threads to use when writing output data (Default 0). \"\n",
    "        \"If 0 is specified, multi-threading will not be used for IO.\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Data-Decomposition Parameters\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--part_mem_frac\",\n",
    "        default=0.125,\n",
    "        type=float,\n",
    "        help=\"Maximum size desired for dataset partitions as a fraction \"\n",
    "        \"of GPU capacity (Default 0.125)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out_files_per_proc\",\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=\"Number of output files to write on each worker (Default 1)\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Preprocessing Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"-f\",\n",
    "        \"--freq_limit\",\n",
    "        default=0,\n",
    "        type=int,\n",
    "        help=\"Frequency limit for categorical encoding (Default 0)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-s\",\n",
    "        \"--shuffle\",\n",
    "        choices=[\"PER_WORKER\", \"PER_PARTITION\", \"NONE\"],\n",
    "        default=\"PER_PARTITION\",\n",
    "        help=\"Shuffle algorithm to use when writing output data to disk (Default PER_PARTITION)\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--feature_cross_list\", default=None, type=str, help=\"List of feature crossing cols (e.g. C1_C2, C3_C4)\"\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Diagnostics Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--profile\",\n",
    "        metavar=\"PATH\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"Specify a file path to export a Dask profile report (E.g. dask-report.html).\"\n",
    "        \"If this option is excluded from the command, not profile will be exported\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dashboard_port\",\n",
    "        default=\"8787\",\n",
    "        type=str,\n",
    "        help=\"Specify the desired port of Dask's diagnostics-dashboard (Default `3787`). \"\n",
    "        \"The dashboard will be hosted at http://<IP>:<PORT>/status\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Format\n",
    "    #\n",
    "\n",
    "    parser.add_argument('--criteo_mode', type=int, default=0)\n",
    "    parser.add_argument('--parquet_format', type=int, default=1)\n",
    "    parser.add_argument('--dataset_type', type=str, default='train')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.n_workers = len(args.devices.split(\",\"))\n",
    "    return args\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args = parse_args()\n",
    "\n",
    "    process_NVT(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97aae996-9677-40ae-a976-6edb51fcc61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d57560b-c6cf-418e-b45b-3b6722d29a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-11 08:14:51,395 NVTabular processing\n",
      "Training output data: /wdl_train/train\n",
      "Validation output data: /wdl_train/val\n",
      "Training dataset: /wdl_train/train/train.txt\n",
      "2022-11-11 08:14:53,489 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2022-11-11 08:14:53,506 Unable to start CUDA Context\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pynvml/nvml.py\", line 782, in _nvmlGetFunctionPointer\n",
      "    _nvmlGetFunctionPointer_cache[name] = getattr(nvmlLib, name)\n",
      "  File \"/usr/lib/python3.8/ctypes/__init__.py\", line 386, in __getattr__\n",
      "    func = self.__getitem__(name)\n",
      "  File \"/usr/lib/python3.8/ctypes/__init__.py\", line 391, in __getitem__\n",
      "    func = self._FuncPtr((name_or_ordinal, self))\n",
      "AttributeError: /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1: undefined symbol: nvmlDeviceGetComputeRunningProcesses_v2\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/dask_cuda/initialize.py\", line 41, in _create_cuda_context\n",
      "    ctx = has_cuda_context()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/distributed/diagnostics/nvml.py\", line 120, in has_cuda_context\n",
      "    running_processes = pynvml.nvmlDeviceGetComputeRunningProcesses_v2(handle)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pynvml/nvml.py\", line 2191, in nvmlDeviceGetComputeRunningProcesses_v2\n",
      "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetComputeRunningProcesses_v2\")\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pynvml/nvml.py\", line 785, in _nvmlGetFunctionPointer\n",
      "    raise NVMLError(NVML_ERROR_FUNCTION_NOT_FOUND)\n",
      "pynvml.nvml.NVMLError_FunctionNotFound: Function Not Found\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/core/utils.py:384: FutureWarning: The `client` argument is deprecated from DaskExecutor and will be removed in a future version of NVTabular. By default, a global client in the same python context will be detected automatically, and `merlin.utils.set_dask_client` (as well as `Distributed` and `Serial`) can be used for explicit control.\n",
      "  warnings.warn(\n",
      "2022-11-11 08:15:11,956 Preprocessing\n",
      "2022-11-11 08:15:12,182 Train Datasets Preprocessing.....\n",
      "[249058, 19561, 14212, 6890, 18592, 4, 6356, 1254, 52, 226170, 80508, 72308, 11, 2169, 7597, 61, 4, 923, 15, 249619, 168974, 243480, 68212, 9169, 75, 34, 281564, 415262]\n",
      "2022-11-11 08:16:17,137 Valid Datasets Preprocessing.....\n",
      "[249058, 19561, 14212, 6890, 18592, 4, 6356, 1254, 52, 226170, 80508, 72308, 11, 2169, 7597, 61, 4, 923, 15, 249619, 168974, 243480, 68212, 9169, 75, 34, 281564, 415262]\n",
      "2022-11-11 08:16:18,356 NVTabular processing done\n",
      "\n",
      "Dask-NVTabular Criteo Preprocessing\n",
      "--------------------------------------\n",
      "data_path          | /wdl_train/\n",
      "output_path        | /wdl_train/\n",
      "partition size     | 3.97 GB\n",
      "protocol           | tcp\n",
      "device(s)          | 0\n",
      "rmm-pool-frac      | 0.5\n",
      "out-files-per-proc | 1\n",
      "num_io_threads     | 2\n",
      "shuffle            | PER_PARTITION\n",
      "======================================\n",
      "Runtime[s]         | 83.79252552986145\n",
      "======================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 /wdl_train/preprocess.py --data_path /wdl_train/ --out_path /wdl_train/ --freq_limit 6 --feature_cross_list C1_C2,C3_C4 --device_pool_frac 0.5 --num_io_threads 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20924837",
   "metadata": {},
   "source": [
    "### 2.4 Checke the preprocessed training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc6e4663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 14581444\n",
      "-rw-r--r-- 1 root root          34 Nov 11 07:47 _file_list.txt\n",
      "-rw-r--r-- 1 root root      450252 Nov 11 07:47 _metadata\n",
      "-rw-r--r-- 1 root root        1510 Nov 11 07:47 _metadata.json\n",
      "lrwxrwxrwx 1 root root           8 Nov 11 07:36 day_0.gz -> day_0.gz\n",
      "-rw-r--r-- 1 root root  3381186419 Nov 11 07:47 part_0.parquet\n",
      "-rw-r--r-- 1 root root       27296 Nov 11 07:47 schema.pbtxt\n",
      "drwxr-xr-x 2 root root        4096 Nov 11 07:46 temp-parquet-after-conversion\n",
      "-rw-r--r-- 1 root root 11549710546 Nov 11 07:45 train.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -ll /wdl_train/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0455ab5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>...</th>\n",
       "      <th>C17</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "      <th>C22</th>\n",
       "      <th>C23</th>\n",
       "      <th>C24</th>\n",
       "      <th>C25</th>\n",
       "      <th>C26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.032831</td>\n",
       "      <td>0.118178</td>\n",
       "      <td>-0.594327</td>\n",
       "      <td>0.275234</td>\n",
       "      <td>-0.100776</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>3.357030</td>\n",
       "      <td>-0.760031</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>335</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.050565</td>\n",
       "      <td>-0.527089</td>\n",
       "      <td>-0.379705</td>\n",
       "      <td>-0.115539</td>\n",
       "      <td>0.581131</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.279201</td>\n",
       "      <td>-0.615432</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>13472</td>\n",
       "      <td>14005</td>\n",
       "      <td>14002</td>\n",
       "      <td>1822</td>\n",
       "      <td>92</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         I1        I2        I3        I4        I5        I6        I7  \\\n",
       "0 -0.032831  0.118178 -0.594327  0.275234 -0.100776 -0.206385 -0.064249   \n",
       "1 -0.050565 -0.527089 -0.379705 -0.115539  0.581131 -0.206385 -0.064249   \n",
       "\n",
       "         I8        I9       I10  ...  C17  C18  C19    C20    C21    C22  \\\n",
       "0  3.357030 -0.760031 -0.470383  ...    1    1    2      1      1      1   \n",
       "1 -0.279201 -0.615432 -0.470383  ...    2    1    4  13472  14005  14002   \n",
       "\n",
       "    C23  C24  C25  C26  \n",
       "0     1  335    1    2  \n",
       "1  1822   92    6    5  \n",
       "\n",
       "[2 rows x 42 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"/wdl_train/train/part_0.parquet\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c7026-017e-46a2-ae74-bc820e60cc6e",
   "metadata": {},
   "source": [
    "## 3. WDL Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3d01115-b0eb-4b7f-82d2-1d4a457d3170",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile './model.py'\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 4000,\n",
    "                              batchsize_eval = 2720,\n",
    "                              batchsize = 2720,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[2]],\n",
    "                              repeat_dataset = True,\n",
    "                              i64_input_key = True)\n",
    "\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Parquet,\n",
    "                                  source = [\"/wdl_train/train/_file_list.txt\"],\n",
    "                                  eval_source = \"/wdl_train/val/_file_list.txt\",\n",
    "                                  check_type = hugectr.Check_t.Non,\n",
    "                                  slot_size_array = [278018, 415262,249058, 19561, 14212, 6890, 18592, 4, 6356, 1254, 52, 226170, 80508, 72308, 11, 2169, 7597, 61, 4, 923, 15, 249619, 168974, 243480, 68212, 9169, 75, 34])\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam,\n",
    "                                    update_type = hugectr.Update_t.Global,\n",
    "                                    beta1 = 0.9,\n",
    "                                    beta2 = 0.999,\n",
    "                                    epsilon = 0.0000001)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"wide_data\", 1, True, 2),\n",
    "                        hugectr.DataReaderSparseParam(\"deep_data\", 2, False, 26)]))\n",
    "\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 80,\n",
    "                            embedding_vec_size = 1,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding2\",\n",
    "                            bottom_name = \"wide_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 1350,\n",
    "                            embedding_vec_size = 16,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"deep_data\",\n",
    "                            optimizer = optimizer))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=416))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding2\"],\n",
    "                            top_names = [\"reshape2\"],\n",
    "                            leading_dim=2))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReduceSum,\n",
    "                            bottom_names = [\"reshape2\"],\n",
    "                            top_names = [\"wide_redn\"],\n",
    "                            axis = 1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"dense\"],\n",
    "                            top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"dropout2\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Add,\n",
    "                            bottom_names = [\"fc3\", \"wide_redn\"],\n",
    "                            top_names = [\"add1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"add1\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.fit(max_iter = 21000, display = 1000, eval_interval = 4000, snapshot = 20000, snapshot_prefix = \"wdl\")\n",
    "model.graph_to_json(graph_config_file = \"wdl.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fc7ee08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HugeCTR Version: 4.0\n",
      "====================================================Model Init=====================================================\n",
      "[HCTR][07:50:47.782][WARNING][RK0][main]: The model name is not specified when creating the solver.\n",
      "[HCTR][07:50:47.782][INFO][RK0][main]: Global seed is 3928542589\n",
      "[HCTR][07:50:47.785][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 2 ->  node 0\n",
      "[HCTR][07:50:49.887][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.\n",
      "[HCTR][07:50:49.887][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][07:50:49.887][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][07:50:49.888][INFO][RK0][main]: Using All-reduce algorithm: NCCL\n",
      "[HCTR][07:50:49.888][INFO][RK0][main]: Device 2: Tesla V100-SXM2-32GB\n",
      "[HCTR][07:50:49.889][INFO][RK0][main]: num of DataReader workers for train: 1\n",
      "[HCTR][07:50:49.889][INFO][RK0][main]: num of DataReader workers for eval: 1\n",
      "[HCTR][07:50:49.894][INFO][RK0][main]: Vocabulary size: 2138588\n",
      "[HCTR][07:50:49.895][INFO][RK0][main]: max_vocabulary_size_per_gpu_=6990506\n",
      "[HCTR][07:50:49.910][INFO][RK0][main]: max_vocabulary_size_per_gpu_=7372800\n",
      "[HCTR][07:50:49.916][INFO][RK0][main]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "[HCTR][07:50:58.374][INFO][RK0][main]: gpu0 start to init embedding\n",
      "[HCTR][07:50:58.374][INFO][RK0][main]: gpu0 init embedding done\n",
      "[HCTR][07:50:58.374][INFO][RK0][main]: gpu0 start to init embedding\n",
      "[HCTR][07:50:58.377][INFO][RK0][main]: gpu0 init embedding done\n",
      "[HCTR][07:50:58.379][INFO][RK0][main]: Starting AUC NCCL warm-up\n",
      "[HCTR][07:50:58.384][INFO][RK0][main]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "[HCTR][07:50:58.384][INFO][RK0][main]: Model structure on each GPU\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          wide_data,deep_data           \n",
      "(2720,1)                                (2720,13)                               \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "DistributedSlotSparseEmbeddingHash      wide_data                     sparse_embedding2             (2720,2,1)                    \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      deep_data                     sparse_embedding1             (2720,26,16)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding1             reshape1                      (2720,416)                    \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding2             reshape2                      (2720,2)                      \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReduceSum                               reshape2                      wide_redn                     (2720,1)                      \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Concat                                  reshape1                      concat1                       (2720,429)                    \n",
      "                                        dense                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            concat1                       fc1                           (2720,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc1                           relu1                         (2720,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu1                         dropout1                      (2720,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout1                      fc2                           (2720,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc2                           relu2                         (2720,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu2                         dropout2                      (2720,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout2                      fc3                           (2720,1)                      \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Add                                     fc3                           add1                          (2720,1)                      \n",
      "                                        wide_redn                                                                                 \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  add1                          loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[HCTR][07:50:58.384][INFO][RK0][main]: Use non-epoch mode with number of iterations: 21000\n",
      "[HCTR][07:50:58.384][INFO][RK0][main]: Training batchsize: 2720, evaluation batchsize: 2720\n",
      "[HCTR][07:50:58.384][INFO][RK0][main]: Evaluation interval: 4000, snapshot interval: 20000\n",
      "[HCTR][07:50:58.384][INFO][RK0][main]: Dense network trainable: True\n",
      "[HCTR][07:50:58.384][INFO][RK0][main]: Sparse embedding sparse_embedding1 trainable: True\n",
      "[HCTR][07:50:58.384][INFO][RK0][main]: Sparse embedding sparse_embedding2 trainable: True\n",
      "[HCTR][07:50:58.384][INFO][RK0][main]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HCTR][07:50:58.384][INFO][RK0][main]: lr: 0.001000, warmup_steps: 1, end_lr: 0.000000\n",
      "[HCTR][07:50:58.384][INFO][RK0][main]: decay_start: 0, decay_steps: 1, decay_power: 2.000000\n",
      "[HCTR][07:50:58.384][INFO][RK0][main]: Training source file: /wdl_train/train/_file_list.txt\n",
      "[HCTR][07:50:58.384][INFO][RK0][main]: Evaluation source file: /wdl_train/val/_file_list.txt\n",
      "[HCTR][07:51:06.968][INFO][RK0][main]: Iter: 1000 Time(1000 iters): 8.57918s Loss: 0.156711 lr:0.001\n",
      "[HCTR][07:51:15.460][INFO][RK0][main]: Iter: 2000 Time(1000 iters): 8.48834s Loss: 0.139188 lr:0.001\n",
      "[HCTR][07:51:23.955][INFO][RK0][main]: Iter: 3000 Time(1000 iters): 8.48977s Loss: 0.108323 lr:0.001\n",
      "[HCTR][07:51:32.465][INFO][RK0][main]: Iter: 4000 Time(1000 iters): 8.50591s Loss: 0.126609 lr:0.001\n",
      "[HCTR][07:51:36.979][INFO][RK0][main]: Evaluation, AUC: 0.761753\n",
      "[HCTR][07:51:36.979][INFO][RK0][main]: Eval Time for 4000 iters: 4.51319s\n",
      "[HCTR][07:51:45.483][INFO][RK0][main]: Iter: 5000 Time(1000 iters): 13.014s Loss: 0.140714 lr:0.001\n",
      "[HCTR][07:51:54.007][INFO][RK0][main]: Iter: 6000 Time(1000 iters): 8.51955s Loss: 0.102723 lr:0.001\n",
      "[HCTR][07:52:02.513][INFO][RK0][main]: Iter: 7000 Time(1000 iters): 8.50098s Loss: 0.121707 lr:0.001\n",
      "[HCTR][07:52:11.041][INFO][RK0][main]: Iter: 8000 Time(1000 iters): 8.52373s Loss: 0.135188 lr:0.001\n",
      "[HCTR][07:52:15.556][INFO][RK0][main]: Evaluation, AUC: 0.765908\n",
      "[HCTR][07:52:15.556][INFO][RK0][main]: Eval Time for 4000 iters: 4.51466s\n",
      "[HCTR][07:52:24.072][INFO][RK0][main]: Iter: 9000 Time(1000 iters): 13.027s Loss: 0.112884 lr:0.001\n",
      "[HCTR][07:52:32.589][INFO][RK0][main]: Iter: 10000 Time(1000 iters): 8.51274s Loss: 0.121695 lr:0.001\n",
      "[HCTR][07:52:41.092][INFO][RK0][main]: Iter: 11000 Time(1000 iters): 8.49781s Loss: 0.13792 lr:0.001\n",
      "[HCTR][07:52:49.620][INFO][RK0][main]: Iter: 12000 Time(1000 iters): 8.52383s Loss: 0.13388 lr:0.001\n",
      "[HCTR][07:52:54.135][INFO][RK0][main]: Evaluation, AUC: 0.770399\n",
      "[HCTR][07:52:54.135][INFO][RK0][main]: Eval Time for 4000 iters: 4.51434s\n",
      "[HCTR][07:53:02.653][INFO][RK0][main]: Iter: 13000 Time(1000 iters): 13.0286s Loss: 0.122835 lr:0.001\n",
      "[HCTR][07:53:11.170][INFO][RK0][main]: Iter: 14000 Time(1000 iters): 8.51302s Loss: 0.114205 lr:0.001\n",
      "[HCTR][07:53:19.689][INFO][RK0][main]: Iter: 15000 Time(1000 iters): 8.51426s Loss: 0.119325 lr:0.001\n",
      "[HCTR][07:53:28.215][INFO][RK0][main]: Iter: 16000 Time(1000 iters): 8.52213s Loss: 0.131266 lr:0.001\n",
      "[HCTR][07:53:32.725][INFO][RK0][main]: Evaluation, AUC: 0.772337\n",
      "[HCTR][07:53:32.725][INFO][RK0][main]: Eval Time for 4000 iters: 4.50754s\n",
      "[HCTR][07:53:41.355][INFO][RK0][main]: Iter: 17000 Time(1000 iters): 13.1355s Loss: 0.123475 lr:0.001\n",
      "[HCTR][07:53:49.873][INFO][RK0][main]: Iter: 18000 Time(1000 iters): 8.51358s Loss: 0.112642 lr:0.001\n",
      "[HCTR][07:53:58.406][INFO][RK0][main]: Iter: 19000 Time(1000 iters): 8.52822s Loss: 0.132159 lr:0.001\n",
      "[HCTR][07:54:06.929][INFO][RK0][main]: Iter: 20000 Time(1000 iters): 8.5184s Loss: 0.115293 lr:0.001\n",
      "[HCTR][07:54:11.427][INFO][RK0][main]: Evaluation, AUC: 0.767352\n",
      "[HCTR][07:54:11.427][INFO][RK0][main]: Eval Time for 4000 iters: 4.49762s\n",
      "[HCTR][07:54:11.427][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][07:54:11.444][INFO][RK0][main]: Rank0: Write hash table to file\n",
      "[HCTR][07:54:11.467][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][07:54:11.528][INFO][RK0][main]: Rank0: Write hash table to file\n",
      "[HCTR][07:54:11.804][INFO][RK0][main]: Dumping sparse weights to files, successful\n",
      "[HCTR][07:54:11.820][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][07:54:11.820][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][07:54:11.888][INFO][RK0][main]: Done\n",
      "[HCTR][07:54:11.891][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][07:54:11.891][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][07:54:11.954][INFO][RK0][main]: Done\n",
      "[HCTR][07:54:12.221][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][07:54:12.221][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][07:54:13.391][INFO][RK0][main]: Done\n",
      "[HCTR][07:54:13.669][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][07:54:13.669][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][07:54:14.877][INFO][RK0][main]: Done\n",
      "[HCTR][07:54:14.892][INFO][RK0][main]: Dumping sparse optimzer states to files, successful\n",
      "[HCTR][07:54:14.893][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][07:54:14.908][INFO][RK0][main]: Dumping dense weights to file, successful\n",
      "[HCTR][07:54:14.909][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][07:54:14.940][INFO][RK0][main]: Dumping dense optimizer states to file, successful\n",
      "[HCTR][07:54:23.472][INFO][RK0][main]: Finish 21000 iterations with batchsize: 2720 in 205.09s.\n",
      "[HCTR][07:54:23.473][INFO][RK0][main]: Save the model graph to wdl.json successfully\n"
     ]
    }
   ],
   "source": [
    "!python ./model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a300e4",
   "metadata": {},
   "source": [
    "## 4. Inference Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80c436c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 639344\n",
      "-rw-r--r-- 1 root root        32 Nov 11 07:47 _file_list.txt\n",
      "-rw-r--r-- 1 root root     21896 Nov 11 07:47 _metadata\n",
      "-rw-r--r-- 1 root root      1509 Nov 11 07:47 _metadata.json\n",
      "-rw-r--r-- 1 root root 144853353 Nov 11 07:47 part_0.parquet\n",
      "-rw-r--r-- 1 root root     27296 Nov 11 07:47 schema.pbtxt\n",
      "drwxr-xr-x 2 root root      4096 Nov 11 07:46 temp-parquet-after-conversion\n",
      "-rw-r--r-- 1 root root 509766965 Nov 11 07:45 test.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -l /wdl_train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44ff2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"/wdl_train/val/part_0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5998dcc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>...</th>\n",
       "      <th>C17</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "      <th>C22</th>\n",
       "      <th>C23</th>\n",
       "      <th>C24</th>\n",
       "      <th>C25</th>\n",
       "      <th>C26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.045245</td>\n",
       "      <td>-0.501278</td>\n",
       "      <td>-0.594327</td>\n",
       "      <td>-0.157301</td>\n",
       "      <td>-0.162767</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.253116</td>\n",
       "      <td>-0.760031</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>658</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.048792</td>\n",
       "      <td>-0.230945</td>\n",
       "      <td>-0.272394</td>\n",
       "      <td>-0.139403</td>\n",
       "      <td>0.271173</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>0.743047</td>\n",
       "      <td>-0.279201</td>\n",
       "      <td>0.107562</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>127</td>\n",
       "      <td>193</td>\n",
       "      <td>145</td>\n",
       "      <td>2635</td>\n",
       "      <td>518</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.061206</td>\n",
       "      <td>-0.176607</td>\n",
       "      <td>2.517687</td>\n",
       "      <td>-0.157301</td>\n",
       "      <td>-0.224758</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.281810</td>\n",
       "      <td>2.131947</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.020417</td>\n",
       "      <td>-0.548824</td>\n",
       "      <td>-0.379705</td>\n",
       "      <td>-0.154318</td>\n",
       "      <td>-0.224758</td>\n",
       "      <td>0.205918</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.281810</td>\n",
       "      <td>-0.615432</td>\n",
       "      <td>1.386036</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6070</td>\n",
       "      <td>7978</td>\n",
       "      <td>3238</td>\n",
       "      <td>342</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000864</td>\n",
       "      <td>0.566469</td>\n",
       "      <td>-0.165084</td>\n",
       "      <td>-0.157301</td>\n",
       "      <td>-0.224758</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.281810</td>\n",
       "      <td>-0.543133</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>443</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         I1        I2        I3        I4        I5        I6        I7  \\\n",
       "0 -0.045245 -0.501278 -0.594327 -0.157301 -0.162767 -0.206385 -0.064249   \n",
       "1 -0.048792 -0.230945 -0.272394 -0.139403  0.271173 -0.206385  0.743047   \n",
       "2 -0.061206 -0.176607  2.517687 -0.157301 -0.224758 -0.206385 -0.064249   \n",
       "3 -0.020417 -0.548824 -0.379705 -0.154318 -0.224758  0.205918 -0.064249   \n",
       "4  0.000864  0.566469 -0.165084 -0.157301 -0.224758 -0.206385 -0.064249   \n",
       "\n",
       "         I8        I9       I10  ...  C17  C18  C19  C20   C21   C22   C23  \\\n",
       "0 -0.253116 -0.760031 -0.470383  ...    1    2    1    4     5     4     2   \n",
       "1 -0.279201  0.107562 -0.470383  ...    3    2    1  127   193   145  2635   \n",
       "2 -0.281810  2.131947 -0.470383  ...    0    1    1    0     0     0     0   \n",
       "3 -0.281810 -0.615432  1.386036  ...    2    1    1    0  6070  7978  3238   \n",
       "4 -0.281810 -0.543133 -0.470383  ...    0    1    9    2     2     2     0   \n",
       "\n",
       "   C24  C25  C26  \n",
       "0  658    1    4  \n",
       "1  518    2   10  \n",
       "2   18    1    1  \n",
       "3  342    1   14  \n",
       "4  443    2    1  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74d1d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10).to_csv('/wdl_train/infer_test.csv', sep=',', index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59a8b677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wdl2predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wdl2predict.py\n",
    "from hugectr.inference import InferenceParams, CreateInferenceSession\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "slot_size_array = [249058, 19561, 14212, 6890, 18592, 4, 6356, 1254, 52, 226170, 80508, 72308, 11, 2169, 7597, 61, 4, 923, 15, 249619, 168974, 243480, 68212, 9169, 75, 34, 281564, 415262]\n",
    "\n",
    "def wdl_inference(model_name, network_file, dense_file, embedding_file_list, data_file,enable_cache):\n",
    "    CATEGORICAL_COLUMNS=[\"C1_C2\",\"C3_C4\"]+[\"C\" + str(x) for x in range(1, 27)]\n",
    "    CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "    LABEL_COLUMNS = ['label']\n",
    "    shift = np.insert(np.cumsum(slot_size_array), 0, 0)[:-1]\n",
    "    test_df=pd.read_csv(data_file,sep=',')\n",
    "    config_file = network_file\n",
    "    row_ptrs = list(range(0,21))+list(range(0,261))\n",
    "    dense_features =  list(test_df[CONTINUOUS_COLUMNS].values.flatten())\n",
    "    test_df[CATEGORICAL_COLUMNS].astype(np.int64)\n",
    "    embedding_columns = list((test_df[CATEGORICAL_COLUMNS]+shift).values.flatten())\n",
    "\n",
    "    # create parameter server, embedding cache and inference session\n",
    "    inference_params = InferenceParams(model_name = model_name,\n",
    "                                max_batchsize = 64,\n",
    "                                hit_rate_threshold = 0.9,\n",
    "                                dense_model_file = dense_file,\n",
    "                                sparse_model_files = embedding_file_list,\n",
    "                                device_id = 0,\n",
    "                                use_gpu_embedding_cache = enable_cache,\n",
    "                                cache_size_percentage = 0.9,\n",
    "                                i64_input_key = True,\n",
    "                                use_mixed_precision = False\n",
    "                                )\n",
    "    inference_session = CreateInferenceSession(config_file, inference_params)\n",
    "    output = inference_session.predict(dense_features, embedding_columns, row_ptrs)\n",
    "    print(\"WDL multi-embedding table inference result is {}\".format(output))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    model_name = sys.argv[1]\n",
    "    network_file = sys.argv[2]\n",
    "    dense_file = sys.argv[3]\n",
    "    embedding_file_list = str(sys.argv[4]).split(',')\n",
    "    print(embedding_file_list)\n",
    "    data_file = sys.argv[5]\n",
    "  \n",
    "\n",
    "    #wdl_inference(model_name, network_file, dense_file, embedding_file_list, data_file, True,hugectr.Database_t.Redis)\n",
    "    wdl_inference(model_name, network_file, dense_file, embedding_file_list, data_file, True)\n",
    "    #wdl_inference(model_name, network_file, dense_file, embedding_file_list, data_file, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ebceb5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wdl0_sparse_20000.model', 'wdl1_sparse_20000.model']\n",
      "[HCTR][08:02:48.458][WARNING][RK0][main]: default_value_for_each_table.size() is not equal to the number of embedding tables\n",
      "[HCTR][08:02:48.458][INFO][RK0][main]: default_emb_vec_value is not specified using default: 0\n",
      "[HCTR][08:02:48.458][INFO][RK0][main]: default_emb_vec_value is not specified using default: 0\n",
      "====================================================HPS Create====================================================\n",
      "[HCTR][08:02:48.458][INFO][RK0][main]: Creating HashMap CPU database backend...\n",
      "[HCTR][08:02:48.458][DEBUG][RK0][main]: Created blank database backend in local memory!\n",
      "[HCTR][08:02:48.458][INFO][RK0][main]: Volatile DB: initial cache rate = 1\n",
      "[HCTR][08:02:48.458][INFO][RK0][main]: Volatile DB: cache missed embeddings = 0\n",
      "[HCTR][08:02:48.458][DEBUG][RK0][main]: Created raw model loader in local memory!\n",
      "[HCTR][08:02:48.458][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][08:02:49.421][INFO][RK0][main]: Table: hps_et.wdl.sparse_embedding2; cached 693280 / 693280 embeddings in volatile database (HashMapBackend); load: 693280 / 18446744073709551615 (0.00%).\n",
      "[HCTR][08:02:49.421][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][08:02:49.742][INFO][RK0][main]: Table: hps_et.wdl.sparse_embedding1; cached 1445304 / 1445304 embeddings in volatile database (HashMapBackend); load: 1445304 / 18446744073709551615 (0.00%).\n",
      "[HCTR][08:02:49.746][DEBUG][RK0][main]: Real-time subscribers created!\n",
      "[HCTR][08:02:49.746][INFO][RK0][main]: Creating embedding cache in device 0.\n",
      "[HCTR][08:02:49.754][INFO][RK0][main]: Model name: wdl\n",
      "[HCTR][08:02:49.754][INFO][RK0][main]: Number of embedding tables: 2\n",
      "[HCTR][08:02:49.754][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 0.900000\n",
      "[HCTR][08:02:49.754][INFO][RK0][main]: Use I64 input key: True\n",
      "[HCTR][08:02:49.754][INFO][RK0][main]: Configured cache hit rate threshold: 0.900000\n",
      "[HCTR][08:02:49.754][INFO][RK0][main]: The size of thread pool: 80\n",
      "[HCTR][08:02:49.754][INFO][RK0][main]: The size of worker memory pool: 2\n",
      "[HCTR][08:02:49.754][INFO][RK0][main]: The size of refresh memory pool: 1\n",
      "[HCTR][08:02:49.754][INFO][RK0][main]: The refresh percentage : 0.000000\n",
      "[HCTR][08:02:50.713][INFO][RK0][main]: Global seed is 1664419455\n",
      "[HCTR][08:02:50.717][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "[HCTR][08:02:51.618][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.\n",
      "[HCTR][08:02:51.618][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][08:02:51.618][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][08:02:51.619][INFO][RK0][main]: Model name: wdl\n",
      "[HCTR][08:02:51.619][INFO][RK0][main]: Use mixed precision: False\n",
      "[HCTR][08:02:51.619][INFO][RK0][main]: Use cuda graph: True\n",
      "[HCTR][08:02:51.619][INFO][RK0][main]: Max batchsize: 64\n",
      "[HCTR][08:02:51.619][INFO][RK0][main]: Use I64 input key: True\n",
      "[HCTR][08:02:51.619][INFO][RK0][main]: start create embedding for inference\n",
      "[HCTR][08:02:51.619][INFO][RK0][main]: sparse_input name wide_data\n",
      "[HCTR][08:02:51.619][INFO][RK0][main]: sparse_input name deep_data\n",
      "[HCTR][08:02:51.619][INFO][RK0][main]: create embedding for inference success\n",
      "[HCTR][08:02:51.620][INFO][RK0][main]: Inference stage skip BinaryCrossEntropyLoss layer, replaced by Sigmoid layer\n",
      "WDL multi-embedding table inference result is [0.286158949136734, 0.1003180593252182, 0.04812748357653618, 0.12224160134792328, 0.044658202677965164, 0.10072185844182968, 0.08667725324630737, 0.044690731912851334, 0.1073652133345604, 0.10047562420368195]\n",
      "[HCTR][08:02:53.191][INFO][RK0][main]: MPI finalization done.\n"
     ]
    }
   ],
   "source": [
    "!python wdl2predict.py \"wdl\" \"wdl.json\" \"wdl_dense_20000.model\" \"wdl0_sparse_20000.model,wdl1_sparse_20000.model\" \"/wdl_train/infer_test.csv\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
