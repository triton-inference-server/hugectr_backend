{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195a2ae5-1bd3-4e04-bbe2-04760d4f025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e8c871-540b-4e82-aff3-fc5bc340f64e",
   "metadata": {},
   "source": [
    "# 1.Overview\n",
    "\n",
    "In this notebook, we want to provide an tutorial how to train a wdl model using HugeCTR High-level python API. We will use original Criteo dataset as training data\n",
    "\n",
    "1. Overview\n",
    "2. Dataset Preprocessing\n",
    "3. WDL Model Training\n",
    "4. Save the Model Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f37d076-b23e-4bb6-876b-180699ab5ea2",
   "metadata": {},
   "source": [
    "# 2. Dataset Preprocessing\n",
    "## 2.1 Generate training and validation data folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07eb0150-c1ec-425f-8768-d307030c57c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some data folder to store the original and preprocessed data\n",
    "# Standard Libraries\n",
    "import os\n",
    "from time import time\n",
    "import re\n",
    "import shutil\n",
    "import glob\n",
    "import warnings\n",
    "BASE_DIR = \"/wdl_train\"\n",
    "train_path  = os.path.join(BASE_DIR, \"train\")\n",
    "val_path = os.path.join(BASE_DIR, \"val\")\n",
    "CUDA_VISIBLE_DEVICES = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
    "n_workers = len(CUDA_VISIBLE_DEVICES.split(\",\"))\n",
    "frac_size = 0.15\n",
    "allow_multi_gpu = False\n",
    "use_rmm_pool = False\n",
    "max_day = None  # (Optional) -- Limit the dataset to day 0-max_day for debugging\n",
    "\n",
    "if os.path.isdir(train_path):\n",
    "    shutil.rmtree(train_path)\n",
    "os.makedirs(train_path)\n",
    "\n",
    "if os.path.isdir(val_path):\n",
    "    shutil.rmtree(val_path)\n",
    "os.makedirs(val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea33738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n"
     ]
    }
   ],
   "source": [
    "ls -l $train_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3943a3c-7ae9-46da-98ec-7c3a0d479feb",
   "metadata": {},
   "source": [
    "## 2.2 Download the Original Criteo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0213bcaf-6c13-4cdd-ba2c-01722f334579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "wget is already the newest version (1.20.3-1ubuntu1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac0abe2-032a-4f1b-b32b-99738d513cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P $train_path https://storage.googleapis.com/criteo-cail-datasets/day_0.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72fff32e-6ad6-43c2-aecd-500c59d7471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the split data set to training and validation \n",
    "#!gzip -d -c $train_path/day_0.gz > day_0\n",
    "!head -n 45840617 day_0 > $train_path/train.txt\n",
    "!tail -n 2000000 day_0 > $val_path/test.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33165dda-ca3c-4721-a125-24b9745d3699",
   "metadata": {},
   "source": [
    "## 2.3 Preprocessing by NVTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b7644f3-e4b6-40a0-bd35-cd056618481d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /wdl_train/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /wdl_train/preprocess.py\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import glob\n",
    "import time\n",
    "from cudf.io.parquet import ParquetWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures as cf\n",
    "from concurrent.futures import as_completed\n",
    "import shutil\n",
    "\n",
    "import dask_cudf\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "from dask.utils import parse_bytes\n",
    "from dask.delayed import delayed\n",
    "\n",
    "import cudf\n",
    "import rmm\n",
    "import nvtabular as nvt\n",
    "from nvtabular.io import Shuffle\n",
    "from nvtabular.utils import device_mem_size\n",
    "from nvtabular.ops import Categorify, Clip, FillMissing, HashBucket, LambdaOp, Normalize, Rename, Operator, get_embedding_sizes\n",
    "#%load_ext memory_profiler\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(message)s')\n",
    "logging.root.setLevel(logging.NOTSET)\n",
    "logging.getLogger('numba').setLevel(logging.WARNING)\n",
    "logging.getLogger('asyncio').setLevel(logging.WARNING)\n",
    "\n",
    "# define dataset schema\n",
    "CATEGORICAL_COLUMNS=[\"C\" + str(x) for x in range(1, 27)]\n",
    "CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "LABEL_COLUMNS = ['label']\n",
    "COLUMNS =  LABEL_COLUMNS + CONTINUOUS_COLUMNS +  CATEGORICAL_COLUMNS\n",
    "#/samples/criteo mode doesn't have dense features\n",
    "criteo_COLUMN=LABEL_COLUMNS +  CATEGORICAL_COLUMNS\n",
    "#For new feature cross columns\n",
    "CROSS_COLUMNS = []\n",
    "\n",
    "\n",
    "NUM_INTEGER_COLUMNS = 13\n",
    "NUM_CATEGORICAL_COLUMNS = 26\n",
    "NUM_TOTAL_COLUMNS = 1 + NUM_INTEGER_COLUMNS + NUM_CATEGORICAL_COLUMNS\n",
    "\n",
    "\n",
    "# Initialize RMM pool on ALL workers\n",
    "def setup_rmm_pool(client, pool_size):\n",
    "    client.run(rmm.reinitialize, pool_allocator=True, initial_pool_size=pool_size)\n",
    "    return None\n",
    "\n",
    "#compute the partition size with GB\n",
    "def bytesto(bytes, to, bsize=1024):\n",
    "    a = {'k' : 1, 'm': 2, 'g' : 3, 't' : 4, 'p' : 5, 'e' : 6 }\n",
    "    r = float(bytes)\n",
    "    return bytes / (bsize ** a[to])\n",
    "\n",
    "class FeatureCross(Operator):\n",
    "    def __init__(self, dependency):\n",
    "        self.dependency = dependency\n",
    "\n",
    "    def transform(self, columns, gdf):\n",
    "        new_df = type(gdf)()\n",
    "        for col in columns.names:\n",
    "            new_df[col] = gdf[col] + gdf[self.dependency]\n",
    "        return new_df\n",
    "\n",
    "    def dependencies(self):\n",
    "        return [self.dependency]\n",
    "\n",
    "#process the data with NVTabular\n",
    "def process_NVT(args):\n",
    "\n",
    "    if args.feature_cross_list:\n",
    "        feature_pairs = [pair.split(\"_\") for pair in args.feature_cross_list.split(\",\")]\n",
    "        for pair in feature_pairs:\n",
    "            CROSS_COLUMNS.append(pair[0]+'_'+pair[1])\n",
    "\n",
    "\n",
    "    logging.info('NVTabular processing')\n",
    "    train_output = os.path.join(args.out_path, \"train\")\n",
    "    print(\"Training output data: \"+train_output)\n",
    "    val_output = os.path.join(args.out_path, \"val\")\n",
    "    print(\"Validation output data: \"+val_output)\n",
    "    train_input = os.path.join(args.data_path, \"train/train.txt\")\n",
    "    print(\"Training dataset: \"+train_input)\n",
    "    val_input = os.path.join(args.data_path, \"val/test.txt\")\n",
    "    PREPROCESS_DIR_temp_train = os.path.join(args.out_path, 'train/temp-parquet-after-conversion')  \n",
    "    PREPROCESS_DIR_temp_val = os.path.join(args.out_path, \"val/temp-parquet-after-conversion\")\n",
    "    if not os.path.exists(PREPROCESS_DIR_temp_train):\n",
    "        os.makedirs(PREPROCESS_DIR_temp_train)\n",
    "    \n",
    "    if not os.path.exists(PREPROCESS_DIR_temp_val):\n",
    "        os.makedirs(PREPROCESS_DIR_temp_val)\n",
    "    \n",
    "    PREPROCESS_DIR_temp = [PREPROCESS_DIR_temp_train, PREPROCESS_DIR_temp_val]\n",
    "    \n",
    "    \n",
    "\n",
    "    # Make sure we have a clean parquet space for cudf conversion\n",
    "    for one_path in PREPROCESS_DIR_temp:\n",
    "        if os.path.exists(one_path):\n",
    "            shutil.rmtree(one_path)\n",
    "        os.mkdir(one_path)\n",
    "\n",
    "\n",
    "    ## Get Dask Client\n",
    "\n",
    "    # Deploy a Single-Machine Multi-GPU Cluster\n",
    "    device_size = device_mem_size(kind=\"total\")\n",
    "    cluster = None\n",
    "    if args.protocol == \"ucx\":\n",
    "        UCX_TLS = os.environ.get(\"UCX_TLS\", \"tcp,cuda_copy,cuda_ipc,sockcm\")\n",
    "        os.environ[\"UCX_TLS\"] = UCX_TLS\n",
    "        cluster = LocalCUDACluster(\n",
    "            protocol = args.protocol,\n",
    "            CUDA_VISIBLE_DEVICES = args.devices,\n",
    "            n_workers = len(args.devices.split(\",\")),\n",
    "            enable_nvlink=True,\n",
    "            device_memory_limit = int(device_size * args.device_limit_frac),\n",
    "            dashboard_address=\":\" + args.dashboard_port\n",
    "        )\n",
    "    else:\n",
    "        cluster = LocalCUDACluster(\n",
    "            protocol = args.protocol,\n",
    "            n_workers = len(args.devices.split(\",\")),\n",
    "            CUDA_VISIBLE_DEVICES = args.devices,\n",
    "            device_memory_limit = int(device_size * args.device_limit_frac),\n",
    "            dashboard_address=\":\" + args.dashboard_port\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # Create the distributed client\n",
    "    client = Client(cluster)\n",
    "    if args.device_pool_frac > 0.01:\n",
    "        setup_rmm_pool(client, int(args.device_pool_frac*device_size))\n",
    "\n",
    "\n",
    "    #calculate the total processing time\n",
    "    runtime = time.time()\n",
    "\n",
    "    #test dataset without the label feature\n",
    "    if args.dataset_type == 'test':\n",
    "        global LABEL_COLUMNS\n",
    "        LABEL_COLUMNS = []\n",
    "\n",
    "    ##-----------------------------------##\n",
    "    # Dask rapids converts txt to parquet\n",
    "    # Dask cudf dataframe = ddf\n",
    "\n",
    "    ## train/valid txt to parquet\n",
    "    train_valid_paths = [(train_input,PREPROCESS_DIR_temp_train),(val_input,PREPROCESS_DIR_temp_val)]\n",
    "\n",
    "    for input, temp_output in train_valid_paths:\n",
    "\n",
    "        ddf = dask_cudf.read_csv(input,sep='\\t',names=LABEL_COLUMNS + CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS)\n",
    "\n",
    "        ## Convert label col to FP32\n",
    "        if args.parquet_format and args.dataset_type == 'train':\n",
    "            ddf[\"label\"] = ddf['label'].astype('float32')\n",
    "\n",
    "        # Save it as parquet format for better memory usage\n",
    "        ddf.to_parquet(temp_output,header=True)\n",
    "        ##-----------------------------------##\n",
    "\n",
    "    COLUMNS =  LABEL_COLUMNS + CONTINUOUS_COLUMNS + CROSS_COLUMNS + CATEGORICAL_COLUMNS\n",
    "    train_paths = glob.glob(os.path.join(PREPROCESS_DIR_temp_train, \"*.parquet\"))\n",
    "    valid_paths = glob.glob(os.path.join(PREPROCESS_DIR_temp_val, \"*.parquet\"))\n",
    "\n",
    "    categorify_op = Categorify(freq_threshold=args.freq_limit)\n",
    "    cat_features = CATEGORICAL_COLUMNS >> categorify_op\n",
    "    cont_features = CONTINUOUS_COLUMNS >> FillMissing() >> Clip(min_value=0) >> Normalize()\n",
    "    cross_cat_op = Categorify(freq_threshold=args.freq_limit)\n",
    "\n",
    "    features = LABEL_COLUMNS\n",
    "    \n",
    "    if args.criteo_mode == 0:\n",
    "        features += cont_features\n",
    "        if args.feature_cross_list:\n",
    "            feature_pairs = [pair.split(\"_\") for pair in args.feature_cross_list.split(\",\")]\n",
    "            for pair in feature_pairs:\n",
    "                col0 = pair[0]\n",
    "                col1 = pair[1]\n",
    "                features += col0 >> FeatureCross(col1)  >> Rename(postfix=\"_\"+col1) >> cross_cat_op\n",
    "            \n",
    "    features += cat_features\n",
    "\n",
    "    workflow = nvt.Workflow(features, client=client)\n",
    "\n",
    "    logging.info(\"Preprocessing\")\n",
    "\n",
    "    output_format = 'hugectr'\n",
    "    if args.parquet_format:\n",
    "        output_format = 'parquet'\n",
    "\n",
    "    # just for /samples/criteo model\n",
    "    train_ds_iterator = nvt.Dataset(train_paths, engine='parquet', part_size=int(args.part_mem_frac * device_size))\n",
    "    valid_ds_iterator = nvt.Dataset(valid_paths, engine='parquet', part_size=int(args.part_mem_frac * device_size))\n",
    "\n",
    "    shuffle = None\n",
    "    if args.shuffle == \"PER_WORKER\":\n",
    "        shuffle = nvt.io.Shuffle.PER_WORKER\n",
    "    elif args.shuffle == \"PER_PARTITION\":\n",
    "        shuffle = nvt.io.Shuffle.PER_PARTITION\n",
    "\n",
    "    logging.info('Train Datasets Preprocessing.....')\n",
    "\n",
    "    dict_dtypes = {}\n",
    "    for col in CATEGORICAL_COLUMNS:\n",
    "        dict_dtypes[col] = np.int64\n",
    "    if not args.criteo_mode:\n",
    "        for col in CONTINUOUS_COLUMNS:\n",
    "            dict_dtypes[col] = np.float32\n",
    "    for col in CROSS_COLUMNS:\n",
    "        dict_dtypes[col] = np.int64\n",
    "    for col in LABEL_COLUMNS:\n",
    "        dict_dtypes[col] = np.float32\n",
    "    \n",
    "    conts = CONTINUOUS_COLUMNS if not args.criteo_mode else []\n",
    "    \n",
    "    workflow.fit(train_ds_iterator)\n",
    "    \n",
    "    if output_format == 'hugectr':\n",
    "        workflow.transform(train_ds_iterator).to_hugectr(\n",
    "                cats=CATEGORICAL_COLUMNS + CROSS_COLUMNS,\n",
    "                conts=conts,\n",
    "                labels=LABEL_COLUMNS,\n",
    "                output_path=train_output,\n",
    "                shuffle=shuffle,\n",
    "                out_files_per_proc=args.out_files_per_proc,\n",
    "                num_threads=args.num_io_threads)\n",
    "    else:\n",
    "        workflow.transform(train_ds_iterator).to_parquet(\n",
    "                output_path=train_output,\n",
    "                dtypes=dict_dtypes,\n",
    "                cats=CATEGORICAL_COLUMNS + CROSS_COLUMNS,\n",
    "                conts=conts,\n",
    "                labels=LABEL_COLUMNS,\n",
    "                shuffle=shuffle,\n",
    "                out_files_per_proc=args.out_files_per_proc,\n",
    "                num_threads=args.num_io_threads)\n",
    "        \n",
    "        \n",
    "        \n",
    "    ###Getting slot size###    \n",
    "    #--------------------##\n",
    "    embeddings_dict_cat = categorify_op.get_embedding_sizes(CATEGORICAL_COLUMNS)\n",
    "    embeddings_dict_cross = cross_cat_op.get_embedding_sizes(CROSS_COLUMNS)\n",
    "    embeddings = [embeddings_dict_cat[c][0] for c in CATEGORICAL_COLUMNS] + [embeddings_dict_cross[c][0] for c in CROSS_COLUMNS]\n",
    "    \n",
    "    print(embeddings)\n",
    "    ##--------------------##\n",
    "\n",
    "    logging.info('Valid Datasets Preprocessing.....')\n",
    "\n",
    "    if output_format == 'hugectr':\n",
    "        workflow.transform(valid_ds_iterator).to_hugectr(\n",
    "                cats=CATEGORICAL_COLUMNS + CROSS_COLUMNS,\n",
    "                conts=conts,\n",
    "                labels=LABEL_COLUMNS,\n",
    "                output_path=val_output,\n",
    "                shuffle=shuffle,\n",
    "                out_files_per_proc=args.out_files_per_proc,\n",
    "                num_threads=args.num_io_threads)\n",
    "    else:\n",
    "        workflow.transform(valid_ds_iterator).to_parquet(\n",
    "                output_path=val_output,\n",
    "                dtypes=dict_dtypes,\n",
    "                cats=CATEGORICAL_COLUMNS + CROSS_COLUMNS,\n",
    "                conts=conts,\n",
    "                labels=LABEL_COLUMNS,\n",
    "                shuffle=shuffle,\n",
    "                out_files_per_proc=args.out_files_per_proc,\n",
    "                num_threads=args.num_io_threads)\n",
    "\n",
    "    embeddings_dict_cat = categorify_op.get_embedding_sizes(CATEGORICAL_COLUMNS)\n",
    "    embeddings_dict_cross = cross_cat_op.get_embedding_sizes(CROSS_COLUMNS)\n",
    "    embeddings = [embeddings_dict_cat[c][0] for c in CATEGORICAL_COLUMNS] + [embeddings_dict_cross[c][0] for c in CROSS_COLUMNS]\n",
    "    \n",
    "    print(embeddings)\n",
    "    ##--------------------##\n",
    "\n",
    "    ## Shutdown clusters\n",
    "    client.close()\n",
    "    logging.info('NVTabular processing done')\n",
    "\n",
    "    runtime = time.time() - runtime\n",
    "\n",
    "    print(\"\\nDask-NVTabular Criteo Preprocessing\")\n",
    "    print(\"--------------------------------------\")\n",
    "    print(f\"data_path          | {args.data_path}\")\n",
    "    print(f\"output_path        | {args.out_path}\")\n",
    "    print(f\"partition size     | {'%.2f GB'%bytesto(int(args.part_mem_frac * device_size),'g')}\")\n",
    "    print(f\"protocol           | {args.protocol}\")\n",
    "    print(f\"device(s)          | {args.devices}\")\n",
    "    print(f\"rmm-pool-frac      | {(args.device_pool_frac)}\")\n",
    "    print(f\"out-files-per-proc | {args.out_files_per_proc}\")\n",
    "    print(f\"num_io_threads     | {args.num_io_threads}\")\n",
    "    print(f\"shuffle            | {args.shuffle}\")\n",
    "    print(\"======================================\")\n",
    "    print(f\"Runtime[s]         | {runtime}\")\n",
    "    print(\"======================================\\n\")\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=(\"Multi-GPU Criteo Preprocessing\"))\n",
    "\n",
    "    #\n",
    "    # System Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\"--data_path\", type=str, help=\"Input dataset path (Required)\")\n",
    "    parser.add_argument(\"--out_path\", type=str, help=\"Directory path to write output (Required)\")\n",
    "    parser.add_argument(\n",
    "        \"-d\",\n",
    "        \"--devices\",\n",
    "        default=os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\"),\n",
    "        type=str,\n",
    "        help='Comma-separated list of visible devices (e.g. \"0,1,2,3\"). '\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-p\",\n",
    "        \"--protocol\",\n",
    "        choices=[\"tcp\", \"ucx\"],\n",
    "        default=\"tcp\",\n",
    "        type=str,\n",
    "        help=\"Communication protocol to use (Default 'tcp')\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device_limit_frac\",\n",
    "        default=0.5,\n",
    "        type=float,\n",
    "        help=\"Worker device-memory limit as a fraction of GPU capacity (Default 0.8). \"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device_pool_frac\",\n",
    "        default=0.9,\n",
    "        type=float,\n",
    "        help=\"RMM pool size for each worker  as a fraction of GPU capacity (Default 0.9). \"\n",
    "        \"The RMM pool frac is the same for all GPUs, make sure each one has enough memory size\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_io_threads\",\n",
    "        default=0,\n",
    "        type=int,\n",
    "        help=\"Number of threads to use when writing output data (Default 0). \"\n",
    "        \"If 0 is specified, multi-threading will not be used for IO.\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Data-Decomposition Parameters\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--part_mem_frac\",\n",
    "        default=0.125,\n",
    "        type=float,\n",
    "        help=\"Maximum size desired for dataset partitions as a fraction \"\n",
    "        \"of GPU capacity (Default 0.125)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out_files_per_proc\",\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=\"Number of output files to write on each worker (Default 1)\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Preprocessing Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"-f\",\n",
    "        \"--freq_limit\",\n",
    "        default=0,\n",
    "        type=int,\n",
    "        help=\"Frequency limit for categorical encoding (Default 0)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-s\",\n",
    "        \"--shuffle\",\n",
    "        choices=[\"PER_WORKER\", \"PER_PARTITION\", \"NONE\"],\n",
    "        default=\"PER_PARTITION\",\n",
    "        help=\"Shuffle algorithm to use when writing output data to disk (Default PER_PARTITION)\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--feature_cross_list\", default=None, type=str, help=\"List of feature crossing cols (e.g. C1_C2, C3_C4)\"\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Diagnostics Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--profile\",\n",
    "        metavar=\"PATH\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"Specify a file path to export a Dask profile report (E.g. dask-report.html).\"\n",
    "        \"If this option is excluded from the command, not profile will be exported\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dashboard_port\",\n",
    "        default=\"8787\",\n",
    "        type=str,\n",
    "        help=\"Specify the desired port of Dask's diagnostics-dashboard (Default `3787`). \"\n",
    "        \"The dashboard will be hosted at http://<IP>:<PORT>/status\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Format\n",
    "    #\n",
    "\n",
    "    parser.add_argument('--criteo_mode', type=int, default=0)\n",
    "    parser.add_argument('--parquet_format', type=int, default=1)\n",
    "    parser.add_argument('--dataset_type', type=str, default='train')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.n_workers = len(args.devices.split(\",\"))\n",
    "    return args\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args = parse_args()\n",
    "\n",
    "    process_NVT(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97aae996-9677-40ae-a976-6edb51fcc61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d57560b-c6cf-418e-b45b-3b6722d29a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-29 05:26:15,779 NVTabular processing\n",
      "Training output data: /wdl_train/train\n",
      "Validation output data: /wdl_train/val\n",
      "Training dataset: /wdl_train/train/train.txt\n",
      "distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2021-11-29 05:26:33,989 Preprocessing\n",
      "2021-11-29 05:26:34,194 Train Datasets Preprocessing.....\n",
      "/usr/local/lib/python3.8/dist-packages/numba/cuda/compiler.py:865: NumbaPerformanceWarning: Grid size (1) < 2 * SM count (160) will likely result in GPU under utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "[249058, 19561, 14212, 6890, 18592, 4, 6356, 1254, 52, 226170, 80508, 72308, 11, 2169, 7597, 61, 4, 923, 15, 249619, 168974, 243480, 68212, 9169, 75, 34, 278018, 415262]\n",
      "2021-11-29 05:27:42,216 Valid Datasets Preprocessing.....\n",
      "[249058, 19561, 14212, 6890, 18592, 4, 6356, 1254, 52, 226170, 80508, 72308, 11, 2169, 7597, 61, 4, 923, 15, 249619, 168974, 243480, 68212, 9169, 75, 34, 278018, 415262]\n",
      "2021-11-29 05:27:44,134 NVTabular processing done\n",
      "\n",
      "Dask-NVTabular Criteo Preprocessing\n",
      "--------------------------------------\n",
      "data_path          | /wdl_train/\n",
      "output_path        | /wdl_train/\n",
      "partition size     | 1.97 GB\n",
      "protocol           | tcp\n",
      "device(s)          | 0\n",
      "rmm-pool-frac      | 0.5\n",
      "out-files-per-proc | 1\n",
      "num_io_threads     | 2\n",
      "shuffle            | PER_PARTITION\n",
      "======================================\n",
      "Runtime[s]         | 85.47145938873291\n",
      "======================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 /wdl_train/preprocess.py --data_path /wdl_train/ --out_path /wdl_train/ --freq_limit 6 --feature_cross_list C1_C2,C3_C4 --device_pool_frac 0.5  --devices \"0\" --num_io_threads 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20924837",
   "metadata": {},
   "source": [
    "### 2.4 Checke the preprocessed training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc6e4663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 14542626\r\n",
      "-rw-r--r-- 1 root root          34 Nov 29 05:27 _file_list.txt\r\n",
      "-rw-r--r-- 1 root root     8554464 Nov 29 05:27 _hugectr.keyset\r\n",
      "-rw-r--r-- 1 root root      471772 Nov 29 05:27 _metadata\r\n",
      "-rw-r--r-- 1 root root        1510 Nov 29 05:27 _metadata.json\r\n",
      "-rw-r--r-- 1 root root  3332773998 Nov 29 05:27 part_0.parquet\r\n",
      "-rw-r--r-- 1 root root       21459 Nov 29 05:27 schema.pbtxt\r\n",
      "drwxr-xr-x 2 root root        4096 Nov 29 05:26 temp-parquet-after-conversion\r\n",
      "-rw-r--r-- 1 root root 11549710546 Nov 29 03:50 train.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ll /wdl_train/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0455ab5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>...</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "      <th>C22</th>\n",
       "      <th>C23</th>\n",
       "      <th>C24</th>\n",
       "      <th>C25</th>\n",
       "      <th>C26</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.031058</td>\n",
       "      <td>0.350474</td>\n",
       "      <td>0.264160</td>\n",
       "      <td>-0.091675</td>\n",
       "      <td>0.023207</td>\n",
       "      <td>0.068484</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.224423</td>\n",
       "      <td>-0.760031</td>\n",
       "      <td>1.386036</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.059432</td>\n",
       "      <td>-0.217361</td>\n",
       "      <td>-0.594327</td>\n",
       "      <td>-0.157301</td>\n",
       "      <td>-0.147269</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.279201</td>\n",
       "      <td>-0.760031</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>210</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         I1        I2        I3        I4        I5        I6        I7  \\\n",
       "0 -0.031058  0.350474  0.264160 -0.091675  0.023207  0.068484 -0.064249   \n",
       "1 -0.059432 -0.217361 -0.594327 -0.157301 -0.147269 -0.206385 -0.064249   \n",
       "\n",
       "         I8        I9       I10  ...  C18  C19  C20  C21  C22  C23  C24  C25  \\\n",
       "0 -0.224423 -0.760031  1.386036  ...    1    2    1    1    1    1   25    1   \n",
       "1 -0.279201 -0.760031 -0.470383  ...    2    2    1    1    1    1  210    1   \n",
       "\n",
       "   C26  label  \n",
       "0    2    0.0  \n",
       "1    2    0.0  \n",
       "\n",
       "[2 rows x 42 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"/wdl_train/train/part_0.parquet\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c7026-017e-46a2-ae74-bc820e60cc6e",
   "metadata": {},
   "source": [
    "## 3. WDL Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3d01115-b0eb-4b7f-82d2-1d4a457d3170",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile './model.py'\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 4000,\n",
    "                              batchsize_eval = 2720,\n",
    "                              batchsize = 2720,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[2]],\n",
    "                              repeat_dataset = True,\n",
    "                              i64_input_key = True)\n",
    "\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Parquet,\n",
    "                                  source = [\"./train/_file_list.txt\"],\n",
    "                                  eval_source = \"./val/_file_list.txt\",\n",
    "                                  check_type = hugectr.Check_t.Non,\n",
    "                                  slot_size_array = [278018, 415262,249058, 19561, 14212, 6890, 18592, 4, 6356, 1254, 52, 226170, 80508, 72308, 11, 2169, 7597, 61, 4, 923, 15, 249619, 168974, 243480, 68212, 9169, 75, 34])\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam,\n",
    "                                    update_type = hugectr.Update_t.Global,\n",
    "                                    beta1 = 0.9,\n",
    "                                    beta2 = 0.999,\n",
    "                                    epsilon = 0.0000001)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"wide_data\", 1, True, 2),\n",
    "                        hugectr.DataReaderSparseParam(\"deep_data\", 2, False, 26)]))\n",
    "\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 8,\n",
    "                            embedding_vec_size = 1,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding2\",\n",
    "                            bottom_name = \"wide_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 135,\n",
    "                            embedding_vec_size = 16,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"deep_data\",\n",
    "                            optimizer = optimizer))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=416))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding2\"],\n",
    "                            top_names = [\"reshape2\"],\n",
    "                            leading_dim=2))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReduceSum,\n",
    "                            bottom_names = [\"reshape2\"],\n",
    "                            top_names = [\"wide_redn\"],\n",
    "                            axis = 1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"dense\"],\n",
    "                            top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"dropout2\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Add,\n",
    "                            bottom_names = [\"fc3\", \"wide_redn\"],\n",
    "                            top_names = [\"add1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"add1\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.fit(max_iter = 21000, display = 1000, eval_interval = 4000, snapshot = 20000, snapshot_prefix = \"wdl\")\n",
    "model.graph_to_json(graph_config_file = \"wdl.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fc7ee08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HugeCTR Version: 3.2\n",
      "====================================================Model Init=====================================================\n",
      "[HUGECTR][10:15:54][WARNING][RANK0]: The model name is not specified when creating the solver.\n",
      "[HUGECTR][10:15:54][INFO][RANK0]: Global seed is 1845651137\n",
      "[HUGECTR][10:15:54][INFO][RANK0]: Device to NUMA mapping:\n",
      "  GPU 2 ->  node 0\n",
      "\n",
      "[HUGECTR][10:15:56][WARNING][RANK0]: Peer-to-peer access cannot be fully enabled.\n",
      "[HUGECTR][10:15:56][INFO][RANK0]: Start all2all warmup\n",
      "[HUGECTR][10:15:56][INFO][RANK0]: End all2all warmup\n",
      "[HUGECTR][10:15:56][INFO][RANK0]: Using All-reduce algorithm: NCCL\n",
      "[HUGECTR][10:15:56][INFO][RANK0]: Device 2: Tesla V100-SXM2-16GB\n",
      "[HUGECTR][10:15:56][INFO][RANK0]: num of DataReader workers: 1\n",
      "[HUGECTR][10:15:56][INFO][RANK0]: Vocabulary size: 2138588\n",
      "[HUGECTR][10:15:56][INFO][RANK0]: max_vocabulary_size_per_gpu_=2097152\n",
      "[HUGECTR][10:15:56][INFO][RANK0]: max_vocabulary_size_per_gpu_=2211840\n",
      "[HUGECTR][10:15:56][INFO][RANK0]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "[HUGECTR][10:16:05][INFO][RANK0]: gpu0 start to init embedding\n",
      "[HUGECTR][10:16:05][INFO][RANK0]: gpu0 init embedding done\n",
      "[HUGECTR][10:16:05][INFO][RANK0]: gpu0 start to init embedding\n",
      "[HUGECTR][10:16:05][INFO][RANK0]: gpu0 init embedding done\n",
      "[HUGECTR][10:16:05][INFO][RANK0]: Starting AUC NCCL warm-up\n",
      "[HUGECTR][10:16:05][INFO][RANK0]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "label                                   Dense                         Sparse                        \n",
      "label                                   dense                          wide_data,deep_data           \n",
      "(None, 1)                               (None, 13)                              \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "DistributedSlotSparseEmbeddingHash      wide_data                     sparse_embedding2             (None, 2, 1)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      deep_data                     sparse_embedding1             (None, 26, 16)                \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 416)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding2             reshape2                      (None, 2)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReduceSum                               reshape2                      wide_redn                     (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Concat                                  reshape1                      concat1                       (None, 429)                   \n",
      "                                        dense                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            concat1                       fc1                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout1                      fc2                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc2                           relu2                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu2                         dropout2                      (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout2                      fc3                           (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Add                                     fc3                           add1                          (None, 1)                     \n",
      "                                        wide_redn                                                                                 \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  add1                          loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[HUGECTR][10:16:05][INFO][RANK0]: Use non-epoch mode with number of iterations: 21000\n",
      "[HUGECTR][10:16:05][INFO][RANK0]: Training batchsize: 2720, evaluation batchsize: 2720\n",
      "[HUGECTR][10:16:05][INFO][RANK0]: Evaluation interval: 4000, snapshot interval: 20000\n",
      "[HUGECTR][10:16:05][INFO][RANK0]: Dense network trainable: True\n",
      "[HUGECTR][10:16:05][INFO][RANK0]: Sparse embedding sparse_embedding1 trainable: True\n",
      "[HUGECTR][10:16:05][INFO][RANK0]: Sparse embedding sparse_embedding2 trainable: True\n",
      "[HUGECTR][10:16:05][INFO][RANK0]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HUGECTR][10:16:05][INFO][RANK0]: lr: 0.001000, warmup_steps: 1, end_lr: 0.000000\n",
      "[HUGECTR][10:16:05][INFO][RANK0]: decay_start: 0, decay_steps: 1, decay_power: 2.000000\n",
      "[HUGECTR][10:16:05][INFO][RANK0]: Training source file: ./train/_file_list.txt\n",
      "[HUGECTR][10:16:05][INFO][RANK0]: Evaluation source file: ./val/_file_list.txt\n",
      "[HUGECTR][10:16:10][INFO][RANK0]: Iter: 1000 Time(1000 iters): 4.799694s Loss: 0.108714 lr:0.001000\n",
      "[HUGECTR][10:16:15][INFO][RANK0]: Iter: 2000 Time(1000 iters): 4.710662s Loss: 0.114875 lr:0.001000\n",
      "[HUGECTR][10:16:19][INFO][RANK0]: Iter: 3000 Time(1000 iters): 4.697201s Loss: 0.138273 lr:0.001000\n",
      "[HUGECTR][10:16:24][INFO][RANK0]: Iter: 4000 Time(1000 iters): 4.704950s Loss: 0.117671 lr:0.001000\n",
      "[HUGECTR][10:16:29][INFO][RANK0]: Evaluation, AUC: 0.759860\n",
      "[HUGECTR][10:16:29][INFO][RANK0]: Eval Time for 4000 iters: 4.613376s\n",
      "[HUGECTR][10:16:33][INFO][RANK0]: Iter: 5000 Time(1000 iters): 9.402435s Loss: 0.133574 lr:0.001000\n",
      "[HUGECTR][10:16:38][INFO][RANK0]: Iter: 6000 Time(1000 iters): 4.806117s Loss: 0.106559 lr:0.001000\n",
      "[HUGECTR][10:16:43][INFO][RANK0]: Iter: 7000 Time(1000 iters): 4.826235s Loss: 0.124802 lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUGECTR][10:16:48][INFO][RANK0]: Iter: 8000 Time(1000 iters): 4.850043s Loss: 0.137412 lr:0.001000\n",
      "[HUGECTR][10:16:53][INFO][RANK0]: Evaluation, AUC: 0.764927\n",
      "[HUGECTR][10:16:53][INFO][RANK0]: Eval Time for 4000 iters: 4.632716s\n",
      "[HUGECTR][10:16:57][INFO][RANK0]: Iter: 9000 Time(1000 iters): 9.446955s Loss: 0.119597 lr:0.001000\n",
      "[HUGECTR][10:17:02][INFO][RANK0]: Iter: 10000 Time(1000 iters): 4.802527s Loss: 0.125044 lr:0.001000\n",
      "[HUGECTR][10:17:07][INFO][RANK0]: Iter: 11000 Time(1000 iters): 4.824831s Loss: 0.124651 lr:0.001000\n",
      "[HUGECTR][10:17:12][INFO][RANK0]: Iter: 12000 Time(1000 iters): 4.800592s Loss: 0.138358 lr:0.001000\n",
      "[HUGECTR][10:17:17][INFO][RANK0]: Evaluation, AUC: 0.766296\n",
      "[HUGECTR][10:17:17][INFO][RANK0]: Eval Time for 4000 iters: 4.642774s\n",
      "[HUGECTR][10:17:21][INFO][RANK0]: Iter: 13000 Time(1000 iters): 9.470000s Loss: 0.112785 lr:0.001000\n",
      "[HUGECTR][10:17:26][INFO][RANK0]: Iter: 14000 Time(1000 iters): 4.792785s Loss: 0.132969 lr:0.001000\n",
      "[HUGECTR][10:17:31][INFO][RANK0]: Iter: 15000 Time(1000 iters): 4.815955s Loss: 0.126141 lr:0.001000\n",
      "[HUGECTR][10:17:36][INFO][RANK0]: Iter: 16000 Time(1000 iters): 4.806740s Loss: 0.152352 lr:0.001000\n",
      "[HUGECTR][10:17:40][INFO][RANK0]: Evaluation, AUC: 0.768923\n",
      "[HUGECTR][10:17:40][INFO][RANK0]: Eval Time for 4000 iters: 4.632523s\n",
      "[HUGECTR][10:17:45][INFO][RANK0]: Iter: 17000 Time(1000 iters): 9.484565s Loss: 0.117772 lr:0.001000\n",
      "[HUGECTR][10:17:50][INFO][RANK0]: Iter: 18000 Time(1000 iters): 4.695520s Loss: 0.116997 lr:0.001000\n",
      "[HUGECTR][10:17:55][INFO][RANK0]: Iter: 19000 Time(1000 iters): 4.722065s Loss: 0.119962 lr:0.001000\n",
      "[HUGECTR][10:17:59][INFO][RANK0]: Iter: 20000 Time(1000 iters): 4.703395s Loss: 0.126523 lr:0.001000\n",
      "[HUGECTR][10:18:04][INFO][RANK0]: Evaluation, AUC: 0.765821\n",
      "[HUGECTR][10:18:04][INFO][RANK0]: Eval Time for 4000 iters: 4.617699s\n",
      "[HUGECTR][10:18:04][INFO][RANK0]: Rank0: Write hash table to file\n",
      "[HUGECTR][10:18:04][INFO][RANK0]: Rank0: Write hash table to file\n",
      "[HUGECTR][10:18:04][INFO][RANK0]: Dumping sparse weights to files, successful\n",
      "[HUGECTR][10:18:04][INFO][RANK0]: Rank0: Write optimzer state to file\n",
      "[HUGECTR][10:18:04][INFO][RANK0]: Done\n",
      "[HUGECTR][10:18:04][INFO][RANK0]: Rank0: Write optimzer state to file\n",
      "[HUGECTR][10:18:04][INFO][RANK0]: Done\n",
      "[HUGECTR][10:18:04][INFO][RANK0]: Rank0: Write optimzer state to file\n",
      "[HUGECTR][10:18:04][INFO][RANK0]: Done\n",
      "[HUGECTR][10:18:04][INFO][RANK0]: Rank0: Write optimzer state to file\n",
      "[HUGECTR][10:18:04][INFO][RANK0]: Done\n",
      "[HUGECTR][10:18:04][INFO][RANK0]: Dumping sparse optimzer states to files, successful\n",
      "[HUGECTR][10:18:04][INFO][RANK0]: Dumping dense weights to file, successful\n",
      "[HUGECTR][10:18:04][INFO][RANK0]: Dumping dense optimizer states to file, successful\n",
      "[HUGECTR][10:18:04][INFO][RANK0]: Dumping untrainable weights to file, successful\n",
      "[HUGECTR][10:18:09][INFO][RANK0]: Finish 21000 iterations with batchsize: 2720 in 123.94s.\n",
      "[HUGECTR][10:18:09][INFO][RANK0]: Save the model graph to wdl.json successfully\n"
     ]
    }
   ],
   "source": [
    "!python ./model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f40e50c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 48913925\r\n",
      "-rw-r--r-- 1 38458 dip        66603 Nov 30 10:17 HugeCTR_WDL_Training.ipynb\r\n",
      "drwxr-xr-x 2 root  root        4096 Nov 29 05:27 categories\r\n",
      "drwxr-xr-x 3 root  root        4096 Nov 29 05:27 dask-worker-space\r\n",
      "-rw-r--r-- 1 38458 dip  49769814062 Nov 29 03:34 day_0\r\n",
      "-rw-r--r-- 1 root  root        2461 Nov 30 09:24 infer_test.csv\r\n",
      "-rw-r--r-- 1 root  root        5400 Nov 30 10:15 model.py\r\n",
      "-rw-r--r-- 1 root  root       14633 Nov 29 05:25 preprocess.py\r\n",
      "drwxr-xr-x 3 root  root        4096 Nov 29 05:27 train\r\n",
      "drwxr-xr-x 3 root  root        4096 Nov 29 05:27 val\r\n",
      "-rw-r--r-- 1 root  root        3731 Nov 30 10:18 wdl.json\r\n",
      "-rw-r--r-- 1 root  root    16777216 Nov 30 10:18 wdl0_opt_sparse_20000.model\r\n",
      "drwxr-xr-x 2 root  root        4096 Nov 29 05:49 wdl0_sparse_20000.model\r\n",
      "-rw-r--r-- 1 root  root   283115520 Nov 30 10:18 wdl1_opt_sparse_20000.model\r\n",
      "drwxr-xr-x 2 root  root        4096 Nov 29 05:49 wdl1_sparse_20000.model\r\n",
      "-rwxr-xrwx 1 root  root        2497 Nov 29 09:58 wdl2predict.py\r\n",
      "-rw-r--r-- 1 root  root     5963780 Nov 30 10:18 wdl_dense_20000.model\r\n",
      "-rw-r--r-- 1 root  root    11927560 Nov 30 10:18 wdl_opt_dense_20000.model\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a300e4",
   "metadata": {},
   "source": [
    "## 4. Inference Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80c436c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 645762\r\n",
      "-rw-r--r-- 1 root root        32 Nov 29 05:27 _file_list.txt\r\n",
      "-rw-r--r-- 1 root root   8554464 Nov 29 05:27 _hugectr.keyset\r\n",
      "-rw-r--r-- 1 root root     22726 Nov 29 05:27 _metadata\r\n",
      "-rw-r--r-- 1 root root      1509 Nov 29 05:27 _metadata.json\r\n",
      "-rw-r--r-- 1 root root 142825257 Nov 29 05:27 part_0.parquet\r\n",
      "-rw-r--r-- 1 root root     21459 Nov 29 05:27 schema.pbtxt\r\n",
      "drwxr-xr-x 2 root root      4096 Nov 29 05:26 temp-parquet-after-conversion\r\n",
      "-rw-r--r-- 1 root root 509766965 Nov 29 03:50 test.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /wdl_train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44ff2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"/wdl_train/val/part_0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5998dcc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>...</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "      <th>C22</th>\n",
       "      <th>C23</th>\n",
       "      <th>C24</th>\n",
       "      <th>C25</th>\n",
       "      <th>C26</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.055886</td>\n",
       "      <td>-0.548824</td>\n",
       "      <td>-0.272394</td>\n",
       "      <td>-0.157301</td>\n",
       "      <td>-0.224758</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>0.096421</td>\n",
       "      <td>-0.543133</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3856</td>\n",
       "      <td>4891</td>\n",
       "      <td>4119</td>\n",
       "      <td>143</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.059432</td>\n",
       "      <td>-0.380376</td>\n",
       "      <td>-0.272394</td>\n",
       "      <td>5.629719</td>\n",
       "      <td>-0.224758</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.279201</td>\n",
       "      <td>-0.253935</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>327</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.059432</td>\n",
       "      <td>-0.539315</td>\n",
       "      <td>-0.594327</td>\n",
       "      <td>-0.142386</td>\n",
       "      <td>-0.193763</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.023569</td>\n",
       "      <td>-0.687732</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2439</td>\n",
       "      <td>41980</td>\n",
       "      <td>349</td>\n",
       "      <td>3549</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.059432</td>\n",
       "      <td>-0.463242</td>\n",
       "      <td>-0.594327</td>\n",
       "      <td>-0.097641</td>\n",
       "      <td>-0.209261</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.219206</td>\n",
       "      <td>-0.687732</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4024</td>\n",
       "      <td>3677</td>\n",
       "      <td>4287</td>\n",
       "      <td>565</td>\n",
       "      <td>306</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022145</td>\n",
       "      <td>-0.509429</td>\n",
       "      <td>-0.379705</td>\n",
       "      <td>-0.151335</td>\n",
       "      <td>-0.162767</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.281810</td>\n",
       "      <td>-0.470833</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>40847</td>\n",
       "      <td>3862</td>\n",
       "      <td>41562</td>\n",
       "      <td>1066</td>\n",
       "      <td>132</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         I1        I2        I3        I4        I5        I6        I7  \\\n",
       "0 -0.055886 -0.548824 -0.272394 -0.157301 -0.224758 -0.206385 -0.064249   \n",
       "1 -0.059432 -0.380376 -0.272394  5.629719 -0.224758 -0.206385 -0.064249   \n",
       "2 -0.059432 -0.539315 -0.594327 -0.142386 -0.193763 -0.206385 -0.064249   \n",
       "3 -0.059432 -0.463242 -0.594327 -0.097641 -0.209261 -0.206385 -0.064249   \n",
       "4  0.022145 -0.509429 -0.379705 -0.151335 -0.162767 -0.206385 -0.064249   \n",
       "\n",
       "         I8        I9       I10  ...  C18  C19    C20   C21    C22   C23  \\\n",
       "0  0.096421 -0.543133 -0.470383  ...    1    1   3856  4891   4119   143   \n",
       "1 -0.279201 -0.253935 -0.470383  ...    2    1      2     2      2     0   \n",
       "2 -0.023569 -0.687732 -0.470383  ...    1    1      0  2439  41980   349   \n",
       "3 -0.219206 -0.687732 -0.470383  ...    1    1   4024  3677   4287   565   \n",
       "4 -0.281810 -0.470833 -0.470383  ...    2    3  40847  3862  41562  1066   \n",
       "\n",
       "    C24  C25  C26  label  \n",
       "0    50    1    1    0.0  \n",
       "1   327    2    1    0.0  \n",
       "2  3549    6    1    1.0  \n",
       "3   306    4    1    0.0  \n",
       "4   132    2    1    0.0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74d1d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10).to_csv('/wdl_train/infer_test.csv', sep=',', index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59a8b677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /wdl_train/wdl2predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /wdl_train/wdl2predict.py\n",
    "from hugectr.inference import InferenceParams, CreateInferenceSession\n",
    "import hugectr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "def wdl_inference(model_name, network_file, dense_file, embedding_file_list, data_file,enable_cache):\n",
    "    CATEGORICAL_COLUMNS=[\"C1_C2\",\"C3_C4\"]+[\"C\" + str(x) for x in range(1, 27)]\n",
    "    CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "    LABEL_COLUMNS = ['label']\n",
    "    emb_size = [278018, 415262,249058, 19561, 14212, 6890, 18592, 4, 6356, 1254, 52, 226170, 80508, 72308, 11, 2169, 7597, 61, 4, 923, 15, 249619, 168974, 243480, 68212, 9169, 75, 34]\n",
    "    shift = np.insert(np.cumsum(emb_size), 0, 0)[:-1]\n",
    "    test_df=pd.read_csv(data_file,sep=',')\n",
    "    config_file = network_file\n",
    "    row_ptrs = list(range(0,21))+list(range(0,261))\n",
    "    dense_features =  list(test_df[CONTINUOUS_COLUMNS].values.flatten())\n",
    "    test_df[CATEGORICAL_COLUMNS].astype(np.int64)\n",
    "    embedding_columns = list((test_df[CATEGORICAL_COLUMNS]+shift).values.flatten())\n",
    "\n",
    "    # create parameter server, embedding cache and inference session\n",
    "    inference_params = InferenceParams(model_name = model_name,\n",
    "                                max_batchsize = 64,\n",
    "                                hit_rate_threshold = 0.9,\n",
    "                                dense_model_file = dense_file,\n",
    "                                sparse_model_files = embedding_file_list,\n",
    "                                device_id = 0,\n",
    "                                use_gpu_embedding_cache = enable_cache,\n",
    "                                cache_size_percentage = 0.9,\n",
    "                                i64_input_key = True,\n",
    "                                use_mixed_precision = False\n",
    "                                )\n",
    "    inference_session = CreateInferenceSession(config_file, inference_params)\n",
    "    output = inference_session.predict(dense_features, embedding_columns, row_ptrs)\n",
    "    print(\"WDL multi-embedding table inference result is {}\".format(output))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    model_name = sys.argv[1]\n",
    "    network_file = sys.argv[2]\n",
    "    dense_file = sys.argv[3]\n",
    "    embedding_file_list = str(sys.argv[4]).split(',')\n",
    "    print(embedding_file_list)\n",
    "    data_file = sys.argv[5]\n",
    "  \n",
    "\n",
    "    #wdl_inference(model_name, network_file, dense_file, embedding_file_list, data_file, True,hugectr.Database_t.Redis)\n",
    "    wdl_inference(model_name, network_file, dense_file, embedding_file_list, data_file, True)\n",
    "    #wdl_inference(model_name, network_file, dense_file, embedding_file_list, data_file, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebceb5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/wdl_train/wdl0_sparse_20000.model', '/wdl_train/wdl1_sparse_20000.model']\n",
      "[HUGECTR][10:45:05][INFO][RANK0]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[HUGECTR][10:45:05][INFO][RANK0]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[HUGECTR][10:45:05][INFO][RANK0]: Creating ParallelHashMap CPU database backend...\n",
      "[HUGECTR][10:45:05][INFO][RANK0]: Created parallel (16 partitions) blank database backend in local memory!\n",
      "[HUGECTR][10:45:05][INFO][RANK0]: ParallelHashMap backend. Table: hctr_et.wdl.sparse_embedding2. Inserted 664320 / 664320 pairs.\n",
      "[HUGECTR][10:45:05][INFO][RANK0]: Table: hctr_et.wdl.sparse_embedding2; cached 664320 / 664320 embeddings in CPU memory database!\n",
      "[HUGECTR][10:45:05][INFO][RANK0]: ParallelHashMap backend. Table: hctr_et.wdl.sparse_embedding1. Inserted 1445304 / 1445304 pairs.\n",
      "[HUGECTR][10:45:05][INFO][RANK0]: Table: hctr_et.wdl.sparse_embedding1; cached 1445304 / 1445304 embeddings in CPU memory database!\n",
      "[HUGECTR][10:45:05][DEBUG][RANK0]: Real-time subscribers created!\n",
      "[HUGECTR][10:45:05][INFO][RANK0]: Create embedding cache in device 0.\n",
      "[HUGECTR][10:45:05][INFO][RANK0]: Use GPU embedding cache: True, cache size percentage: 0.900000\n",
      "[HUGECTR][10:45:05][INFO][RANK0]: Configured cache hit rate threshold: 0.900000\n",
      "[HUGECTR][10:45:06][INFO][RANK0]: Global seed is 2905163689\n",
      "[HUGECTR][10:45:06][INFO][RANK0]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[HUGECTR][10:45:07][WARNING][RANK0]: Peer-to-peer access cannot be fully enabled.\n",
      "[HUGECTR][10:45:07][INFO][RANK0]: Start all2all warmup\n",
      "[HUGECTR][10:45:07][INFO][RANK0]: End all2all warmup\n",
      "[HUGECTR][10:45:07][INFO][RANK0]: Model name: wdl\n",
      "[HUGECTR][10:45:07][INFO][RANK0]: Use mixed precision: False\n",
      "[HUGECTR][10:45:07][INFO][RANK0]: Use cuda graph: True\n",
      "[HUGECTR][10:45:07][INFO][RANK0]: Max batchsize: 64\n",
      "[HUGECTR][10:45:07][INFO][RANK0]: Use I64 input key: True\n",
      "[HUGECTR][10:45:07][INFO][RANK0]: start create embedding for inference\n",
      "[HUGECTR][10:45:07][INFO][RANK0]: sparse_input name wide_data\n",
      "[HUGECTR][10:45:07][INFO][RANK0]: sparse_input name deep_data\n",
      "[HUGECTR][10:45:07][INFO][RANK0]: create embedding for inference success\n",
      "[HUGECTR][10:45:07][INFO][RANK0]: Inference stage skip BinaryCrossEntropyLoss layer, replaced by Sigmoid layer\n",
      "[HUGECTR][10:45:08][INFO][RANK0]: *****Insert embedding cache of model wdl on device 0*****\n",
      "[HUGECTR][10:45:08][INFO][RANK0]: Looking up 17 embeddings (each with 1 values)...\n",
      "[HUGECTR][10:45:08][INFO][RANK0]: ParallelHashMap backend. Table: hctr_et.wdl.sparse_embedding2. Fetched 17 / 17 values.\n",
      "[HUGECTR][10:45:08][INFO][RANK0]: ParallelHashMap: 17 hits, 0 missing!\n",
      "[HUGECTR][10:45:08][INFO][RANK0]: Parameter server lookup of 17 / 17 embeddings took 335 us.\n",
      "[HUGECTR][10:45:08][INFO][RANK0]: Looking up 158 embeddings (each with 16 values)...\n",
      "[HUGECTR][10:45:08][INFO][RANK0]: ParallelHashMap backend. Table: hctr_et.wdl.sparse_embedding1. Fetched 158 / 158 values.\n",
      "[HUGECTR][10:45:08][INFO][RANK0]: ParallelHashMap: 158 hits, 0 missing!\n",
      "[HUGECTR][10:45:08][INFO][RANK0]: Parameter server lookup of 158 / 158 embeddings took 177 us.\n",
      "WDL multi-embedding table inference result is [0.03392845019698143, 0.02259000577032566, 0.002557354513555765, 0.0002879526000469923, 0.0022612495813518763, 0.02724345028400421, 0.0038985891733318567, 0.0018046938348561525, 0.03567842021584511, 0.0080226119607687]\n"
     ]
    }
   ],
   "source": [
    "!python /wdl_train/wdl2predict.py \"wdl\" \"/wdl_train/wdl.json\" \"/wdl_train/wdl_dense_20000.model\" \"/wdl_train/wdl0_sparse_20000.model,/wdl_train/wdl1_sparse_20000.model\" \"/wdl_train/infer_test.csv\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
