{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb030e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4715d4f5",
   "metadata": {},
   "source": [
    "# 1.Overview\n",
    "\n",
    "In this notebook, we want to provide an tutorial how to train a standard dlrm model using HugeCTR High-level python API. We will use original Criteo dataset as training data\n",
    "\n",
    "1. [Overview](#1)\n",
    "2. [Dataset Preprocessing](#2)\n",
    "3. [DLRM Model Training](#3)\n",
    "4. [Save the Model Files](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae6a253",
   "metadata": {},
   "source": [
    "# 2. Dataset Preprocessing\n",
    "## 2.1 Generate training and validation data folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cbe6a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some data folder to store the original and preprocessed data\n",
    "# Standard Libraries\n",
    "import os\n",
    "from time import time\n",
    "import re\n",
    "import shutil\n",
    "import glob\n",
    "import warnings\n",
    "BASE_DIR = \"/dlrm_train\"\n",
    "train_path  = os.path.join(BASE_DIR, \"train\")\n",
    "val_path = os.path.join(BASE_DIR, \"val\")\n",
    "CUDA_VISIBLE_DEVICES = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
    "n_workers = len(CUDA_VISIBLE_DEVICES.split(\",\"))\n",
    "frac_size = 0.15\n",
    "allow_multi_gpu = False\n",
    "use_rmm_pool = False\n",
    "max_day = None  # (Optional) -- Limit the dataset to day 0-max_day for debugging\n",
    "\n",
    "if os.path.isdir(train_path):\n",
    "    shutil.rmtree(train_path)\n",
    "os.makedirs(train_path)\n",
    "\n",
    "if os.path.isdir(val_path):\n",
    "    shutil.rmtree(val_path)\n",
    "os.makedirs(val_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc9acca",
   "metadata": {},
   "source": [
    "## 2.2 Download the Original Criteo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf09dd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "wget is already the newest version (1.20.3-1ubuntu1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "296ee7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-25 12:03:40--  http://azuremlsampleexperiments.blob.core.windows.net/criteo/day_0.gz\n",
      "Resolving azuremlsampleexperiments.blob.core.windows.net (azuremlsampleexperiments.blob.core.windows.net)... 20.60.140.36\n",
      "Connecting to azuremlsampleexperiments.blob.core.windows.net (azuremlsampleexperiments.blob.core.windows.net)|20.60.140.36|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16309554343 (15G) [application/octet-stream]\n",
      "Saving to: ‘./dlrm/train/day_0.gz’\n",
      "\n",
      "day_0.gz               100%[===================================================>]   15G  --.-KB/s    in  6m 12s \n",
      "2021-03-25 12:09:52 (79.2 MB/s) - 'day_0.gz' saved [16309554343/16309554343]"
     ]
    }
   ],
   "source": [
    "!wget -P $train_path http://azuremlsampleexperiments.blob.core.windows.net/criteo/day_0.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4111e2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the split data set to training and validation \n",
    "!gzip -d -c $train_path/day_0.gz > day_0\n",
    "!head -n 45840617 day_0 > $train_path/train.txt \n",
    "!tail -n 2000000 day_0 > $val_path/test.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d885a41a",
   "metadata": {},
   "source": [
    "## 2.3 Preprocessing by NVTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e86173c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /dlrm_train/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /dlrm_train/preprocess.py\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "from cudf.io.parquet import ParquetWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures as cf\n",
    "from concurrent.futures import as_completed\n",
    "import shutil\n",
    "\n",
    "import dask_cudf\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "from dask.utils import parse_bytes\n",
    "from dask.delayed import delayed\n",
    "\n",
    "import cudf\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import rmm\n",
    "import nvtabular as nvt\n",
    "from nvtabular.io import Shuffle\n",
    "from nvtabular.ops import Categorify, Clip, FillMissing, HashBucket, LambdaOp, LogOp, Normalize, Rename, get_embedding_sizes\n",
    "from nvtabular.utils import _pynvml_mem_size, device_mem_size\n",
    "\n",
    "#%load_ext memory_profiler\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(message)s')\n",
    "logging.root.setLevel(logging.NOTSET)\n",
    "logging.getLogger('numba').setLevel(logging.WARNING)\n",
    "logging.getLogger('asyncio').setLevel(logging.WARNING)\n",
    "\n",
    "# define dataset schema\n",
    "CATEGORICAL_COLUMNS=[\"C\" + str(x) for x in range(1, 27)]\n",
    "CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "LABEL_COLUMNS = ['label']\n",
    "COLUMNS =  LABEL_COLUMNS + CONTINUOUS_COLUMNS +  CATEGORICAL_COLUMNS\n",
    "#/samples/criteo mode doesn't have dense features\n",
    "criteo_COLUMN=LABEL_COLUMNS +  CATEGORICAL_COLUMNS\n",
    "#For new feature cross columns\n",
    "CROSS_COLUMNS = []\n",
    "\n",
    "\n",
    "NUM_INTEGER_COLUMNS = 13\n",
    "NUM_CATEGORICAL_COLUMNS = 26\n",
    "NUM_TOTAL_COLUMNS = 1 + NUM_INTEGER_COLUMNS + NUM_CATEGORICAL_COLUMNS\n",
    "\n",
    "\n",
    "# Initialize RMM pool on ALL workers\n",
    "def setup_rmm_pool(client, pool_size):\n",
    "    client.run(rmm.reinitialize, pool_allocator=True, initial_pool_size=pool_size)\n",
    "    return None\n",
    "\n",
    "#compute the partition size with GB\n",
    "def bytesto(bytes, to, bsize=1024):\n",
    "    a = {'k' : 1, 'm': 2, 'g' : 3, 't' : 4, 'p' : 5, 'e' : 6 }\n",
    "    r = float(bytes)\n",
    "    return bytes / (bsize ** a[to])\n",
    "\n",
    "\n",
    "#process the data with NVTabular\n",
    "def process_NVT(args):\n",
    "\n",
    "    if args.feature_cross_list:\n",
    "        feature_pairs = [pair.split(\"_\") for pair in args.feature_cross_list.split(\",\")]\n",
    "        for pair in feature_pairs:\n",
    "            CROSS_COLUMNS.append(pair[0]+'_'+pair[1])\n",
    "\n",
    "\n",
    "    logging.info('NVTabular processing')\n",
    "    train_input = os.path.join(args.data_path, \"train/train.txt\")\n",
    "    val_input = os.path.join(args.data_path, \"val/test.txt\")\n",
    "    PREPROCESS_DIR_temp_train = os.path.join(args.out_path, 'train/temp-parquet-after-conversion')\n",
    "    PREPROCESS_DIR_temp_val = os.path.join(args.out_path, 'val/temp-parquet-after-conversion')\n",
    "    if not os.path.exists(PREPROCESS_DIR_temp_train):\n",
    "        os.makedirs(PREPROCESS_DIR_temp_train)\n",
    "    \n",
    "    if not os.path.exists(PREPROCESS_DIR_temp_val):\n",
    "        os.makedirs(PREPROCESS_DIR_temp_val)\n",
    "    PREPROCESS_DIR_temp = [PREPROCESS_DIR_temp_train, PREPROCESS_DIR_temp_val]\n",
    "    train_output = os.path.join(args.out_path, \"train\")\n",
    "    val_output = os.path.join(args.out_path, \"val\")\n",
    "\n",
    "    # Make sure we have a clean parquet space for cudf conversion\n",
    "    for one_path in PREPROCESS_DIR_temp:\n",
    "        if os.path.exists(one_path):\n",
    "           shutil.rmtree(one_path)\n",
    "        os.mkdir(one_path)\n",
    "\n",
    "\n",
    "    ## Get Dask Client\n",
    "\n",
    "    # Deploy a Single-Machine Multi-GPU Cluster\n",
    "    device_size = device_mem_size(kind=\"total\")\n",
    "    cluster = None\n",
    "    if args.protocol == \"ucx\":\n",
    "        UCX_TLS = os.environ.get(\"UCX_TLS\", \"tcp,cuda_copy,cuda_ipc,sockcm\")\n",
    "        os.environ[\"UCX_TLS\"] = UCX_TLS\n",
    "        cluster = LocalCUDACluster(\n",
    "            protocol = args.protocol,\n",
    "            CUDA_VISIBLE_DEVICES = args.devices,\n",
    "            n_workers = len(args.devices.split(\",\")),\n",
    "            enable_nvlink=True,\n",
    "            device_memory_limit = int(device_size * args.device_limit_frac),\n",
    "            dashboard_address=\":\" + args.dashboard_port\n",
    "        )\n",
    "    else:\n",
    "        cluster = LocalCUDACluster(\n",
    "            protocol = args.protocol,\n",
    "            n_workers = len(args.devices.split(\",\")),\n",
    "            CUDA_VISIBLE_DEVICES = args.devices,\n",
    "            device_memory_limit = int(device_size * args.device_limit_frac),\n",
    "            dashboard_address=\":\" + args.dashboard_port\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # Create the distributed client\n",
    "    client = Client(cluster)\n",
    "    if args.device_pool_frac > 0.01:\n",
    "        setup_rmm_pool(client, int(args.device_pool_frac*device_size))\n",
    "\n",
    "\n",
    "    #calculate the total processing time\n",
    "    runtime = time.time()\n",
    "\n",
    "    #test dataset without the label feature\n",
    "    if args.dataset_type == 'test':\n",
    "        global LABEL_COLUMNS\n",
    "        LABEL_COLUMNS = []\n",
    "\n",
    "    ##-----------------------------------##\n",
    "    # Dask rapids converts txt to parquet\n",
    "    # Dask cudf dataframe = ddf\n",
    "\n",
    "    ## train/valid txt to parquet\n",
    "    train_valid_paths = [(train_input,PREPROCESS_DIR_temp_train),(val_input,PREPROCESS_DIR_temp_val)]\n",
    "\n",
    "    for input, temp_output in train_valid_paths:\n",
    "\n",
    "        ddf = dask_cudf.read_csv(input,sep='\\t',names=LABEL_COLUMNS + CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS)\n",
    "\n",
    "        ## Convert label col to FP32\n",
    "        if args.parquet_format and args.dataset_type == 'train':\n",
    "            ddf[\"label\"] = ddf['label'].astype('float32')\n",
    "\n",
    "        # Save it as parquet format for better memory usage\n",
    "        ddf.to_parquet(temp_output,header=True)\n",
    "        ##-----------------------------------##\n",
    "\n",
    "    COLUMNS =  LABEL_COLUMNS + CONTINUOUS_COLUMNS + CROSS_COLUMNS + CATEGORICAL_COLUMNS\n",
    "    train_paths = glob.glob(os.path.join(PREPROCESS_DIR_temp_train, \"*.parquet\"))\n",
    "    valid_paths = glob.glob(os.path.join(PREPROCESS_DIR_temp_val, \"*.parquet\"))\n",
    " \n",
    "    num_buckets=10000000\n",
    "    categorify_op = Categorify(out_path=\"./\", max_size=num_buckets)\n",
    "    cat_features = CATEGORICAL_COLUMNS >> categorify_op\n",
    "    logging.info('Fillmissing processing')\n",
    "    logging.info('Nomalization processing')\n",
    "    cont_features = CONTINUOUS_COLUMNS >> FillMissing() >> Clip(min_value=0) >> Normalize()\n",
    "    features = cat_features + cont_features + LABEL_COLUMNS\n",
    "    workflow = nvt.Workflow(features, client=client)\n",
    "\n",
    "\n",
    "    ##Define the output format##\n",
    "    output_format='hugectr'\n",
    "    if args.parquet_format:\n",
    "        output_format='parquet'\n",
    "    ##--------------------##\n",
    "\n",
    "    # just for /samples/criteo model\n",
    "    train_ds_iterator = nvt.Dataset(train_paths, engine='parquet', part_size=int(args.part_mem_frac * device_size))\n",
    "    valid_ds_iterator = nvt.Dataset(valid_paths, engine='parquet', part_size=int(args.part_mem_frac * device_size))\n",
    "\n",
    "    shuffle = None\n",
    "    if args.shuffle == \"PER_WORKER\":\n",
    "        shuffle = nvt.io.Shuffle.PER_WORKER\n",
    "    elif args.shuffle == \"PER_PARTITION\":\n",
    "        shuffle = nvt.io.Shuffle.PER_PARTITION\n",
    "        \n",
    "    dict_dtypes={}\n",
    "\n",
    "    for col in CATEGORICAL_COLUMNS:\n",
    "        dict_dtypes[col] = np.int64\n",
    "    \n",
    "    for col in CONTINUOUS_COLUMNS:\n",
    "        dict_dtypes[col] = np.float32\n",
    "    \n",
    "    for col in LABEL_COLUMNS:\n",
    "        dict_dtypes[col] = np.float32\n",
    "\n",
    "    logging.info('Train Datasets Preprocessing.....')\n",
    "\n",
    "    workflow.fit(train_ds_iterator)\n",
    "\n",
    "    workflow.transform(train_ds_iterator).to_parquet(output_path=train_output,\n",
    "                                         shuffle=shuffle, \n",
    "                                         dtypes=dict_dtypes,\n",
    "                                         labels=LABEL_COLUMNS,\n",
    "                                         conts=CONTINUOUS_COLUMNS,\n",
    "                                         cats=CATEGORICAL_COLUMNS)\n",
    "\n",
    "\n",
    "\n",
    "    logging.info('Valid Datasets Preprocessing.....')\n",
    "\n",
    "    workflow.transform(valid_ds_iterator).to_parquet(output_path=val_output, \n",
    "                                             dtypes=dict_dtypes,\n",
    "                                             labels=LABEL_COLUMNS,\n",
    "                                             conts=CONTINUOUS_COLUMNS,\n",
    "                                             cats=CATEGORICAL_COLUMNS)\n",
    "    #--------------------##\n",
    "    #Output slot_size for each categorical feature\n",
    "    embeddings = [c[0] for c in categorify_op.get_embedding_sizes(CATEGORICAL_COLUMNS).values()]\n",
    "    embeddings = np.clip(a=embeddings, a_min=None, a_max=num_buckets).tolist()\n",
    "    print(embeddings)\n",
    "    ##--------------------##\n",
    "\n",
    "    ## Shutdown clusters\n",
    "    client.close()\n",
    "    logging.info('NVTabular processing done')\n",
    "\n",
    "    runtime = time.time() - runtime\n",
    "\n",
    "    print(\"\\nDask-NVTabular Criteo Preprocessing\")\n",
    "    print(\"--------------------------------------\")\n",
    "    print(f\"data_path          | {args.data_path}\")\n",
    "    print(f\"output_path        | {args.out_path}\")\n",
    "    print(f\"partition size     | {'%.2f GB'%bytesto(int(args.part_mem_frac * device_size),'g')}\")\n",
    "    print(f\"protocol           | {args.protocol}\")\n",
    "    print(f\"device(s)          | {args.devices}\")\n",
    "    print(f\"rmm-pool-frac      | {(args.device_pool_frac)}\")\n",
    "    print(f\"out-files-per-proc | {args.out_files_per_proc}\")\n",
    "    print(f\"num_io_threads     | {args.num_io_threads}\")\n",
    "    print(f\"shuffle            | {args.shuffle}\")\n",
    "    print(\"======================================\")\n",
    "    print(f\"Runtime[s]         | {runtime}\")\n",
    "    print(\"======================================\\n\")\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=(\"Multi-GPU Criteo Preprocessing\"))\n",
    "\n",
    "    #\n",
    "    # System Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\"--data_path\", type=str, help=\"Input dataset path (Required)\")\n",
    "    parser.add_argument(\"--out_path\", type=str, help=\"Directory path to write output (Required)\")\n",
    "    parser.add_argument(\n",
    "        \"-d\",\n",
    "        \"--devices\",\n",
    "        default=os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\"),\n",
    "        type=str,\n",
    "        help='Comma-separated list of visible devices (e.g. \"0,1,2,3\"). '\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-p\",\n",
    "        \"--protocol\",\n",
    "        choices=[\"tcp\", \"ucx\"],\n",
    "        default=\"tcp\",\n",
    "        type=str,\n",
    "        help=\"Communication protocol to use (Default 'tcp')\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device_limit_frac\",\n",
    "        default=0.5,\n",
    "        type=float,\n",
    "        help=\"Worker device-memory limit as a fraction of GPU capacity (Default 0.8). \"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device_pool_frac\",\n",
    "        default=0.9,\n",
    "        type=float,\n",
    "        help=\"RMM pool size for each worker  as a fraction of GPU capacity (Default 0.9). \"\n",
    "        \"The RMM pool frac is the same for all GPUs, make sure each one has enough memory size\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_io_threads\",\n",
    "        default=0,\n",
    "        type=int,\n",
    "        help=\"Number of threads to use when writing output data (Default 0). \"\n",
    "        \"If 0 is specified, multi-threading will not be used for IO.\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Data-Decomposition Parameters\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--part_mem_frac\",\n",
    "        default=0.125,\n",
    "        type=float,\n",
    "        help=\"Maximum size desired for dataset partitions as a fraction \"\n",
    "        \"of GPU capacity (Default 0.125)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out_files_per_proc\",\n",
    "        default=8,\n",
    "        type=int,\n",
    "        help=\"Number of output files to write on each worker (Default 8)\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Preprocessing Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"-f\",\n",
    "        \"--freq_limit\",\n",
    "        default=0,\n",
    "        type=int,\n",
    "        help=\"Frequency limit for categorical encoding (Default 0)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-s\",\n",
    "        \"--shuffle\",\n",
    "        choices=[\"PER_WORKER\", \"PER_PARTITION\", \"NONE\"],\n",
    "        default=\"PER_PARTITION\",\n",
    "        help=\"Shuffle algorithm to use when writing output data to disk (Default PER_PARTITION)\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--feature_cross_list\", default=None, type=str, help=\"List of feature crossing cols (e.g. C1_C2, C3_C4)\"\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Diagnostics Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--profile\",\n",
    "        metavar=\"PATH\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"Specify a file path to export a Dask profile report (E.g. dask-report.html).\"\n",
    "        \"If this option is excluded from the command, not profile will be exported\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dashboard_port\",\n",
    "        default=\"8787\",\n",
    "        type=str,\n",
    "        help=\"Specify the desired port of Dask's diagnostics-dashboard (Default `3787`). \"\n",
    "        \"The dashboard will be hosted at http://<IP>:<PORT>/status\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Format\n",
    "    #\n",
    "\n",
    "    parser.add_argument('--parquet_format', type=int, default=1)\n",
    "    parser.add_argument('--dataset_type', type=str, default='train')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.n_workers = len(args.devices.split(\",\"))\n",
    "    return args\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args = parse_args()\n",
    "\n",
    "    process_NVT(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d91e2d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-29 06:41:04,839 NVTabular processing\n",
      "distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2021-11-29 06:41:29,382 Fillmissing processing\n",
      "2021-11-29 06:41:29,382 Nomalization processing\n",
      "2021-11-29 06:41:29,573 Train Datasets Preprocessing.....\n",
      "/usr/local/lib/python3.8/dist-packages/numba/cuda/compiler.py:865: NumbaPerformanceWarning: Grid size (1) < 2 * SM count (160) will likely result in GPU under utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "2021-11-29 06:42:26,554 Valid Datasets Preprocessing.....\n",
      "[4976199, 25419, 14705, 7112, 19283, 4, 6391, 1282, 60, 3289052, 282487, 138210, 11, 2203, 8901, 67, 4, 948, 15, 5577159, 1385790, 4348882, 178673, 10023, 88, 34]\n",
      "2021-11-29 06:42:31,046 NVTabular processing done\n",
      "\n",
      "Dask-NVTabular Criteo Preprocessing\n",
      "--------------------------------------\n",
      "data_path          | /dlrm_train\n",
      "output_path        | /dlrm_train\n",
      "partition size     | 1.97 GB\n",
      "protocol           | tcp\n",
      "device(s)          | 0\n",
      "rmm-pool-frac      | 0.5\n",
      "out-files-per-proc | 1\n",
      "num_io_threads     | 2\n",
      "shuffle            | PER_PARTITION\n",
      "======================================\n",
      "Runtime[s]         | 83.59456992149353\n",
      "======================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 ./preprocess.py --data_path /dlrm_train --out_path /dlrm_train --freq_limit 6 --device_limit_frac 0.5 --device_pool_frac 0.5 --out_files_per_proc 1  --devices \"0\" --num_io_threads 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c45d3c",
   "metadata": {},
   "source": [
    "## 3. DLRM Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5761ad52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile './model.py'\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "\n",
    "# 1. Create Solver, DataReaderParams and Optimizer\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 300,\n",
    "                              batchsize_eval = 16384,\n",
    "                              batchsize = 16384,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[0,1,2,3]],\n",
    "                              repeat_dataset = True,\n",
    "                              i64_input_key = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Parquet,\n",
    "                                  source = [\"./train/_file_list.txt\"],\n",
    "                                  eval_source = \"./val/_file_list.txt\",\n",
    "                                  slot_size_array = [4976199, 25419, 14705, 7112, 19283, 4, 6391, 1282, 60, 3289052, 282487, 138210, 11, 2203, 8901, 67, 4, 948, 15, 5577159, 1385790, 4348882, 178673, 10023, 88, 34],\n",
    "                                  check_type = hugectr.Check_t.Non)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.SGD,\n",
    "                                    update_type = hugectr.Update_t.Local,\n",
    "                                    atomic_update = True)\n",
    "# 2. Initialize the Model instance\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "\n",
    "# 3. Construct the Model graph\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"data1\", 2, False, 26)]))\n",
    "                        \n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 10000,\n",
    "                            embedding_vec_size = 128,                            \n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"data1\",\n",
    "                            slot_size_array = [4976199, 3289052, 282487, 138210, 11, 2203, 8901, 67, 4, 948, 15, 25419, 5577159, 1385790, 4348882, 178673, 10023, 88, 34, 14705, 7112, 19283, 4, 6391, 1282, 60],\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dense\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=512))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))                           \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=256))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))                            \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=128))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc3\"],\n",
    "                            top_names = [\"relu3\"]))                              \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Interaction,\n",
    "                            bottom_names = [\"relu3\",\"sparse_embedding1\"],\n",
    "                            top_names = [\"interaction1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"interaction1\"],\n",
    "                            top_names = [\"fc4\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc4\"],\n",
    "                            top_names = [\"relu4\"]))                              \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu4\"],\n",
    "                            top_names = [\"fc5\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc5\"],\n",
    "                            top_names = [\"relu5\"]))                              \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu5\"],\n",
    "                            top_names = [\"fc6\"],\n",
    "                            num_output=512))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc6\"],\n",
    "                            top_names = [\"relu6\"]))                               \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu6\"],\n",
    "                            top_names = [\"fc7\"],\n",
    "                            num_output=256))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc7\"],\n",
    "                            top_names = [\"relu7\"]))                                                                              \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu7\"],\n",
    "                            top_names = [\"fc8\"],\n",
    "                            num_output=1))                                                                                           \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"fc8\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "\n",
    "# 4. Dump the Model graph to JSON\n",
    "model.graph_to_json(graph_config_file = \"dlrm.json\")\n",
    "\n",
    "# 5. Compile & Fit\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.fit(max_iter = 21000, display = 1000, eval_interval = 4000, snapshot = 20000, snapshot_prefix = \"dlrm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ab06c11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "By default, for Open MPI 4.0 and later, infiniband ports on a device\n",
      "are not used by default.  The intent is to use UCX for these devices.\n",
      "You can override this policy by setting the btl_openib_allow_ib MCA parameter\n",
      "to true.\n",
      "\n",
      "  Local host:              prm-dgx-09\n",
      "  Local adapter:           mlx5_0\n",
      "  Local port:              1\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "WARNING: There was an error initializing an OpenFabrics device.\n",
      "\n",
      "  Local host:   prm-dgx-09\n",
      "  Local device: mlx5_1\n",
      "--------------------------------------------------------------------------\n",
      "HugeCTR Version: 3.2.0\n",
      "====================================================Model Init=====================================================\n",
      "[29d07h02m29s][HUGECTR][INFO]: Global seed is 2643346044\n",
      "[29d07h02m30s][HUGECTR][INFO]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "  GPU 1 ->  node 0\n",
      "  GPU 2 ->  node 0\n",
      "  GPU 3 ->  node 0\n",
      "\n",
      "[prm-dgx-09:01029] 1 more process has sent help message help-mpi-btl-openib.txt / ib port not selected\n",
      "[prm-dgx-09:01029] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\n",
      "[29d07h02m34s][HUGECTR][INFO]: Start all2all warmup\n",
      "[29d07h02m34s][HUGECTR][INFO]: End all2all warmup\n",
      "[29d07h02m34s][HUGECTR][INFO]: Using All-reduce algorithm NCCL\n",
      "Device 0: Tesla V100-SXM2-16GB\n",
      "Device 1: Tesla V100-SXM2-16GB\n",
      "Device 2: Tesla V100-SXM2-16GB\n",
      "Device 3: Tesla V100-SXM2-16GB\n",
      "[29d07h02m34s][HUGECTR][INFO]: num of DataReader workers: 4\n",
      "[29d07h02m34s][HUGECTR][INFO]: Vocabulary size: 20273002\n",
      "[29d07h02m34s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=20480000\n",
      "[29d07h02m34s][HUGECTR][INFO]: Save the model graph to dlrm.json, successful\n",
      "===================================================Model Compile===================================================\n",
      "[29d07h02m56s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[29d07h02m56s][HUGECTR][INFO]: gpu1 start to init embedding\n",
      "[29d07h02m56s][HUGECTR][INFO]: gpu3 start to init embedding\n",
      "[29d07h02m56s][HUGECTR][INFO]: gpu2 start to init embedding\n",
      "[29d07h02m56s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[29d07h02m56s][HUGECTR][INFO]: gpu3 init embedding done\n",
      "[29d07h02m56s][HUGECTR][INFO]: gpu2 init embedding done\n",
      "[29d07h02m56s][HUGECTR][INFO]: gpu1 init embedding done\n",
      "[29d07h02m56s][HUGECTR][INFO]: Starting AUC NCCL warm-up\n",
      "[29d07h02m56s][HUGECTR][INFO]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data1                         \n",
      "(None, 1)                               (None, 13)                              \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      data1                         sparse_embedding1             (None, 26, 128)               \n",
      "InnerProduct                            dense                         fc1                           (None, 512)                   \n",
      "ReLU                                    fc1                           relu1                         (None, 512)                   \n",
      "InnerProduct                            relu1                         fc2                           (None, 256)                   \n",
      "ReLU                                    fc2                           relu2                         (None, 256)                   \n",
      "InnerProduct                            relu2                         fc3                           (None, 128)                   \n",
      "ReLU                                    fc3                           relu3                         (None, 128)                   \n",
      "Interaction                             relu3,sparse_embedding1       interaction1                  (None, 480)                   \n",
      "InnerProduct                            interaction1                  fc4                           (None, 1024)                  \n",
      "ReLU                                    fc4                           relu4                         (None, 1024)                  \n",
      "InnerProduct                            relu4                         fc5                           (None, 1024)                  \n",
      "ReLU                                    fc5                           relu5                         (None, 1024)                  \n",
      "InnerProduct                            relu5                         fc6                           (None, 512)                   \n",
      "ReLU                                    fc6                           relu6                         (None, 512)                   \n",
      "InnerProduct                            relu6                         fc7                           (None, 256)                   \n",
      "ReLU                                    fc7                           relu7                         (None, 256)                   \n",
      "InnerProduct                            relu7                         fc8                           (None, 1)                     \n",
      "BinaryCrossEntropyLoss                  fc8,label                     loss                                                        \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[29d70h20m56s][HUGECTR][INFO]: Use non-epoch mode with number of iterations: 21000\n",
      "[29d70h20m56s][HUGECTR][INFO]: Training batchsize: 16384, evaluation batchsize: 16384\n",
      "[29d70h20m56s][HUGECTR][INFO]: Evaluation interval: 4000, snapshot interval: 20000\n",
      "[29d70h20m56s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[29d70h20m56s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[29d70h20m56s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[29d70h20m56s][HUGECTR][INFO]: Training source file: ./train/_file_list.txt\n",
      "[29d70h20m56s][HUGECTR][INFO]: Evaluation source file: ./val/_file_list.txt\n",
      "[29d70h30m90s][HUGECTR][INFO]: Iter: 1000 Time(1000 iters): 12.618435s Loss: 0.155167 lr:0.001000\n",
      "[29d70h30m22s][HUGECTR][INFO]: Iter: 2000 Time(1000 iters): 12.671653s Loss: 0.149424 lr:0.001000\n",
      "[29d70h30m34s][HUGECTR][INFO]: Iter: 3000 Time(1000 iters): 12.654138s Loss: 0.141401 lr:0.001000\n",
      "[29d70h30m47s][HUGECTR][INFO]: Iter: 4000 Time(1000 iters): 12.569332s Loss: 0.160830 lr:0.001000\n",
      "[29d70h30m49s][HUGECTR][INFO]: Evaluation, AUC: 0.506011\n",
      "[29d70h30m49s][HUGECTR][INFO]: Eval Time for 300 iters: 1.697283s\n",
      "[29d70h40m10s][HUGECTR][INFO]: Iter: 5000 Time(1000 iters): 14.358748s Loss: 0.150652 lr:0.001000\n",
      "[29d70h40m14s][HUGECTR][INFO]: Iter: 6000 Time(1000 iters): 12.663047s Loss: 0.155320 lr:0.001000\n",
      "[29d70h40m26s][HUGECTR][INFO]: Iter: 7000 Time(1000 iters): 12.579980s Loss: 0.157751 lr:0.001000\n",
      "[29d70h40m39s][HUGECTR][INFO]: Iter: 8000 Time(1000 iters): 12.637855s Loss: 0.135864 lr:0.001000\n",
      "[29d70h40m41s][HUGECTR][INFO]: Evaluation, AUC: 0.578262\n",
      "[29d70h40m41s][HUGECTR][INFO]: Eval Time for 300 iters: 1.664496s\n",
      "[29d70h40m53s][HUGECTR][INFO]: Iter: 9000 Time(1000 iters): 14.336298s Loss: 0.149137 lr:0.001000\n",
      "[29d70h50m60s][HUGECTR][INFO]: Iter: 10000 Time(1000 iters): 12.576717s Loss: 0.135164 lr:0.001000\n",
      "[29d70h50m19s][HUGECTR][INFO]: Iter: 11000 Time(1000 iters): 12.633977s Loss: 0.132804 lr:0.001000\n",
      "[29d70h50m31s][HUGECTR][INFO]: Iter: 12000 Time(1000 iters): 12.591427s Loss: 0.148388 lr:0.001000\n",
      "[29d70h50m33s][HUGECTR][INFO]: Evaluation, AUC: 0.609795\n",
      "[29d70h50m33s][HUGECTR][INFO]: Eval Time for 300 iters: 1.691665s\n",
      "[29d70h50m45s][HUGECTR][INFO]: Iter: 13000 Time(1000 iters): 14.156894s Loss: 0.157789 lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29d70h50m58s][HUGECTR][INFO]: Iter: 14000 Time(1000 iters): 12.451465s Loss: 0.147939 lr:0.001000\n",
      "[29d70h60m10s][HUGECTR][INFO]: Iter: 15000 Time(1000 iters): 12.422438s Loss: 0.145342 lr:0.001000\n",
      "[29d70h60m23s][HUGECTR][INFO]: Iter: 16000 Time(1000 iters): 12.411487s Loss: 0.136565 lr:0.001000\n",
      "[29d70h60m24s][HUGECTR][INFO]: Evaluation, AUC: 0.625069\n",
      "[29d70h60m24s][HUGECTR][INFO]: Eval Time for 300 iters: 1.658554s\n",
      "[29d70h60m37s][HUGECTR][INFO]: Iter: 17000 Time(1000 iters): 14.107148s Loss: 0.138869 lr:0.001000\n",
      "[29d70h60m49s][HUGECTR][INFO]: Iter: 18000 Time(1000 iters): 12.450642s Loss: 0.150656 lr:0.001000\n",
      "[29d70h70m20s][HUGECTR][INFO]: Iter: 19000 Time(1000 iters): 12.438676s Loss: 0.139049 lr:0.001000\n",
      "[29d70h70m14s][HUGECTR][INFO]: Iter: 20000 Time(1000 iters): 12.395778s Loss: 0.147212 lr:0.001000\n",
      "[29d70h70m16s][HUGECTR][INFO]: Evaluation, AUC: 0.632774\n",
      "[29d70h70m16s][HUGECTR][INFO]: Eval Time for 300 iters: 1.698418s\n",
      "[29d70h70m21s][HUGECTR][INFO]: Rank0: Write hash table to file\n",
      "[29d70h70m26s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[29d70h70m26s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[29d70h70m27s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[29d70h70m27s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[29d70h70m27s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "Finish 21000 iterations with batchsize: 16384 in 282.65s\n"
     ]
    }
   ],
   "source": [
    "!python model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e05332",
   "metadata": {},
   "source": [
    "## 4. Save the Model Files & Inference Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e56629ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root       0 Nov 29 07:07 dlrm0_opt_sparse_20000.model\r\n",
      "-rw-r--r-- 1 root root 9479684 Nov 29 07:07 dlrm_dense_20000.model\r\n",
      "-rw-r--r-- 1 root root       0 Nov 29 07:07 dlrm_opt_dense_20000.model\r\n",
      "\r\n",
      "dlrm0_sparse_20000.model:\r\n",
      "total 10294912\r\n",
      "-rw-r--r-- 1 root root 10379768832 Nov 29 07:07 emb_vector\r\n",
      "-rw-r--r-- 1 root root   162183888 Nov 29 07:07 key\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l *20000.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce01e472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 702242\r\n",
      "-rw-r--r-- 1 root root        33 Nov 29 06:42 _file_list.txt\r\n",
      "-rw-r--r-- 1 root root  81092112 Nov 29 06:42 _hugectr.keyset\r\n",
      "-rw-r--r-- 1 root root     21528 Nov 29 06:42 _metadata\r\n",
      "-rw-r--r-- 1 root root      1437 Nov 29 06:42 _metadata.json\r\n",
      "-rw-r--r-- 1 root root 128131055 Nov 29 06:42 part_0.parquet\r\n",
      "-rw-r--r-- 1 root root     19945 Nov 29 06:42 schema.pbtxt\r\n",
      "drwxr-xr-x 2 root root      4096 Nov 29 06:41 temp-parquet-after-conversion\r\n",
      "-rw-r--r-- 1 root root 509766965 Nov 29 06:39 test.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /dlrm_train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86cfba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"/dlrm_train/val/part_0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51ae7584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>...</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>I11</th>\n",
       "      <th>I12</th>\n",
       "      <th>I13</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>669260</td>\n",
       "      <td>28</td>\n",
       "      <td>473</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>157</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>19900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.209261</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.281810</td>\n",
       "      <td>0.035263</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>-0.261958</td>\n",
       "      <td>-0.173750</td>\n",
       "      <td>-0.262248</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.209261</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.258333</td>\n",
       "      <td>-0.760031</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>-0.261958</td>\n",
       "      <td>-0.194540</td>\n",
       "      <td>-0.262248</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>148</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240178</td>\n",
       "      <td>0.205918</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.276593</td>\n",
       "      <td>2.204247</td>\n",
       "      <td>1.386036</td>\n",
       "      <td>0.690729</td>\n",
       "      <td>-0.277389</td>\n",
       "      <td>0.046789</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>177</td>\n",
       "      <td>61</td>\n",
       "      <td>377</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>402</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023207</td>\n",
       "      <td>0.068484</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.276593</td>\n",
       "      <td>1.987348</td>\n",
       "      <td>1.386036</td>\n",
       "      <td>0.055604</td>\n",
       "      <td>-0.289136</td>\n",
       "      <td>-0.306396</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.526501</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>0.339399</td>\n",
       "      <td>-0.281810</td>\n",
       "      <td>0.396760</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>2.596102</td>\n",
       "      <td>-0.257140</td>\n",
       "      <td>0.399973</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       C1  C2   C3  C4  C5  C6   C7  C8  C9    C10  ...        I5        I6  \\\n",
       "0  669260  28  473  18   2   1  157   4   3  19900  ... -0.209261 -0.206385   \n",
       "1       1   1   31   2  20   1   51   3   1      1  ... -0.209261 -0.206385   \n",
       "2       2  25  148  52   2   1  101   6   5      2  ...  0.240178  0.205918   \n",
       "3     177  61  377   2   1   1  402  17   1    209  ...  0.023207  0.068484   \n",
       "4       2   3   30   4   2   1    8   1   2      2  ...  1.526501 -0.206385   \n",
       "\n",
       "         I7        I8        I9       I10       I11       I12       I13  label  \n",
       "0 -0.064249 -0.281810  0.035263 -0.470383 -0.261958 -0.173750 -0.262248    0.0  \n",
       "1 -0.064249 -0.258333 -0.760031 -0.470383 -0.261958 -0.194540 -0.262248    0.0  \n",
       "2 -0.064249 -0.276593  2.204247  1.386036  0.690729 -0.277389  0.046789    0.0  \n",
       "3 -0.064249 -0.276593  1.987348  1.386036  0.055604 -0.289136 -0.306396    0.0  \n",
       "4  0.339399 -0.281810  0.396760 -0.470383  2.596102 -0.257140  0.399973    0.0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff39f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10).to_csv('/dlrm_train/infer_test.csv', sep=',', index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5f65f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /dlrm_train/dlrm2predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /dlrm_train/dlrm2predict.py\n",
    "from hugectr.inference import InferenceParams, CreateInferenceSession\n",
    "import hugectr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "\n",
    "config_file = \"/dlrm_train/dlrm.json\"\n",
    "CATEGORICAL_COLUMNS=[\"C\" + str(x) for x in range(1, 27)]\n",
    "CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "LABEL_COLUMNS = ['label']\n",
    "emb_size = [4976199, 3289052, 282487, 138210, 11, 2203, 8901, 67, 4, 948, 15, 25419, 5577159, 1385790, 4348882, 178673, 10023, 88, 34, 14705, 7112, 19283, 4, 6391, 1282, 60]\n",
    "shift = np.insert(np.cumsum(emb_size), 0, 0)[:-1]\n",
    "test_df=pd.read_csv(\"/dlrm_train/infer_test.csv\",sep=',')\n",
    "row_ptrs = list(range(0,261))\n",
    "dense_features = list(test_df[CONTINUOUS_COLUMNS].values.flatten())\n",
    "test_df[CATEGORICAL_COLUMNS].astype(np.int64)\n",
    "embedding_columns = list((test_df[CATEGORICAL_COLUMNS]+shift).values.flatten())\n",
    "                \n",
    "\n",
    "# create parameter server, embedding cache and inference session\n",
    "inference_params = InferenceParams(model_name = \"dlrm\",\n",
    "                                max_batchsize = 64,\n",
    "                                hit_rate_threshold = 0.5,\n",
    "                                dense_model_file = \"/dlrm_train/dlrm_dense_20000.model\",\n",
    "                                sparse_model_files = [\"/dlrm_train/dlrm0_sparse_20000.model\"],\n",
    "                                device_id = 0,\n",
    "                                use_gpu_embedding_cache = True,\n",
    "                                cache_size_percentage = 0.2,\n",
    "                                i64_input_key = True,\n",
    "                                use_mixed_precision = False)\n",
    "inference_session = CreateInferenceSession(config_file, inference_params)\n",
    "output = inference_session.predict(dense_features, embedding_columns, row_ptrs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3c2e277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "By default, for Open MPI 4.0 and later, infiniband ports on a device\n",
      "are not used by default.  The intent is to use UCX for these devices.\n",
      "You can override this policy by setting the btl_openib_allow_ib MCA parameter\n",
      "to true.\n",
      "\n",
      "  Local host:              prm-dgx-09\n",
      "  Local adapter:           mlx5_0\n",
      "  Local port:              1\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "WARNING: There was an error initializing an OpenFabrics device.\n",
      "\n",
      "  Local host:   prm-dgx-09\n",
      "  Local device: mlx5_2\n",
      "--------------------------------------------------------------------------\n",
      "[29d07h19m36s][HUGECTR][INFO]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[prm-dgx-09:01716] 2 more processes have sent help message help-mpi-btl-openib.txt / ib port not selected\n",
      "[prm-dgx-09:01716] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\n",
      "[29d07h20m33s][HUGECTR][INFO]: Global seed is 2882415532\n",
      "[29d07h20m33s][HUGECTR][INFO]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[29d07h20m34s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "[29d07h20m34s][HUGECTR][INFO]: Start all2all warmup\n",
      "[29d07h20m34s][HUGECTR][INFO]: End all2all warmup\n",
      "[29d07h20m34s][HUGECTR][INFO]: Use mixed precision: 0\n",
      "[29d07h20m34s][HUGECTR][INFO]: start create embedding for inference\n",
      "[29d07h20m34s][HUGECTR][INFO]: sparse_input name data1\n",
      "[29d07h20m34s][HUGECTR][INFO]: create embedding for inference success\n",
      "[29d07h20m34s][HUGECTR][INFO]: Inference stage skip BinaryCrossEntropyLoss layer, replaced by Sigmoid layer\n",
      "[0.029544265940785408, 0.03057795763015747, 0.03455175459384918, 0.0351637527346611, 0.041445542126894, 0.030013341456651688, 0.02823212370276451, 0.033557742834091187, 0.029562227427959442, 0.029309432953596115]\n"
     ]
    }
   ],
   "source": [
    "!python /dlrm_train/dlrm2predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c2ae85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
