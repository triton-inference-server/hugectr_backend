{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-mozambique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-globe",
   "metadata": {},
   "source": [
    "# 1.Overview\n",
    "\n",
    "In this notebook, we want to provide an tutorial how to train a standard dlrm model using HugeCTR High-level python API. We will use original Criteo dataset as training data\n",
    "\n",
    "1. [Overview](#1)\n",
    "2. [Dataset Preprocessing](#2)\n",
    "3. [DLRM Model Training](#3)\n",
    "4. [Save the Model Files](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-engagement",
   "metadata": {},
   "source": [
    "# 2. Dataset Preprocessing\n",
    "## 2.1 Generate training and validation data folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "noted-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some data folder to store the original and preprocessed data\n",
    "# Standard Libraries\n",
    "import os\n",
    "from time import time\n",
    "import re\n",
    "import shutil\n",
    "import glob\n",
    "import warnings\n",
    "BASE_DIR = \"./dlrm\"\n",
    "train_path  = os.path.join(BASE_DIR, \"train\")\n",
    "val_path = os.path.join(BASE_DIR, \"val\")\n",
    "CUDA_VISIBLE_DEVICES = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
    "n_workers = len(CUDA_VISIBLE_DEVICES.split(\",\"))\n",
    "frac_size = 0.15\n",
    "allow_multi_gpu = False\n",
    "use_rmm_pool = False\n",
    "max_day = None  # (Optional) -- Limit the dataset to day 0-max_day for debugging\n",
    "\n",
    "if os.path.isdir(train_path):\n",
    "    shutil.rmtree(train_path)\n",
    "os.makedirs(train_path)\n",
    "\n",
    "if os.path.isdir(val_path):\n",
    "    shutil.rmtree(val_path)\n",
    "os.makedirs(val_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-coral",
   "metadata": {},
   "source": [
    "## 2.2 Download the Original Criteo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "great-contamination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "wget is already the newest version (1.20.3-1ubuntu1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "asian-command",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-25 12:03:40--  http://azuremlsampleexperiments.blob.core.windows.net/criteo/day_0.gz\n",
      "Resolving azuremlsampleexperiments.blob.core.windows.net (azuremlsampleexperiments.blob.core.windows.net)... 20.60.140.36\n",
      "Connecting to azuremlsampleexperiments.blob.core.windows.net (azuremlsampleexperiments.blob.core.windows.net)|20.60.140.36|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16309554343 (15G) [application/octet-stream]\n",
      "Saving to: ‘./dlrm/train/day_0.gz’\n",
      "\n",
      "day_0.gz               100%[===================================================>]   15G  --.-KB/s    in  6m 12s \n",
      "2021-03-25 12:09:52 (79.2 MB/s) - 'day_0.gz' saved [16309554343/16309554343]"
     ]
    }
   ],
   "source": [
    "!wget -P $train_path http://azuremlsampleexperiments.blob.core.windows.net/criteo/day_0.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "previous-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the split data set to training and validation \n",
    "!gzip -d -c $train_path/day_0.gz > day_0\n",
    "!head -n 45840617 day_0 > $train_path/train.txt \n",
    "!tail -n 2000000 day_0 > $val_path/test.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-spanish",
   "metadata": {},
   "source": [
    "## 2.3 Preprocessing by NVTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "moderate-handy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "from cudf.io.parquet import ParquetWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures as cf\n",
    "from concurrent.futures import as_completed\n",
    "import shutil\n",
    "\n",
    "import dask_cudf\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "from dask.utils import parse_bytes\n",
    "from dask.delayed import delayed\n",
    "\n",
    "import cudf\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import rmm\n",
    "import nvtabular as nvt\n",
    "from nvtabular.io import Shuffle\n",
    "from nvtabular.ops import Categorify, Clip, FillMissing, HashBucket, LambdaOp, LogOp, Normalize, Rename, get_embedding_sizes\n",
    "from nvtabular.utils import _pynvml_mem_size, device_mem_size\n",
    "\n",
    "#%load_ext memory_profiler\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(message)s')\n",
    "logging.root.setLevel(logging.NOTSET)\n",
    "logging.getLogger('numba').setLevel(logging.WARNING)\n",
    "logging.getLogger('asyncio').setLevel(logging.WARNING)\n",
    "\n",
    "# define dataset schema\n",
    "CATEGORICAL_COLUMNS=[\"C\" + str(x) for x in range(1, 27)]\n",
    "CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "LABEL_COLUMNS = ['label']\n",
    "COLUMNS =  LABEL_COLUMNS + CONTINUOUS_COLUMNS +  CATEGORICAL_COLUMNS\n",
    "#/samples/criteo mode doesn't have dense features\n",
    "criteo_COLUMN=LABEL_COLUMNS +  CATEGORICAL_COLUMNS\n",
    "#For new feature cross columns\n",
    "CROSS_COLUMNS = []\n",
    "\n",
    "\n",
    "NUM_INTEGER_COLUMNS = 13\n",
    "NUM_CATEGORICAL_COLUMNS = 26\n",
    "NUM_TOTAL_COLUMNS = 1 + NUM_INTEGER_COLUMNS + NUM_CATEGORICAL_COLUMNS\n",
    "\n",
    "\n",
    "# Initialize RMM pool on ALL workers\n",
    "def setup_rmm_pool(client, pool_size):\n",
    "    client.run(rmm.reinitialize, pool_allocator=True, initial_pool_size=pool_size)\n",
    "    return None\n",
    "\n",
    "#compute the partition size with GB\n",
    "def bytesto(bytes, to, bsize=1024):\n",
    "    a = {'k' : 1, 'm': 2, 'g' : 3, 't' : 4, 'p' : 5, 'e' : 6 }\n",
    "    r = float(bytes)\n",
    "    return bytes / (bsize ** a[to])\n",
    "\n",
    "\n",
    "#process the data with NVTabular\n",
    "def process_NVT(args):\n",
    "\n",
    "    if args.feature_cross_list:\n",
    "        feature_pairs = [pair.split(\"_\") for pair in args.feature_cross_list.split(\",\")]\n",
    "        for pair in feature_pairs:\n",
    "            CROSS_COLUMNS.append(pair[0]+'_'+pair[1])\n",
    "\n",
    "\n",
    "    logging.info('NVTabular processing')\n",
    "    train_input = os.path.join(args.data_path, \"train/train.txt\")\n",
    "    val_input = os.path.join(args.data_path, \"val/test.txt\")\n",
    "    PREPROCESS_DIR_temp_train = os.path.join(args.out_path, 'train/temp-parquet-after-conversion')\n",
    "    PREPROCESS_DIR_temp_val = os.path.join(args.out_path, 'val/temp-parquet-after-conversion')\n",
    "    PREPROCESS_DIR_temp = [PREPROCESS_DIR_temp_train, PREPROCESS_DIR_temp_val]\n",
    "    train_output = os.path.join(args.out_path, \"train\")\n",
    "    val_output = os.path.join(args.out_path, \"val\")\n",
    "\n",
    "    # Make sure we have a clean parquet space for cudf conversion\n",
    "    for one_path in PREPROCESS_DIR_temp:\n",
    "        if os.path.exists(one_path):\n",
    "           shutil.rmtree(one_path)\n",
    "        os.mkdir(one_path)\n",
    "\n",
    "\n",
    "    ## Get Dask Client\n",
    "\n",
    "    # Deploy a Single-Machine Multi-GPU Cluster\n",
    "    device_size = device_mem_size(kind=\"total\")\n",
    "    cluster = None\n",
    "    if args.protocol == \"ucx\":\n",
    "        UCX_TLS = os.environ.get(\"UCX_TLS\", \"tcp,cuda_copy,cuda_ipc,sockcm\")\n",
    "        os.environ[\"UCX_TLS\"] = UCX_TLS\n",
    "        cluster = LocalCUDACluster(\n",
    "            protocol = args.protocol,\n",
    "            CUDA_VISIBLE_DEVICES = args.devices,\n",
    "            n_workers = len(args.devices.split(\",\")),\n",
    "            enable_nvlink=True,\n",
    "            device_memory_limit = int(device_size * args.device_limit_frac),\n",
    "            dashboard_address=\":\" + args.dashboard_port\n",
    "        )\n",
    "    else:\n",
    "        cluster = LocalCUDACluster(\n",
    "            protocol = args.protocol,\n",
    "            n_workers = len(args.devices.split(\",\")),\n",
    "            CUDA_VISIBLE_DEVICES = args.devices,\n",
    "            device_memory_limit = int(device_size * args.device_limit_frac),\n",
    "            dashboard_address=\":\" + args.dashboard_port\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # Create the distributed client\n",
    "    client = Client(cluster)\n",
    "    if args.device_pool_frac > 0.01:\n",
    "        setup_rmm_pool(client, int(args.device_pool_frac*device_size))\n",
    "\n",
    "\n",
    "    #calculate the total processing time\n",
    "    runtime = time.time()\n",
    "\n",
    "    #test dataset without the label feature\n",
    "    if args.dataset_type == 'test':\n",
    "        global LABEL_COLUMNS\n",
    "        LABEL_COLUMNS = []\n",
    "\n",
    "    ##-----------------------------------##\n",
    "    # Dask rapids converts txt to parquet\n",
    "    # Dask cudf dataframe = ddf\n",
    "\n",
    "    ## train/valid txt to parquet\n",
    "    train_valid_paths = [(train_input,PREPROCESS_DIR_temp_train),(val_input,PREPROCESS_DIR_temp_val)]\n",
    "\n",
    "    for input, temp_output in train_valid_paths:\n",
    "\n",
    "        ddf = dask_cudf.read_csv(input,sep='\\t',names=LABEL_COLUMNS + CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS)\n",
    "\n",
    "        ## Convert label col to FP32\n",
    "        if args.parquet_format and args.dataset_type == 'train':\n",
    "            ddf[\"label\"] = ddf['label'].astype('float32')\n",
    "\n",
    "        # Save it as parquet format for better memory usage\n",
    "        ddf.to_parquet(temp_output,header=True)\n",
    "        ##-----------------------------------##\n",
    "\n",
    "    COLUMNS =  LABEL_COLUMNS + CONTINUOUS_COLUMNS + CROSS_COLUMNS + CATEGORICAL_COLUMNS\n",
    "    train_paths = glob.glob(os.path.join(PREPROCESS_DIR_temp_train, \"*.parquet\"))\n",
    "    valid_paths = glob.glob(os.path.join(PREPROCESS_DIR_temp_val, \"*.parquet\"))\n",
    " \n",
    "    num_buckets=10000000\n",
    "    categorify_op = Categorify(out_path=\"./\", max_size=num_buckets)\n",
    "    cat_features = CATEGORICAL_COLUMNS >> categorify_op\n",
    "    logging.info('Fillmissing processing')\n",
    "    logging.info('Nomalization processing')\n",
    "    cont_features = CONTINUOUS_COLUMNS >> FillMissing() >> Clip(min_value=0) >> Normalize()\n",
    "    features = cat_features + cont_features + LABEL_COLUMNS\n",
    "    workflow = nvt.Workflow(features, client=client)\n",
    "\n",
    "\n",
    "    ##Define the output format##\n",
    "    output_format='hugectr'\n",
    "    if args.parquet_format:\n",
    "        output_format='parquet'\n",
    "    ##--------------------##\n",
    "\n",
    "    # just for /samples/criteo model\n",
    "    train_ds_iterator = nvt.Dataset(train_paths, engine='parquet', part_size=int(args.part_mem_frac * device_size))\n",
    "    valid_ds_iterator = nvt.Dataset(valid_paths, engine='parquet', part_size=int(args.part_mem_frac * device_size))\n",
    "\n",
    "    shuffle = None\n",
    "    if args.shuffle == \"PER_WORKER\":\n",
    "        shuffle = nvt.io.Shuffle.PER_WORKER\n",
    "    elif args.shuffle == \"PER_PARTITION\":\n",
    "        shuffle = nvt.io.Shuffle.PER_PARTITION\n",
    "        \n",
    "    dict_dtypes={}\n",
    "\n",
    "    for col in CATEGORICAL_COLUMNS:\n",
    "        dict_dtypes[col] = np.int64\n",
    "    \n",
    "    for col in CONTINUOUS_COLUMNS:\n",
    "        dict_dtypes[col] = np.float32\n",
    "    \n",
    "    for col in LABEL_COLUMNS:\n",
    "        dict_dtypes[col] = np.float32\n",
    "\n",
    "    logging.info('Train Datasets Preprocessing.....')\n",
    "\n",
    "    workflow.fit(train_ds_iterator)\n",
    "\n",
    "    workflow.transform(train_ds_iterator).to_parquet(output_path=train_output,\n",
    "                                         shuffle=shuffle, \n",
    "                                         dtypes=dict_dtypes,\n",
    "                                         labels=LABEL_COLUMNS,\n",
    "                                         conts=CONTINUOUS_COLUMNS,\n",
    "                                         cats=CATEGORICAL_COLUMNS)\n",
    "\n",
    "\n",
    "\n",
    "    logging.info('Valid Datasets Preprocessing.....')\n",
    "\n",
    "    workflow.transform(valid_ds_iterator).to_parquet(output_path=val_output, \n",
    "                                             dtypes=dict_dtypes,\n",
    "                                             labels=LABEL_COLUMNS,\n",
    "                                             conts=CONTINUOUS_COLUMNS,\n",
    "                                             cats=CATEGORICAL_COLUMNS)\n",
    "    #--------------------##\n",
    "    #Output slot_size for each categorical feature\n",
    "    embeddings = [c[0] for c in categorify_op.get_embedding_sizes(CATEGORICAL_COLUMNS).values()]\n",
    "    embeddings = np.clip(a=embeddings, a_min=None, a_max=num_buckets).tolist()\n",
    "    print(embeddings)\n",
    "    ##--------------------##\n",
    "\n",
    "    ## Shutdown clusters\n",
    "    client.close()\n",
    "    logging.info('NVTabular processing done')\n",
    "\n",
    "    runtime = time.time() - runtime\n",
    "\n",
    "    print(\"\\nDask-NVTabular Criteo Preprocessing\")\n",
    "    print(\"--------------------------------------\")\n",
    "    print(f\"data_path          | {args.data_path}\")\n",
    "    print(f\"output_path        | {args.out_path}\")\n",
    "    print(f\"partition size     | {'%.2f GB'%bytesto(int(args.part_mem_frac * device_size),'g')}\")\n",
    "    print(f\"protocol           | {args.protocol}\")\n",
    "    print(f\"device(s)          | {args.devices}\")\n",
    "    print(f\"rmm-pool-frac      | {(args.device_pool_frac)}\")\n",
    "    print(f\"out-files-per-proc | {args.out_files_per_proc}\")\n",
    "    print(f\"num_io_threads     | {args.num_io_threads}\")\n",
    "    print(f\"shuffle            | {args.shuffle}\")\n",
    "    print(\"======================================\")\n",
    "    print(f\"Runtime[s]         | {runtime}\")\n",
    "    print(\"======================================\\n\")\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=(\"Multi-GPU Criteo Preprocessing\"))\n",
    "\n",
    "    #\n",
    "    # System Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\"--data_path\", type=str, help=\"Input dataset path (Required)\")\n",
    "    parser.add_argument(\"--out_path\", type=str, help=\"Directory path to write output (Required)\")\n",
    "    parser.add_argument(\n",
    "        \"-d\",\n",
    "        \"--devices\",\n",
    "        default=os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\"),\n",
    "        type=str,\n",
    "        help='Comma-separated list of visible devices (e.g. \"0,1,2,3\"). '\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-p\",\n",
    "        \"--protocol\",\n",
    "        choices=[\"tcp\", \"ucx\"],\n",
    "        default=\"tcp\",\n",
    "        type=str,\n",
    "        help=\"Communication protocol to use (Default 'tcp')\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device_limit_frac\",\n",
    "        default=0.5,\n",
    "        type=float,\n",
    "        help=\"Worker device-memory limit as a fraction of GPU capacity (Default 0.8). \"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device_pool_frac\",\n",
    "        default=0.9,\n",
    "        type=float,\n",
    "        help=\"RMM pool size for each worker  as a fraction of GPU capacity (Default 0.9). \"\n",
    "        \"The RMM pool frac is the same for all GPUs, make sure each one has enough memory size\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_io_threads\",\n",
    "        default=0,\n",
    "        type=int,\n",
    "        help=\"Number of threads to use when writing output data (Default 0). \"\n",
    "        \"If 0 is specified, multi-threading will not be used for IO.\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Data-Decomposition Parameters\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--part_mem_frac\",\n",
    "        default=0.125,\n",
    "        type=float,\n",
    "        help=\"Maximum size desired for dataset partitions as a fraction \"\n",
    "        \"of GPU capacity (Default 0.125)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out_files_per_proc\",\n",
    "        default=8,\n",
    "        type=int,\n",
    "        help=\"Number of output files to write on each worker (Default 8)\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Preprocessing Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"-f\",\n",
    "        \"--freq_limit\",\n",
    "        default=0,\n",
    "        type=int,\n",
    "        help=\"Frequency limit for categorical encoding (Default 0)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-s\",\n",
    "        \"--shuffle\",\n",
    "        choices=[\"PER_WORKER\", \"PER_PARTITION\", \"NONE\"],\n",
    "        default=\"PER_PARTITION\",\n",
    "        help=\"Shuffle algorithm to use when writing output data to disk (Default PER_PARTITION)\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--feature_cross_list\", default=None, type=str, help=\"List of feature crossing cols (e.g. C1_C2, C3_C4)\"\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Diagnostics Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--profile\",\n",
    "        metavar=\"PATH\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"Specify a file path to export a Dask profile report (E.g. dask-report.html).\"\n",
    "        \"If this option is excluded from the command, not profile will be exported\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dashboard_port\",\n",
    "        default=\"8787\",\n",
    "        type=str,\n",
    "        help=\"Specify the desired port of Dask's diagnostics-dashboard (Default `3787`). \"\n",
    "        \"The dashboard will be hosted at http://<IP>:<PORT>/status\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Format\n",
    "    #\n",
    "\n",
    "    parser.add_argument('--parquet_format', type=int, default=1)\n",
    "    parser.add_argument('--dataset_type', type=str, default='train')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.n_workers = len(args.devices.split(\",\"))\n",
    "    return args\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args = parse_args()\n",
    "\n",
    "    process_NVT(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "practical-distance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-25 13:28:56,844 NVTabular processing\n",
      "2021-03-25 13:29:28,496 Fillmissing processing\n",
      "2021-03-25 13:29:28,497 Nomalization processing\n",
      "2021-03-25 13:29:28,899 Train Datasets Preprocessing.....\n",
      "2021-03-25 13:31:04,406 Valid Datasets Preprocessing.....\n",
      "[4976199, 3289052, 282487, 138210, 11, 2203, 8901, 67, 4, 948, 15, 25419, 5577159, 1385790, 4348882, 178673, 10023, 88, 34, 14705, 7112, 19283, 4, 6391, 1282, 60]\n",
      "2021-03-25 13:31:13,471 NVTabular processing done\n",
      "\n",
      "Dask-NVTabular Criteo Preprocessing\n",
      "--------------------------------------\n",
      "data_path          | dlrm\n",
      "output_path        | dlrm\n",
      "partition size     | 3.97 GB\n",
      "protocol           | tcp\n",
      "device(s)          | 0\n",
      "rmm-pool-frac      | 0.5\n",
      "out-files-per-proc | 1\n",
      "num_io_threads     | 2\n",
      "shuffle            | PER_PARTITION\n",
      "======================================\n",
      "Runtime[s]         | 127.04197239875793\n",
      "======================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 ./preprocess.py --data_path dlrm --out_path dlrm --freq_limit 6 --device_limit_frac 0.5 --device_pool_frac 0.5 --out_files_per_proc 1  --devices \"0\" --num_io_threads 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-console",
   "metadata": {},
   "source": [
    "## 3. DLRM Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "affected-establishment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile './model.py'\n",
    "# HugeCTR\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "\n",
    "solver = hugectr.solver_parser_helper(num_epochs = 0,\n",
    "                                    max_iter = 21000,\n",
    "                                    max_eval_batches = 4000,\n",
    "                                    batchsize_eval = 2720,\n",
    "                                    batchsize = 2720,\n",
    "                                    vvgpu=[[1,2]],\n",
    "                                    snapshot=20000,  \n",
    "                                    display = 1000,\n",
    "                                    eval_interval = 4000,\n",
    "                                    i64_input_key = True,\n",
    "                                    use_mixed_precision = False,\n",
    "                                    repeat_dataset = True)\n",
    "optimizer = hugectr.optimizer.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.SGD,learning_rate=5,warmup_steps=1000,\n",
    "                                              decay_start=18000,decay_steps=9000,atomic_update=True,use_mixed_precision = False)\n",
    "model = hugectr.Model(solver, optimizer)\n",
    "print(\"model\")\n",
    "model.add(hugectr.Input(data_reader_type = hugectr.DataReaderType_t.Parquet,\n",
    "                            source = \"dlrm/train/_file_list.txt\",\n",
    "                            eval_source = \"dlrm/val/_file_list.txt\",\n",
    "                            check_type = hugectr.Check_t.Non,\n",
    "                            label_dim = 1, label_name = \"label\",\n",
    "                            dense_dim = 13, dense_name = \"dense\",\n",
    "                            slot_size_array = [4976199, 3289052, 282487, 138210, 11, 2203, 8901, 67, 4, 948, 15, 25419, 5577159, 1385790, 4348882, 178673, 10023, 88, 34, 14705, 7112, 19283, 4, 6391, 1282, 60],\n",
    "                            data_reader_sparse_param_array = \n",
    "                            [hugectr.DataReaderSparseParam(hugectr.DataReaderSparse_t.Localized, 30, 1, 26)],\n",
    "                            sparse_names = [\"data1\"]))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.LocalizedSlotSparseEmbeddingHash, \n",
    "                            max_vocabulary_size_per_gpu = 40000000,\n",
    "                            embedding_vec_size = 128,\n",
    "                            combiner = 0,\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"data1\"))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dense\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=512))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=256))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=128))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc3\"],\n",
    "                            top_names = [\"relu3\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Interaction,\n",
    "                            bottom_names = [\"relu3\", \"sparse_embedding1\"],\n",
    "                            top_names = [\"interaction1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"interaction1\"],\n",
    "                            top_names = [\"fc4\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc4\"],\n",
    "                            top_names = [\"relu4\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu4\"],\n",
    "                            top_names = [\"fc5\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc5\"],\n",
    "                            top_names = [\"relu5\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu5\"],\n",
    "                            top_names = [\"fc6\"],\n",
    "                            num_output=512))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc6\"],\n",
    "                            top_names = [\"relu6\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu6\"],\n",
    "                            top_names = [\"fc7\"],\n",
    "                            num_output=256))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc7\"],\n",
    "                            top_names = [\"relu7\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu7\"],\n",
    "                            top_names = [\"fc8\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"fc8\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "enormous-founder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================Model Init====================================\n",
      "[25d11h33m52s][HUGECTR][INFO]: Global seed is 718400599\n",
      "Device 1: Tesla V100-PCIE-32GB\n",
      "Device 2: Tesla V100S-PCIE-32GB\n",
      "model\n",
      "[25d11h33m57s][HUGECTR][INFO]: num of DataReader workers: 2\n",
      "[25d11h33m57s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[25d11h33m57s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[25d11h33m57s][HUGECTR][INFO]: Vocabulary size: 33631364\n",
      "[25d11h33m57s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=40000000\n",
      "[25d11h33m57s][HUGECTR][INFO]: All2All Warmup Start\n",
      "[25d11h33m57s][HUGECTR][INFO]: All2All Warmup End\n",
      "[25d11h34m13s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[25d11h34m13s][HUGECTR][INFO]: gpu1 start to init embedding\n",
      "[25d11h34m13s][HUGECTR][INFO]: gpu1 init embedding done\n",
      "[25d11h34m13s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "==================================Model Summary==================================\n",
      "Label Name                    Dense Name                    Sparse Name                   \n",
      "label                         dense                         data1                         \n",
      "--------------------------------------------------------------------------------\n",
      "Layer Type                    Input Name                    Output Name                   \n",
      "--------------------------------------------------------------------------------\n",
      "LocalizedHash                 data1                         sparse_embedding1             \n",
      "InnerProduct                  dense                         fc1                           \n",
      "ReLU                          fc1                           relu1                         \n",
      "InnerProduct                  relu1                         fc2                           \n",
      "ReLU                          fc2                           relu2                         \n",
      "InnerProduct                  relu2                         fc3                           \n",
      "ReLU                          fc3                           relu3                         \n",
      "Interaction                   relu3, sparse_embedding1      interaction1                  \n",
      "InnerProduct                  interaction1                  fc4                           \n",
      "ReLU                          fc4                           relu4                         \n",
      "InnerProduct                  relu4                         fc5                           \n",
      "ReLU                          fc5                           relu5                         \n",
      "InnerProduct                  relu5                         fc6                           \n",
      "ReLU                          fc6                           relu6                         \n",
      "InnerProduct                  relu6                         fc7                           \n",
      "ReLU                          fc7                           relu7                         \n",
      "InnerProduct                  relu7                         fc8                           \n",
      "BinaryCrossEntropyLoss        fc8, label                    loss                          \n",
      "--------------------------------------------------------------------------------\n",
      "=====================================Model Fit====================================\n",
      "[25d11h34m13s][HUGECTR][INFO]: Use non-epoch mode with number of iterations: 21000\n",
      "[25d11h34m13s][HUGECTR][INFO]: Training batchsize: 2720, evaluation batchsize: 2720\n",
      "[25d11h34m13s][HUGECTR][INFO]: Evaluation interval: 4000, snapshot interval: 20000\n",
      "[25d11h34m20s][HUGECTR][INFO]: Iter: 1000 Time(1000 iters): 7.274753s Loss: 0.498643 lr:5.000000\n",
      "[25d11h34m28s][HUGECTR][INFO]: Iter: 2000 Time(1000 iters): 7.288304s Loss: 0.478844 lr:5.000000\n",
      "[25d11h34m35s][HUGECTR][INFO]: Iter: 3000 Time(1000 iters): 7.246092s Loss: 0.497442 lr:5.000000\n",
      "[25d11h34m42s][HUGECTR][INFO]: Iter: 4000 Time(1000 iters): 7.267779s Loss: 0.492881 lr:5.000000\n",
      "[25d11h34m52s][HUGECTR][INFO]: Evaluation, AUC: 0.777728\n",
      "[25d11h34m52s][HUGECTR][INFO]: Eval Time for 4000 iters: 9.846235s\n",
      "[25d11h35m00s][HUGECTR][INFO]: Iter: 5000 Time(1000 iters): 17.261949s Loss: 0.471097 lr:5.000000\n",
      "[25d11h35m70s][HUGECTR][INFO]: Iter: 6000 Time(1000 iters): 7.255342s Loss: 0.470551 lr:5.000000\n",
      "[25d11h35m14s][HUGECTR][INFO]: Iter: 7000 Time(1000 iters): 7.302215s Loss: 0.463934 lr:5.000000\n",
      "[25d11h35m21s][HUGECTR][INFO]: Iter: 8000 Time(1000 iters): 7.249138s Loss: 0.463335 lr:5.000000\n",
      "[25d11h35m31s][HUGECTR][INFO]: Evaluation, AUC: 0.782700\n",
      "[25d11h35m31s][HUGECTR][INFO]: Eval Time for 4000 iters: 9.935606s\n",
      "[25d11h35m39s][HUGECTR][INFO]: Iter: 9000 Time(1000 iters): 17.279088s Loss: 0.447381 lr:5.000000\n",
      "[25d11h35m46s][HUGECTR][INFO]: Iter: 10000 Time(1000 iters): 7.264433s Loss: 0.473154 lr:5.000000\n",
      "[25d11h35m53s][HUGECTR][INFO]: Iter: 11000 Time(1000 iters): 7.299858s Loss: 0.494980 lr:5.000000\n",
      "[25d11h36m00s][HUGECTR][INFO]: Iter: 12000 Time(1000 iters): 7.259736s Loss: 0.436505 lr:5.000000\n",
      "[25d11h36m10s][HUGECTR][INFO]: Evaluation, AUC: 0.788007\n",
      "[25d11h36m10s][HUGECTR][INFO]: Eval Time for 4000 iters: 9.828027s\n",
      "[25d11h36m18s][HUGECTR][INFO]: Iter: 13000 Time(1000 iters): 17.150100s Loss: 0.427094 lr:5.000000\n",
      "[25d11h36m25s][HUGECTR][INFO]: Iter: 14000 Time(1000 iters): 7.255061s Loss: 0.456496 lr:5.000000\n",
      "[25d11h36m32s][HUGECTR][INFO]: Iter: 15000 Time(1000 iters): 7.279104s Loss: 0.483140 lr:5.000000\n",
      "[25d11h36m39s][HUGECTR][INFO]: Iter: 16000 Time(1000 iters): 7.259845s Loss: 0.454591 lr:5.000000\n",
      "[25d11h36m49s][HUGECTR][INFO]: Evaluation, AUC: 0.790053\n",
      "[25d11h36m49s][HUGECTR][INFO]: Eval Time for 4000 iters: 9.928171s\n",
      "[25d11h36m57s][HUGECTR][INFO]: Iter: 17000 Time(1000 iters): 17.192003s Loss: 0.469906 lr:5.000000\n",
      "[25d11h37m40s][HUGECTR][INFO]: Iter: 18000 Time(1000 iters): 7.284525s Loss: 0.453330 lr:4.998889\n",
      "[25d11h37m11s][HUGECTR][INFO]: Iter: 19000 Time(1000 iters): 7.238128s Loss: 0.444014 lr:3.949630\n",
      "[25d11h37m18s][HUGECTR][INFO]: Iter: 20000 Time(1000 iters): 7.307267s Loss: 0.467373 lr:3.023827\n",
      "[25d11h37m28s][HUGECTR][INFO]: Evaluation, AUC: 0.793590\n",
      "[25d11h37m28s][HUGECTR][INFO]: Eval Time for 4000 iters: 9.873381s\n",
      "[25d11h37m34s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[25d11h37m34s][HUGECTR][INFO]: Rank0: Dump hash table from GPU1\n",
      "[25d11h37m37s][HUGECTR][INFO]: Rank0: Write hash table <key,slot_id,value> pairs to file\n",
      "[25d11h37m42s][HUGECTR][INFO]: Rank0: Write hash table <key,slot_id,value> pairs to file\n",
      "[25d11h37m45s][HUGECTR][INFO]: Done\n"
     ]
    }
   ],
   "source": [
    "!python model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-plane",
   "metadata": {},
   "source": [
    "## 4. Save the Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "united-excellence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 10849372\r\n",
      "-rw-r--r-- 1 root root 11100190464 Mar 25 11:37 0_sparse_20000.model\r\n",
      "-rw-r--r-- 1 root root       38411 Mar 25 12:11 HugeCTR_DLRM_Training.ipynb\r\n",
      "-rw-r--r-- 1 root root     9479684 Mar 25 11:37 _dense_20000.model\r\n",
      "drwxr-xr-x 2 root root        4096 Mar 25 09:12 categories\r\n",
      "drwxr-xr-x 3 root root        4096 Mar 25 09:13 dask-worker-space\r\n",
      "drwxr-xr-x 4 root root        4096 Mar 25 08:30 dlrm\r\n",
      "-rw-r--r-- 1 root root        5655 Mar 25 11:33 model.py\r\n",
      "-rw-r--r-- 1 root root       12372 Mar 25 09:10 preprocess.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
