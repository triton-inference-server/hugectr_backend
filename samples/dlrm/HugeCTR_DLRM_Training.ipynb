{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-mozambique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-globe",
   "metadata": {},
   "source": [
    "# 1.Overview\n",
    "\n",
    "In this notebook, we want to provide an tutorial how to train a standard dlrm model using HugeCTR High-level python API. We will use original Criteo dataset as training data\n",
    "\n",
    "1. [Overview](#1)\n",
    "2. [Dataset Preprocessing](#2)\n",
    "3. [DLRM Model Training](#3)\n",
    "4. [Save the Model Files](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-engagement",
   "metadata": {},
   "source": [
    "# 2. Dataset Preprocessing\n",
    "## 2.1 Generate training and validation data folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "noted-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some data folder to store the original and preprocessed data\n",
    "# Standard Libraries\n",
    "import os\n",
    "from time import time\n",
    "import re\n",
    "import shutil\n",
    "import glob\n",
    "import warnings\n",
    "BASE_DIR = \"/dlrm_train\"\n",
    "train_path  = os.path.join(BASE_DIR, \"train\")\n",
    "val_path = os.path.join(BASE_DIR, \"val\")\n",
    "CUDA_VISIBLE_DEVICES = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
    "n_workers = len(CUDA_VISIBLE_DEVICES.split(\",\"))\n",
    "frac_size = 0.15\n",
    "allow_multi_gpu = False\n",
    "use_rmm_pool = False\n",
    "max_day = None  # (Optional) -- Limit the dataset to day 0-max_day for debugging\n",
    "\n",
    "if os.path.isdir(train_path):\n",
    "    shutil.rmtree(train_path)\n",
    "os.makedirs(train_path)\n",
    "\n",
    "if os.path.isdir(val_path):\n",
    "    shutil.rmtree(val_path)\n",
    "os.makedirs(val_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-coral",
   "metadata": {},
   "source": [
    "## 2.2 Download the Original Criteo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "great-contamination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "wget is already the newest version (1.20.3-1ubuntu1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "asian-command",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-25 12:03:40--  http://azuremlsampleexperiments.blob.core.windows.net/criteo/day_0.gz\n",
      "Resolving azuremlsampleexperiments.blob.core.windows.net (azuremlsampleexperiments.blob.core.windows.net)... 20.60.140.36\n",
      "Connecting to azuremlsampleexperiments.blob.core.windows.net (azuremlsampleexperiments.blob.core.windows.net)|20.60.140.36|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16309554343 (15G) [application/octet-stream]\n",
      "Saving to: ‘./dlrm/train/day_0.gz’\n",
      "\n",
      "day_0.gz               100%[===================================================>]   15G  --.-KB/s    in  6m 12s \n",
      "2021-03-25 12:09:52 (79.2 MB/s) - 'day_0.gz' saved [16309554343/16309554343]"
     ]
    }
   ],
   "source": [
    "!wget -P $train_path http://azuremlsampleexperiments.blob.core.windows.net/criteo/day_0.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "previous-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the split data set to training and validation \n",
    "!gzip -d -c $train_path/day_0.gz > day_0\n",
    "!head -n 45840617 day_0 > $train_path/train.txt \n",
    "!tail -n 2000000 day_0 > $val_path/test.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-spanish",
   "metadata": {},
   "source": [
    "## 2.3 Preprocessing by NVTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "moderate-handy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /dlrm_train/preprocess.py\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "from cudf.io.parquet import ParquetWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures as cf\n",
    "from concurrent.futures import as_completed\n",
    "import shutil\n",
    "\n",
    "import dask_cudf\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "from dask.utils import parse_bytes\n",
    "from dask.delayed import delayed\n",
    "\n",
    "import cudf\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import rmm\n",
    "import nvtabular as nvt\n",
    "from nvtabular.io import Shuffle\n",
    "from nvtabular.ops import Categorify, Clip, FillMissing, HashBucket, LambdaOp, LogOp, Normalize, Rename, get_embedding_sizes\n",
    "from nvtabular.utils import _pynvml_mem_size, device_mem_size\n",
    "\n",
    "#%load_ext memory_profiler\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(message)s')\n",
    "logging.root.setLevel(logging.NOTSET)\n",
    "logging.getLogger('numba').setLevel(logging.WARNING)\n",
    "logging.getLogger('asyncio').setLevel(logging.WARNING)\n",
    "\n",
    "# define dataset schema\n",
    "CATEGORICAL_COLUMNS=[\"C\" + str(x) for x in range(1, 27)]\n",
    "CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "LABEL_COLUMNS = ['label']\n",
    "COLUMNS =  LABEL_COLUMNS + CONTINUOUS_COLUMNS +  CATEGORICAL_COLUMNS\n",
    "#/samples/criteo mode doesn't have dense features\n",
    "criteo_COLUMN=LABEL_COLUMNS +  CATEGORICAL_COLUMNS\n",
    "#For new feature cross columns\n",
    "CROSS_COLUMNS = []\n",
    "\n",
    "\n",
    "NUM_INTEGER_COLUMNS = 13\n",
    "NUM_CATEGORICAL_COLUMNS = 26\n",
    "NUM_TOTAL_COLUMNS = 1 + NUM_INTEGER_COLUMNS + NUM_CATEGORICAL_COLUMNS\n",
    "\n",
    "\n",
    "# Initialize RMM pool on ALL workers\n",
    "def setup_rmm_pool(client, pool_size):\n",
    "    client.run(rmm.reinitialize, pool_allocator=True, initial_pool_size=pool_size)\n",
    "    return None\n",
    "\n",
    "#compute the partition size with GB\n",
    "def bytesto(bytes, to, bsize=1024):\n",
    "    a = {'k' : 1, 'm': 2, 'g' : 3, 't' : 4, 'p' : 5, 'e' : 6 }\n",
    "    r = float(bytes)\n",
    "    return bytes / (bsize ** a[to])\n",
    "\n",
    "\n",
    "#process the data with NVTabular\n",
    "def process_NVT(args):\n",
    "\n",
    "    if args.feature_cross_list:\n",
    "        feature_pairs = [pair.split(\"_\") for pair in args.feature_cross_list.split(\",\")]\n",
    "        for pair in feature_pairs:\n",
    "            CROSS_COLUMNS.append(pair[0]+'_'+pair[1])\n",
    "\n",
    "\n",
    "    logging.info('NVTabular processing')\n",
    "    train_input = os.path.join(args.data_path, \"train/train.txt\")\n",
    "    val_input = os.path.join(args.data_path, \"val/test.txt\")\n",
    "    PREPROCESS_DIR_temp_train = os.path.join(args.out_path, 'train/temp-parquet-after-conversion')\n",
    "    PREPROCESS_DIR_temp_val = os.path.join(args.out_path, 'val/temp-parquet-after-conversion')\n",
    "    PREPROCESS_DIR_temp = [PREPROCESS_DIR_temp_train, PREPROCESS_DIR_temp_val]\n",
    "    train_output = os.path.join(args.out_path, \"train\")\n",
    "    val_output = os.path.join(args.out_path, \"val\")\n",
    "\n",
    "    # Make sure we have a clean parquet space for cudf conversion\n",
    "    for one_path in PREPROCESS_DIR_temp:\n",
    "        if os.path.exists(one_path):\n",
    "           shutil.rmtree(one_path)\n",
    "        os.mkdir(one_path)\n",
    "\n",
    "\n",
    "    ## Get Dask Client\n",
    "\n",
    "    # Deploy a Single-Machine Multi-GPU Cluster\n",
    "    device_size = device_mem_size(kind=\"total\")\n",
    "    cluster = None\n",
    "    if args.protocol == \"ucx\":\n",
    "        UCX_TLS = os.environ.get(\"UCX_TLS\", \"tcp,cuda_copy,cuda_ipc,sockcm\")\n",
    "        os.environ[\"UCX_TLS\"] = UCX_TLS\n",
    "        cluster = LocalCUDACluster(\n",
    "            protocol = args.protocol,\n",
    "            CUDA_VISIBLE_DEVICES = args.devices,\n",
    "            n_workers = len(args.devices.split(\",\")),\n",
    "            enable_nvlink=True,\n",
    "            device_memory_limit = int(device_size * args.device_limit_frac),\n",
    "            dashboard_address=\":\" + args.dashboard_port\n",
    "        )\n",
    "    else:\n",
    "        cluster = LocalCUDACluster(\n",
    "            protocol = args.protocol,\n",
    "            n_workers = len(args.devices.split(\",\")),\n",
    "            CUDA_VISIBLE_DEVICES = args.devices,\n",
    "            device_memory_limit = int(device_size * args.device_limit_frac),\n",
    "            dashboard_address=\":\" + args.dashboard_port\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # Create the distributed client\n",
    "    client = Client(cluster)\n",
    "    if args.device_pool_frac > 0.01:\n",
    "        setup_rmm_pool(client, int(args.device_pool_frac*device_size))\n",
    "\n",
    "\n",
    "    #calculate the total processing time\n",
    "    runtime = time.time()\n",
    "\n",
    "    #test dataset without the label feature\n",
    "    if args.dataset_type == 'test':\n",
    "        global LABEL_COLUMNS\n",
    "        LABEL_COLUMNS = []\n",
    "\n",
    "    ##-----------------------------------##\n",
    "    # Dask rapids converts txt to parquet\n",
    "    # Dask cudf dataframe = ddf\n",
    "\n",
    "    ## train/valid txt to parquet\n",
    "    train_valid_paths = [(train_input,PREPROCESS_DIR_temp_train),(val_input,PREPROCESS_DIR_temp_val)]\n",
    "\n",
    "    for input, temp_output in train_valid_paths:\n",
    "\n",
    "        ddf = dask_cudf.read_csv(input,sep='\\t',names=LABEL_COLUMNS + CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS)\n",
    "\n",
    "        ## Convert label col to FP32\n",
    "        if args.parquet_format and args.dataset_type == 'train':\n",
    "            ddf[\"label\"] = ddf['label'].astype('float32')\n",
    "\n",
    "        # Save it as parquet format for better memory usage\n",
    "        ddf.to_parquet(temp_output,header=True)\n",
    "        ##-----------------------------------##\n",
    "\n",
    "    COLUMNS =  LABEL_COLUMNS + CONTINUOUS_COLUMNS + CROSS_COLUMNS + CATEGORICAL_COLUMNS\n",
    "    train_paths = glob.glob(os.path.join(PREPROCESS_DIR_temp_train, \"*.parquet\"))\n",
    "    valid_paths = glob.glob(os.path.join(PREPROCESS_DIR_temp_val, \"*.parquet\"))\n",
    " \n",
    "    num_buckets=10000000\n",
    "    categorify_op = Categorify(out_path=\"./\", max_size=num_buckets)\n",
    "    cat_features = CATEGORICAL_COLUMNS >> categorify_op\n",
    "    logging.info('Fillmissing processing')\n",
    "    logging.info('Nomalization processing')\n",
    "    cont_features = CONTINUOUS_COLUMNS >> FillMissing() >> Clip(min_value=0) >> Normalize()\n",
    "    features = cat_features + cont_features + LABEL_COLUMNS\n",
    "    workflow = nvt.Workflow(features, client=client)\n",
    "\n",
    "\n",
    "    ##Define the output format##\n",
    "    output_format='hugectr'\n",
    "    if args.parquet_format:\n",
    "        output_format='parquet'\n",
    "    ##--------------------##\n",
    "\n",
    "    # just for /samples/criteo model\n",
    "    train_ds_iterator = nvt.Dataset(train_paths, engine='parquet', part_size=int(args.part_mem_frac * device_size))\n",
    "    valid_ds_iterator = nvt.Dataset(valid_paths, engine='parquet', part_size=int(args.part_mem_frac * device_size))\n",
    "\n",
    "    shuffle = None\n",
    "    if args.shuffle == \"PER_WORKER\":\n",
    "        shuffle = nvt.io.Shuffle.PER_WORKER\n",
    "    elif args.shuffle == \"PER_PARTITION\":\n",
    "        shuffle = nvt.io.Shuffle.PER_PARTITION\n",
    "        \n",
    "    dict_dtypes={}\n",
    "\n",
    "    for col in CATEGORICAL_COLUMNS:\n",
    "        dict_dtypes[col] = np.int64\n",
    "    \n",
    "    for col in CONTINUOUS_COLUMNS:\n",
    "        dict_dtypes[col] = np.float32\n",
    "    \n",
    "    for col in LABEL_COLUMNS:\n",
    "        dict_dtypes[col] = np.float32\n",
    "\n",
    "    logging.info('Train Datasets Preprocessing.....')\n",
    "\n",
    "    workflow.fit(train_ds_iterator)\n",
    "\n",
    "    workflow.transform(train_ds_iterator).to_parquet(output_path=train_output,\n",
    "                                         shuffle=shuffle, \n",
    "                                         dtypes=dict_dtypes,\n",
    "                                         labels=LABEL_COLUMNS,\n",
    "                                         conts=CONTINUOUS_COLUMNS,\n",
    "                                         cats=CATEGORICAL_COLUMNS)\n",
    "\n",
    "\n",
    "\n",
    "    logging.info('Valid Datasets Preprocessing.....')\n",
    "\n",
    "    workflow.transform(valid_ds_iterator).to_parquet(output_path=val_output, \n",
    "                                             dtypes=dict_dtypes,\n",
    "                                             labels=LABEL_COLUMNS,\n",
    "                                             conts=CONTINUOUS_COLUMNS,\n",
    "                                             cats=CATEGORICAL_COLUMNS)\n",
    "    #--------------------##\n",
    "    #Output slot_size for each categorical feature\n",
    "    embeddings = [c[0] for c in categorify_op.get_embedding_sizes(CATEGORICAL_COLUMNS).values()]\n",
    "    embeddings = np.clip(a=embeddings, a_min=None, a_max=num_buckets).tolist()\n",
    "    print(embeddings)\n",
    "    ##--------------------##\n",
    "\n",
    "    ## Shutdown clusters\n",
    "    client.close()\n",
    "    logging.info('NVTabular processing done')\n",
    "\n",
    "    runtime = time.time() - runtime\n",
    "\n",
    "    print(\"\\nDask-NVTabular Criteo Preprocessing\")\n",
    "    print(\"--------------------------------------\")\n",
    "    print(f\"data_path          | {args.data_path}\")\n",
    "    print(f\"output_path        | {args.out_path}\")\n",
    "    print(f\"partition size     | {'%.2f GB'%bytesto(int(args.part_mem_frac * device_size),'g')}\")\n",
    "    print(f\"protocol           | {args.protocol}\")\n",
    "    print(f\"device(s)          | {args.devices}\")\n",
    "    print(f\"rmm-pool-frac      | {(args.device_pool_frac)}\")\n",
    "    print(f\"out-files-per-proc | {args.out_files_per_proc}\")\n",
    "    print(f\"num_io_threads     | {args.num_io_threads}\")\n",
    "    print(f\"shuffle            | {args.shuffle}\")\n",
    "    print(\"======================================\")\n",
    "    print(f\"Runtime[s]         | {runtime}\")\n",
    "    print(\"======================================\\n\")\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=(\"Multi-GPU Criteo Preprocessing\"))\n",
    "\n",
    "    #\n",
    "    # System Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\"--data_path\", type=str, help=\"Input dataset path (Required)\")\n",
    "    parser.add_argument(\"--out_path\", type=str, help=\"Directory path to write output (Required)\")\n",
    "    parser.add_argument(\n",
    "        \"-d\",\n",
    "        \"--devices\",\n",
    "        default=os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\"),\n",
    "        type=str,\n",
    "        help='Comma-separated list of visible devices (e.g. \"0,1,2,3\"). '\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-p\",\n",
    "        \"--protocol\",\n",
    "        choices=[\"tcp\", \"ucx\"],\n",
    "        default=\"tcp\",\n",
    "        type=str,\n",
    "        help=\"Communication protocol to use (Default 'tcp')\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device_limit_frac\",\n",
    "        default=0.5,\n",
    "        type=float,\n",
    "        help=\"Worker device-memory limit as a fraction of GPU capacity (Default 0.8). \"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device_pool_frac\",\n",
    "        default=0.9,\n",
    "        type=float,\n",
    "        help=\"RMM pool size for each worker  as a fraction of GPU capacity (Default 0.9). \"\n",
    "        \"The RMM pool frac is the same for all GPUs, make sure each one has enough memory size\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_io_threads\",\n",
    "        default=0,\n",
    "        type=int,\n",
    "        help=\"Number of threads to use when writing output data (Default 0). \"\n",
    "        \"If 0 is specified, multi-threading will not be used for IO.\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Data-Decomposition Parameters\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--part_mem_frac\",\n",
    "        default=0.125,\n",
    "        type=float,\n",
    "        help=\"Maximum size desired for dataset partitions as a fraction \"\n",
    "        \"of GPU capacity (Default 0.125)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out_files_per_proc\",\n",
    "        default=8,\n",
    "        type=int,\n",
    "        help=\"Number of output files to write on each worker (Default 8)\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Preprocessing Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"-f\",\n",
    "        \"--freq_limit\",\n",
    "        default=0,\n",
    "        type=int,\n",
    "        help=\"Frequency limit for categorical encoding (Default 0)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-s\",\n",
    "        \"--shuffle\",\n",
    "        choices=[\"PER_WORKER\", \"PER_PARTITION\", \"NONE\"],\n",
    "        default=\"PER_PARTITION\",\n",
    "        help=\"Shuffle algorithm to use when writing output data to disk (Default PER_PARTITION)\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--feature_cross_list\", default=None, type=str, help=\"List of feature crossing cols (e.g. C1_C2, C3_C4)\"\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Diagnostics Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--profile\",\n",
    "        metavar=\"PATH\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"Specify a file path to export a Dask profile report (E.g. dask-report.html).\"\n",
    "        \"If this option is excluded from the command, not profile will be exported\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dashboard_port\",\n",
    "        default=\"8787\",\n",
    "        type=str,\n",
    "        help=\"Specify the desired port of Dask's diagnostics-dashboard (Default `3787`). \"\n",
    "        \"The dashboard will be hosted at http://<IP>:<PORT>/status\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Format\n",
    "    #\n",
    "\n",
    "    parser.add_argument('--parquet_format', type=int, default=1)\n",
    "    parser.add_argument('--dataset_type', type=str, default='train')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.n_workers = len(args.devices.split(\",\"))\n",
    "    return args\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args = parse_args()\n",
    "\n",
    "    process_NVT(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "practical-distance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-06 09:35:05,154 NVTabular processing\n",
      "2021-07-06 09:35:36,400 Fillmissing processing\n",
      "2021-07-06 09:35:36,400 Nomalization processing\n",
      "2021-07-06 09:35:37,128 Train Datasets Preprocessing.....\n",
      "2021-07-06 09:37:16,673 Valid Datasets Preprocessing.....\n",
      "[4976199, 3289052, 282487, 138210, 11, 2203, 8901, 67, 4, 948, 15, 25419, 5577159, 1385790, 4348882, 178673, 10023, 88, 34, 14705, 7112, 19283, 4, 6391, 1282, 60]\n",
      "2021-07-06 09:37:18,707 NVTabular processing done\n",
      "\n",
      "Dask-NVTabular Criteo Preprocessing\n",
      "--------------------------------------\n",
      "data_path          | /dlrm_train\n",
      "output_path        | /dlrm_train\n",
      "partition size     | 2.77 GB\n",
      "protocol           | tcp\n",
      "device(s)          | 0\n",
      "rmm-pool-frac      | 0.5\n",
      "out-files-per-proc | 1\n",
      "num_io_threads     | 2\n",
      "shuffle            | PER_PARTITION\n",
      "======================================\n",
      "Runtime[s]         | 127.28504157066345\n",
      "======================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 ./preprocess.py --data_path /dlrm_train --out_path /dlrm_train --freq_limit 6 --device_limit_frac 0.5 --device_pool_frac 0.5 --out_files_per_proc 1  --devices \"0\" --num_io_threads 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-console",
   "metadata": {},
   "source": [
    "## 3. DLRM Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "affected-establishment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile './model.py'\n",
    "import hugectr\n",
    "#from mpi4py import MPI\n",
    "\n",
    "# 1. Create Solver, DataReaderParams and Optimizer\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 300,\n",
    "                              batchsize_eval = 16384,\n",
    "                              batchsize = 16384,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[0]],\n",
    "                              repeat_dataset = True,\n",
    "                              i64_input_key = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Parquet,\n",
    "                                  source = [\"./train/_file_list.txt\"],\n",
    "                                  eval_source = \"./val/_file_list.txt\",\n",
    "                                  slot_size_array = [4976199, 3289052, 282487, 138210, 11, 2203, 8901, 67, 4, 948, 15, 25419, 5577159, 1385790, 4348882, 178673, 10023, 88, 34, 14705, 7112, 19283, 4, 6391, 1282, 60],\n",
    "                                  check_type = hugectr.Check_t.Non)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam,\n",
    "                                    update_type = hugectr.Update_t.Global,\n",
    "                                    beta1 = 0.9,\n",
    "                                    beta2 = 0.999,\n",
    "                                    epsilon = 0.0001)\n",
    "# 2. Initialize the Model instance\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "\n",
    "# 3. Construct the Model graph\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"data1\", 2, False, 26)]))\n",
    "                        \n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.LocalizedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 10000,\n",
    "                            embedding_vec_size = 128,                            \n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"data1\",\n",
    "                            slot_size_array = [4976199, 3289052, 282487, 138210, 11, 2203, 8901, 67, 4, 948, 15, 25419, 5577159, 1385790, 4348882, 178673, 10023, 88, 34, 14705, 7112, 19283, 4, 6391, 1282, 60],\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dense\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=512))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))                           \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=256))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))                            \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=128))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc3\"],\n",
    "                            top_names = [\"relu3\"]))                              \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Interaction,\n",
    "                            bottom_names = [\"relu3\",\"sparse_embedding1\"],\n",
    "                            top_names = [\"interaction1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"interaction1\"],\n",
    "                            top_names = [\"fc4\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc4\"],\n",
    "                            top_names = [\"relu4\"]))                              \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu4\"],\n",
    "                            top_names = [\"fc5\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc5\"],\n",
    "                            top_names = [\"relu5\"]))                              \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu5\"],\n",
    "                            top_names = [\"fc6\"],\n",
    "                            num_output=512))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc6\"],\n",
    "                            top_names = [\"relu6\"]))                               \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu6\"],\n",
    "                            top_names = [\"fc7\"],\n",
    "                            num_output=256))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc7\"],\n",
    "                            top_names = [\"relu7\"]))                                                                              \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu7\"],\n",
    "                            top_names = [\"fc8\"],\n",
    "                            num_output=1))                                                                                           \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"fc8\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "\n",
    "# 4. Dump the Model graph to JSON\n",
    "model.graph_to_json(graph_config_file = \"dlrm.json\")\n",
    "\n",
    "# 5. Compile & Fit\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.fit(max_iter = 21000, display = 1000, eval_interval = 4000, snapshot = 20000, snapshot_prefix = \"dlrm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bf0493f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================Model Init=====================================================\n",
      "[06d09h56m05s][HUGECTR][INFO]: Global seed is 3700572180\n",
      "[06d09h56m07s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "Device 0: NVIDIA A100-PCIE-40GB\n",
      "[06d09h56m07s][HUGECTR][INFO]: num of DataReader workers: 1\n",
      "[06d09h56m07s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[06d09h56m07s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[06d09h56m07s][HUGECTR][INFO]: Vocabulary size: 20273002\n",
      "[06d09h56m07s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=20273002\n",
      "[06d09h56m07s][HUGECTR][INFO]: All2All Warmup Start\n",
      "[06d09h56m07s][HUGECTR][INFO]: All2All Warmup End\n",
      "[06d09h56m07s][HUGECTR][INFO]: Save the model graph to dlrm.json, successful\n",
      "===================================================Model Compile===================================================\n",
      "[06d09h58m11s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[06d09h58m11s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "===================================================Model Summary===================================================\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data1                         \n",
      "(None, 1)                               (None, 13)                              \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "LocalizedSlotSparseEmbeddingHash        data1                         sparse_embedding1             (None, 26, 128)               \n",
      "InnerProduct                            dense                         fc1                           (None, 512)                   \n",
      "ReLU                                    fc1                           relu1                         (None, 512)                   \n",
      "InnerProduct                            relu1                         fc2                           (None, 256)                   \n",
      "ReLU                                    fc2                           relu2                         (None, 256)                   \n",
      "InnerProduct                            relu2                         fc3                           (None, 128)                   \n",
      "ReLU                                    fc3                           relu3                         (None, 128)                   \n",
      "Interaction                             relu3,sparse_embedding1       interaction1                  (None, 480)                   \n",
      "InnerProduct                            interaction1                  fc4                           (None, 1024)                  \n",
      "ReLU                                    fc4                           relu4                         (None, 1024)                  \n",
      "InnerProduct                            relu4                         fc5                           (None, 1024)                  \n",
      "ReLU                                    fc5                           relu5                         (None, 1024)                  \n",
      "InnerProduct                            relu5                         fc6                           (None, 512)                   \n",
      "ReLU                                    fc6                           relu6                         (None, 512)                   \n",
      "InnerProduct                            relu6                         fc7                           (None, 256)                   \n",
      "ReLU                                    fc7                           relu7                         (None, 256)                   \n",
      "InnerProduct                            relu7                         fc8                           (None, 1)                     \n",
      "BinaryCrossEntropyLoss                  fc8,label                     loss                                                        \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[60d90h58m11s][HUGECTR][INFO]: Use non-epoch mode with number of iterations: 21000\n",
      "[60d90h58m11s][HUGECTR][INFO]: Training batchsize: 16384, evaluation batchsize: 16384\n",
      "[60d90h58m11s][HUGECTR][INFO]: Evaluation interval: 4000, snapshot interval: 20000\n",
      "[60d90h58m11s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[60d90h58m11s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[60d90h58m11s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[60d90h58m11s][HUGECTR][INFO]: Training source file: ./train/_file_list.txt\n",
      "[60d90h58m11s][HUGECTR][INFO]: Evaluation source file: ./val/_file_list.txt\n",
      "[60d10h00m22s][HUGECTR][INFO]: Iter: 1000 Time(1000 iters): 131.474668s Loss: 0.121299 lr:0.001000\n",
      "[60d10h20m34s][HUGECTR][INFO]: Iter: 2000 Time(1000 iters): 131.900698s Loss: 0.134545 lr:0.001000\n",
      "[60d10h40m46s][HUGECTR][INFO]: Iter: 3000 Time(1000 iters): 131.877820s Loss: 0.119247 lr:0.001000\n",
      "[60d10h60m57s][HUGECTR][INFO]: Iter: 4000 Time(1000 iters): 131.141643s Loss: 0.122298 lr:0.001000\n",
      "[60d10h70m50s][HUGECTR][INFO]: Evaluation, AUC: 0.771469\n",
      "[60d10h70m50s][HUGECTR][INFO]: Eval Time for 300 iters: 7.068447s\n",
      "[60d10h90m16s][HUGECTR][INFO]: Iter: 5000 Time(1000 iters): 138.728214s Loss: 0.116469 lr:0.001000\n",
      "[60d10h11m27s][HUGECTR][INFO]: Iter: 6000 Time(1000 iters): 131.255541s Loss: 0.115536 lr:0.001000\n",
      "[60d10h13m38s][HUGECTR][INFO]: Iter: 7000 Time(1000 iters): 130.882900s Loss: 0.121121 lr:0.001000\n",
      "[60d10h15m50s][HUGECTR][INFO]: Iter: 8000 Time(1000 iters): 131.488348s Loss: 0.113835 lr:0.001000\n",
      "[60d10h15m57s][HUGECTR][INFO]: Evaluation, AUC: 0.776787\n",
      "[60d10h15m57s][HUGECTR][INFO]: Eval Time for 300 iters: 7.115201s\n",
      "[60d10h18m80s][HUGECTR][INFO]: Iter: 9000 Time(1000 iters): 137.966580s Loss: 0.117214 lr:0.001000\n",
      "[60d10h20m19s][HUGECTR][INFO]: Iter: 10000 Time(1000 iters): 131.023316s Loss: 0.140116 lr:0.001000\n",
      "[60d10h22m30s][HUGECTR][INFO]: Iter: 11000 Time(1000 iters): 131.593101s Loss: 0.118429 lr:0.001000\n",
      "[60d10h24m41s][HUGECTR][INFO]: Iter: 12000 Time(1000 iters): 130.668333s Loss: 0.114565 lr:0.001000\n",
      "[60d10h24m48s][HUGECTR][INFO]: Evaluation, AUC: 0.776194\n",
      "[60d10h24m48s][HUGECTR][INFO]: Eval Time for 300 iters: 7.110022s\n",
      "[60d10h26m59s][HUGECTR][INFO]: Iter: 13000 Time(1000 iters): 138.161077s Loss: 0.132257 lr:0.001000\n",
      "[60d10h29m11s][HUGECTR][INFO]: Iter: 14000 Time(1000 iters): 131.537853s Loss: 0.118791 lr:0.001000\n",
      "[60d10h31m21s][HUGECTR][INFO]: Iter: 15000 Time(1000 iters): 130.529050s Loss: 0.119272 lr:0.001000\n",
      "[60d10h33m32s][HUGECTR][INFO]: Iter: 16000 Time(1000 iters): 131.209768s Loss: 0.122578 lr:0.001000\n",
      "[60d10h33m39s][HUGECTR][INFO]: Evaluation, AUC: 0.767596\n",
      "[60d10h33m39s][HUGECTR][INFO]: Eval Time for 300 iters: 7.113982s\n",
      "[60d10h35m51s][HUGECTR][INFO]: Iter: 17000 Time(1000 iters): 138.247108s Loss: 0.116277 lr:0.001000\n",
      "[60d10h38m10s][HUGECTR][INFO]: Iter: 18000 Time(1000 iters): 130.656860s Loss: 0.119150 lr:0.001000\n",
      "[60d10h40m13s][HUGECTR][INFO]: Iter: 19000 Time(1000 iters): 131.308094s Loss: 0.112350 lr:0.001000\n",
      "[60d10h42m23s][HUGECTR][INFO]: Iter: 20000 Time(1000 iters): 130.900344s Loss: 0.109148 lr:0.001000\n",
      "[60d10h42m31s][HUGECTR][INFO]: Evaluation, AUC: 0.755371\n",
      "[60d10h42m31s][HUGECTR][INFO]: Eval Time for 300 iters: 7.102309s\n",
      "[60d10h42m34s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[60d10h42m34s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[60d10h42m38s][HUGECTR][INFO]: Done\n",
      "[60d10h42m40s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[60d10h42m47s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[60d10h42m54s][HUGECTR][INFO]: Done\n",
      "[60d10h43m10s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[60d10h43m80s][HUGECTR][INFO]: Done\n",
      "[60d10h43m80s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60d10h43m80s][HUGECTR][INFO]: Dumping dense weights to file, successful\r\n",
      "[60d10h43m80s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\r\n",
      "[60d10h43m80s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\r\n"
     ]
    }
   ],
   "source": [
    "!python model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-plane",
   "metadata": {},
   "source": [
    "## 4. Save the Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "united-excellence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 20759554048 Jul  6 10:43 dlrm0_opt_sparse_20000.model\r\n",
      "-rw-r--r-- 1 root root     9479684 Jul  6 10:43 dlrm_dense_20000.model\r\n",
      "-rw-r--r-- 1 root root    18959368 Jul  6 10:43 dlrm_opt_dense_20000.model\r\n",
      "\r\n",
      "dlrm0_sparse_20000.model:\r\n",
      "total 7135028\r\n",
      "-rw-r--r-- 1 root root 7084856832 Jul  6 10:42 emb_vector\r\n",
      "-rw-r--r-- 1 root root  110700888 Jul  6 10:42 key\r\n",
      "-rw-r--r-- 1 root root  110700888 Jul  6 10:42 slot_id\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l *20000.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020cca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
