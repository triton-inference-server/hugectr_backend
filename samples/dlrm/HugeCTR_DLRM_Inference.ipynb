{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "electronic-terry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-greek",
   "metadata": {},
   "source": [
    "# 1 Overview\n",
    "In this notebook, we want to provide a tutorial on how to use standard DLRM model that trained on HugeCTR_DLRM_Training.\n",
    "notebook and deploy the saved model to Triton Inference Server. We could collect the inference benchmark by Triton performance analyzer  tool\n",
    "\n",
    "1. [Overview](#1)\n",
    "2. [Generate the DLRM Deployment Configuration](#2)\n",
    "3. [Load Models on Triton Server](#3)\n",
    "4. [Prepare Inference Input Data](#4) \n",
    "5. [Inference Benchmarm by Triton Performance Tool](#5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-formula",
   "metadata": {},
   "source": [
    "# 2. Generate the DLRM Deployment Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-minority",
   "metadata": {},
   "source": [
    "## 2.1 Generate related model folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "opposite-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some data folder to store the model related files\n",
    "# Standard Libraries\n",
    "import os\n",
    "from time import time\n",
    "import re\n",
    "import shutil\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "BASE_DIR = \"/dlrm_infer\"\n",
    "model_folder  = os.path.join(BASE_DIR, \"model\")\n",
    "dlrm_model_repo= os.path.join(model_folder, \"dlrm\")\n",
    "dlrm_version =os.path.join(dlrm_model_repo, \"1\")\n",
    "\n",
    "if os.path.isdir(model_folder):\n",
    "    shutil.rmtree(model_folder)\n",
    "os.makedirs(model_folder)\n",
    "\n",
    "if os.path.isdir(dlrm_model_repo):\n",
    "    shutil.rmtree(dlrm_model_repo)\n",
    "os.makedirs(dlrm_model_repo)\n",
    "\n",
    "if os.path.isdir(dlrm_version):\n",
    "    shutil.rmtree(dlrm_version)\n",
    "os.makedirs(dlrm_version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-malawi",
   "metadata": {},
   "source": [
    "### 2.2 Copy DLRM model files to model repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "divine-sterling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 10849300\r\n",
      "-rw-r--r-- 1 root root 11100190464 Mar 30 03:28 0_sparse_20000.model\r\n",
      "-rw-r--r-- 1 root root     9479684 Mar 30 03:28 _dense_20000.model\r\n"
     ]
    }
   ],
   "source": [
    "! cp /dlrm_train/0_sparse_20000.model $dlrm_version/\n",
    "! cp /dlrm_train/_dense_20000.model $dlrm_version/\n",
    "!ls -l $dlrm_version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-kitty",
   "metadata": {},
   "source": [
    "### 2.3 Generate the Triton configuration for deploying DLRM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "minor-singer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /dlrm_infer/model/dlrm/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile $dlrm_model_repo/config.pbtxt\n",
    "name: \"dlrm\"\n",
    "backend: \"hugectr\"\n",
    "max_batch_size:1,\n",
    "input [\n",
    "   {\n",
    "    name: \"DES\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"CATCOLUMN\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"ROWINDEX\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind : KIND_GPU\n",
    "    gpus:[2]\n",
    "  }\n",
    "]\n",
    "\n",
    "parameters [\n",
    "  {\n",
    "  key: \"config\"\n",
    "  value: { string_value: \"/dlrm_infer/model/dlrm/1/dlrm.json\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"gpucache\"\n",
    "  value: { string_value: \"true\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"hit_rate_threshold\"\n",
    "  value: { string_value: \"0.8\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"gpucacheper\"\n",
    "  value: { string_value: \"0.5\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"label_dim\"\n",
    "  value: { string_value: \"1\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"slots\"\n",
    "  value: { string_value: \"26\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"cat_feature_num\"\n",
    "  value: { string_value: \"26\" }\n",
    "  },\n",
    " {\n",
    "  key: \"des_feature_num\"\n",
    "  value: { string_value: \"13\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"max_nnz\"\n",
    "  value: { string_value: \"2\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"embedding_vector_size\"\n",
    "  value: { string_value: \"128\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"embeddingkey_long_type\"\n",
    "  value: { string_value: \"true\" }\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-investigator",
   "metadata": {},
   "source": [
    "### 2.4 Generate the Hugectr Backend configuration for deploying dlrm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "subsequent-overall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /dlrm_infer/model/dlrm/1/dlrm.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile $dlrm_version/dlrm.json\n",
    "{\n",
    "\"layers\": [\n",
    "        {\n",
    "       \"name\": \"data\",\n",
    "        \"type\": \"Data\",\n",
    "        \"format\": \"Raw\",\n",
    "        \"check\": \"None\",\n",
    "        \"label\": {\n",
    "                \"top\": \"label\",\n",
    "                \"label_dim\": 1\n",
    "        },\n",
    "        \"dense\": {\n",
    "                \"top\": \"dense\",\n",
    "                \"dense_dim\": 13\n",
    "        },\n",
    "        \"sparse\": [\n",
    "                {\n",
    "            \"top\": \"data1\",\n",
    "            \"type\": \"LocalizedSlot\",\n",
    "            \"max_feature_num_per_sample\": 26,\n",
    "            \"max_nnz\": 1,\n",
    "            \"slot_num\": 26\n",
    "                }\n",
    "        ]\n",
    "      },\n",
    "\n",
    "\n",
    "      {\n",
    "        \"name\": \"sparse_embedding1\",\n",
    "        \"type\": \"LocalizedSlotSparseEmbeddingHash\",\n",
    "        \"bottom\": \"data1\",\n",
    "        \"top\": \"sparse_embedding1\",\n",
    "        \"sparse_embedding_hparam\": {\n",
    "          \"slot_size_array\": [4976199, 3289052, 282487, 138210, 11, 2203, 8901, 67, 4, 948, 15, 25419, 5577159, 1385790, 4348882, 178673, 10023, 88, 34, 14705, 7112, 19283, 4, 6391, 1282, 60],\n",
    "          \"embedding_vec_size\": 128,\n",
    "          \"combiner\": 0\n",
    "        }\n",
    "      },\n",
    "\n",
    "        {\n",
    "        \"name\": \"fc1\",\n",
    "        \"type\": \"InnerProduct\",\n",
    "        \"bottom\": \"dense\",\n",
    "        \"top\": \"fc1\",\n",
    "\"fc_param\": {\n",
    "          \"num_output\": 512\n",
    "        }\n",
    "      },\n",
    "\n",
    "      {\n",
    "        \"name\": \"relu1\",\n",
    "        \"type\": \"ReLU\",\n",
    "        \"bottom\": \"fc1\",\n",
    "        \"top\": \"relu1\"\n",
    "      },\n",
    "\n",
    "      {\n",
    "        \"name\": \"fc2\",\n",
    "        \"type\": \"InnerProduct\",\n",
    "        \"bottom\": \"relu1\",\n",
    "        \"top\": \"fc2\",\n",
    "         \"fc_param\": {\n",
    "          \"num_output\": 256\n",
    "        }\n",
    "      },\n",
    "\n",
    "      {\n",
    "        \"name\": \"relu2\",\n",
    "        \"type\": \"ReLU\",\n",
    "        \"bottom\": \"fc2\",\n",
    "        \"top\": \"relu2\"\n",
    "      },\n",
    "\n",
    "      {\n",
    "        \"name\": \"fc3\",\n",
    "        \"type\": \"InnerProduct\",\n",
    "        \"bottom\": \"relu2\",\n",
    "        \"top\": \"fc3\",\n",
    "         \"fc_param\": {\n",
    "          \"num_output\": 128\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"relu3\",\n",
    "        \"type\": \"ReLU\",\n",
    "        \"bottom\": \"fc3\",\n",
    "        \"top\": \"relu3\"\n",
    "      },\n",
    "\n",
    "      {\n",
    "        \"name\": \"interaction1\",\n",
    "        \"type\": \"Interaction\",\n",
    "        \"bottom\": [\"relu3\", \"sparse_embedding1\"],\n",
    "        \"top\": \"interaction1\"\n",
    "      },\n",
    "\n",
    "      {\n",
    "        \"name\": \"fc4\",\n",
    "        \"type\": \"InnerProduct\",\n",
    "        \"bottom\": \"interaction1\",\n",
    "        \"top\": \"fc4\",\n",
    "        \"fc_param\": {\n",
    "          \"num_output\": 1024\n",
    "        }\n",
    "      },\n",
    "\n",
    "      {\n",
    "        \"name\": \"relu4\",\n",
    "        \"type\": \"ReLU\",\n",
    "        \"bottom\": \"fc4\",\n",
    "        \"top\": \"relu4\"\n",
    "      },\n",
    "\n",
    "\n",
    "      {\n",
    "        \"name\": \"fc5\",\n",
    "        \"type\": \"InnerProduct\",\n",
    "        \"bottom\": \"relu4\",\n",
    "        \"top\": \"fc5\",\n",
    "         \"fc_param\": {\n",
    "          \"num_output\": 1024\n",
    "        }\n",
    "      },\n",
    "\n",
    "      {\n",
    "        \"name\": \"relu5\",\n",
    "        \"type\": \"ReLU\",\n",
    "        \"bottom\": \"fc5\",\n",
    "        \"top\": \"relu5\"\n",
    "      },\n",
    "\n",
    "      {\n",
    "        \"name\": \"fc6\",\n",
    "        \"type\": \"InnerProduct\",\n",
    "        \"bottom\": \"relu5\",\n",
    "        \"top\": \"fc6\",\n",
    "         \"fc_param\": {\n",
    "          \"num_output\": 512\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"relu6\",\n",
    "        \"type\": \"ReLU\",\n",
    "        \"bottom\": \"fc6\",\n",
    "        \"top\": \"relu6\"\n",
    "      },\n",
    "\n",
    "      {\n",
    "        \"name\": \"fc7\",\n",
    "        \"type\": \"InnerProduct\",\n",
    "        \"bottom\": \"relu6\",\n",
    " \"top\": \"fc7\",\n",
    "         \"fc_param\": {\n",
    "          \"num_output\": 256\n",
    "        }\n",
    "      },\n",
    "\n",
    "      {\n",
    "        \"name\": \"relu7\",\n",
    "        \"type\": \"ReLU\",\n",
    "        \"bottom\": \"fc7\",\n",
    "        \"top\": \"relu7\"\n",
    "      },\n",
    "\n",
    "      {\n",
    "        \"name\": \"fc8\",\n",
    "        \"type\": \"InnerProduct\",\n",
    "        \"bottom\": \"relu7\",\n",
    "        \"top\": \"fc8\",\n",
    "         \"fc_param\": {\n",
    "          \"num_output\": 1\n",
    "        }\n",
    "      },\n",
    "{\n",
    "      \"name\": \"sigmoid\",\n",
    "      \"type\": \"Sigmoid\",\n",
    "      \"bottom\": \"fc8\",\n",
    "      \"top\": \"sigmoid\"\n",
    "    }\n",
    "\n",
    "    ]\n",
    "  }\n"
   ]
  },
  {
   "source": [
    "### 2.5 Generate the Hugectr Backend parameter server configuration for deploying dlrm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $model_folder/ps.json\n",
    "{\n",
    "    \"supportlonglong\":true,\n",
    "    \"models\":[\n",
    "        {\n",
    "            \"model\":\"dlrm\",\n",
    "            \"sparse_files\":[\"/dlrm_infer/model/dlrm/1/0_sparse_20000.model\"],\n",
    "            \"dense_file\":\"/dlrm_infer/model/dlrm/1/_dense_20000.model\",\n",
    "            \"network_file\":\"/dlrm_infer/model/dlrm/1/dlrm.json\"\n",
    "        }\n",
    "    ]  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "interested-catalog",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 10849304\n",
      "-rw-r--r-- 1 root root 11100190464 Mar 30 03:28 0_sparse_20000.model\n",
      "-rw-r--r-- 1 root root     9479684 Mar 30 03:28 _dense_20000.model\n",
      "-rw-r--r-- 1 root root        3835 Mar 30 03:30 dlrm.json\n",
      "total 8\n",
      "drwxr-xr-x 2 root root 4096 Mar 30 03:30 1\n",
      "-rw-r--r-- 1 root root 1107 Mar 30 03:30 config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!ls -l $dlrm_version\n",
    "!ls -l $dlrm_model_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-matthew",
   "metadata": {},
   "source": [
    "## 3. Deploy DLRM on Triton Server\n",
    "At this stage, you should have already launched the Triton Inference Server with the following command:\n",
    "\n",
    "In this tutorial, we will deploy the DLRM to a single V100(32GB)\n",
    "\n",
    "docker run --gpus=all -it -v /dlrm_infer/:/dlrm_infer -v /dlrm_train/:/dlrm_train --net=host nvcr.io/nvidia/merlin/merlin-inference:0.5.2 /bin/bash\n",
    "\n",
    "After you enter into the container you can launch triton server with the command below:\n",
    "\n",
    "tritonserver --model-repository=/dlrm_infer/model/ --load-model=dlrm \n",
    "    --model-control-mode=explicit \n",
    "    --backend-directory=/usr/local/hugectr/backends \n",
    "    --backend-config=hugectr,ps=/dlrm_infer/model/ps.json \n",
    "    \n",
    "Note: The model-repository path is /dlrm_infer/model/. The path for the dlrm model network json file is /dlrm_infer/model/dlrm/1/dlrm.json. The path for the parameter server configuration file is /dlrm_infer/model/ps.json."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-morning",
   "metadata": {},
   "source": [
    "## 4. Prepare Inference Input Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-webmaster",
   "metadata": {},
   "source": [
    "### 4.1 Read validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "similar-somalia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4243316\r\n",
      "-rw-r--r-- 1 root root  704993648 Mar 25 13:31 0.83ab760d4f4b4505a397e9b90247eb4a.parquet\r\n",
      "-rw-r--r-- 1 root root         54 Mar 25 13:31 _file_list.txt\r\n",
      "-rw-r--r-- 1 root root     113813 Mar 25 13:31 _metadata\r\n",
      "-rw-r--r-- 1 root root       1465 Mar 25 13:31 _metadata.json\r\n",
      "drwxr-xr-x 2 root root       4096 Mar 25 13:29 temp-parquet-after-conversion\r\n",
      "-rw-r--r-- 1 root root 2230023090 Mar 25 08:35 test.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /dlrm_train/dlrm/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "varied-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_parquet('/dlrm_train/dlrm/val/0.83ab760d4f4b4505a397e9b90247eb4a.parquet',engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sticky-century",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>...</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>I11</th>\n",
       "      <th>I12</th>\n",
       "      <th>I13</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>687</td>\n",
       "      <td>75</td>\n",
       "      <td>160</td>\n",
       "      <td>134261</td>\n",
       "      <td>62</td>\n",
       "      <td>10</td>\n",
       "      <td>7542</td>\n",
       "      <td>229</td>\n",
       "      <td>3</td>\n",
       "      <td>89976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117298</td>\n",
       "      <td>-0.264598</td>\n",
       "      <td>-0.241559</td>\n",
       "      <td>-0.569952</td>\n",
       "      <td>-0.441110</td>\n",
       "      <td>-0.570183</td>\n",
       "      <td>-0.5111</td>\n",
       "      <td>-0.0848</td>\n",
       "      <td>-0.436591</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>630</td>\n",
       "      <td>156</td>\n",
       "      <td>46</td>\n",
       "      <td>590512</td>\n",
       "      <td>44</td>\n",
       "      <td>23</td>\n",
       "      <td>10501</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>22899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239154</td>\n",
       "      <td>-0.264598</td>\n",
       "      <td>-0.241559</td>\n",
       "      <td>-0.749732</td>\n",
       "      <td>-0.468817</td>\n",
       "      <td>-0.570183</td>\n",
       "      <td>-0.5111</td>\n",
       "      <td>-0.0848</td>\n",
       "      <td>-0.436591</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    C1   C2   C3      C4  C5  C6     C7   C8  C9    C10  ...        I5  \\\n",
       "0  687   75  160  134261  62  10   7542  229   3  89976  ... -0.117298   \n",
       "1  630  156   46  590512  44  23  10501    2   3  22899  ...  0.239154   \n",
       "\n",
       "         I6        I7        I8        I9       I10     I11     I12       I13  \\\n",
       "0 -0.264598 -0.241559 -0.569952 -0.441110 -0.570183 -0.5111 -0.0848 -0.436591   \n",
       "1 -0.264598 -0.241559 -0.749732 -0.468817 -0.570183 -0.5111 -0.0848 -0.436591   \n",
       "\n",
       "   label  \n",
       "0    1.0  \n",
       "1    0.0  \n",
       "\n",
       "[2 rows x 40 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "third-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(200000).to_csv('infer_test.txt', sep='\\t', index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-fight",
   "metadata": {},
   "source": [
    "### 4.2 Follow the Triton requirements to generate input data with json format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "complex-grass",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./criteo2predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./criteo2predict.py\n",
    "import argparse\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "def parse_config(src_config):\n",
    "    try:\n",
    "        with open(src_config, 'r') as data_json:\n",
    "            j_data = json.load(data_json)\n",
    "            dense_dim = j_data[\"dense\"]\n",
    "            categorical_dim = j_data[\"categorical\"]\n",
    "            slot_size = j_data[\"slot_size\"]\n",
    "        assert(categorical_dim == np.sum(slot_size))\n",
    "        return dense_dim, categorical_dim, slot_size\n",
    "    except:\n",
    "        print(\"Invalid data configuration file!\")\n",
    "\n",
    "def convert(src_csv, src_config, dst, batch_size,segmentation):\n",
    "    dense_dim, categorical_dim, slot_size = parse_config(src_config)\n",
    "    slot_size_array=[4976199, 3289052, 282487, 138210, 11, 2203, 8901, 67, 4, 948, 15, 25419, 5577159, 1385790, 4348882, 178673, 10023, 88, 34, 14705, 7112, 19283, 4, 6391, 1282, 60]\n",
    "    offset = np.insert(np.cumsum(slot_size_array), 0, 0)[:-1]\n",
    "    total_columns = 1 + dense_dim + categorical_dim\n",
    "    df = pd.read_csv(src_csv,  sep='\\t', nrows=batch_size)\n",
    "    cols = df.columns\n",
    "    slot_num = len(slot_size)\n",
    "    row_ptrs = [0 for _ in range(batch_size*slot_num + 1)]\n",
    "    for i in range(1, len(row_ptrs)):\n",
    "        row_ptrs[i] = row_ptrs[i-1] + slot_size[(i-1)%slot_num]\n",
    "    label_df =  pd.DataFrame(df['label'].values.reshape(1,batch_size))\n",
    "    dense_df = pd.DataFrame(df[['I'+str(i+1) for i in range(dense_dim)]].values.reshape(1, batch_size*dense_dim))\n",
    "    embedding_columns_df = pd.DataFrame(df[['C'+str(i+1) for i in range(categorical_dim)]].values.reshape(1, batch_size*categorical_dim))\n",
    "    row_ptrs_df = pd.DataFrame(np.array(row_ptrs).reshape(1, batch_size*slot_num + 1))\n",
    "    with open(dst, 'w') as dst_txt:\n",
    "        dst_txt.write(\"{\\n\\\"data\\\":[\\n{\\n\")\n",
    "        dst_txt.write(\"\\\"DES\\\":\")\n",
    "        dst_txt.write(','.join('%s' %id for id in dense_df.values.tolist()))\n",
    "        dst_txt.write(\",\\n\\\"CATCOLUMN\\\":\")\n",
    "        dst_txt.write(','.join('%s' %id for id in (embedding_columns_df.values.reshape(-1,26)+offset).reshape(1,-1).tolist()))\n",
    "        dst_txt.write(\",\\n\\\"ROWINDEX\\\":\")\n",
    "        dst_txt.write(','.join('%s' %id for id in row_ptrs_df.values.tolist()))\n",
    "        dst_txt.write(\"\\n}\\n]\\n}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    arg_parser = argparse.ArgumentParser(description='Convert Preprocessed Criteo Data to Inference Format')\n",
    "    arg_parser.add_argument('--src_csv_path', type=str, required=True)\n",
    "    arg_parser.add_argument('--src_config_path', type=str, required=True)\n",
    "    arg_parser.add_argument('--dst_path', type=str, required=True)\n",
    "    arg_parser.add_argument('--batch_size', type=int, default=128)\n",
    "    arg_parser.add_argument('--segmentation', type=str, default=' ')\n",
    "    args = arg_parser.parse_args()\n",
    "    src_csv_path = args.src_csv_path\n",
    "    segmentation = args.segmentation\n",
    "    src_config_path = args.src_config_path\n",
    "    dst_path = args.dst_path\n",
    "    batch_size = args.batch_size\n",
    "    convert(src_csv_path, src_config_path, dst_path, batch_size, segmentation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-labor",
   "metadata": {},
   "source": [
    "### 4.3 Define Inference Input Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "mechanical-newsletter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./dlrm_input_format.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./dlrm_input_format.json\n",
    "{\n",
    "    \"dense\": 13,\n",
    "    \"categorical\": 26,\n",
    "    \"slot_size\": [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-electric",
   "metadata": {},
   "source": [
    "### 4.4 Generate the input json data with batch size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "electrical-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=1\n",
    "!python3 criteo2predict.py --src_csv_path=./infer_test.txt --src_config_path=dlrm_input_format.json --dst_path ./$batchsize\".json\" --batch_size=$batchsize --segmentation=','"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-lying",
   "metadata": {},
   "source": [
    "### 4.4 Get Triton server status if deploy DLRM successfully in Step3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dress-granny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Trying localhost...\r\n",
      "* TCP_NODELAY set \r\n",
      "* Connected to locahost (locahost) port 8000 (#0) \r\n",
      "> GET /v2/health/ready HTTP/1.1\r\n",
      "> Host: localhost:8000\r\n",
      "> User-Agent: curl/7.58.0\r\n",
      "> Accept: */*\r\n",
      "> \r\n",
      "< HTTP/1.1 200 OK\r\n",
      "< Content-Length: 0\r\n",
      "< Content-Type: text/plain\r\n",
      "< \r\n",
      "* Connection #0 to host 10.23.137.25 left intact\r\n"
     ]
    }
   ],
   "source": [
    "!curl -v localhost:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-multimedia",
   "metadata": {},
   "source": [
    "## 5. Get Inference benchmark by Triton Performance Tool "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-version",
   "metadata": {},
   "source": [
    "### 5.1 Get the inference performance for batchsize=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "under-fiber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Successfully read data for 1 stream/streams with 1 step/steps.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 6102\n",
      "    Throughput: 1220.4 infer/sec\n",
      "    Avg latency: 808 usec (standard deviation 324 usec)\n",
      "    p50 latency: 821 usec\n",
      "    p90 latency: 858 usec\n",
      "    p95 latency: 866 usec\n",
      "    p99 latency: 913 usec\n",
      "    Avg HTTP time: 807 usec (send/recv 69 usec + response wait 738 usec)\n",
      "  Server: \n",
      "    Inference count: 7275\n",
      "    Execution count: 7275\n",
      "    Successful request count: 7275\n",
      "    Avg request latency: 523 usec (overhead 1 usec + queue 161 usec + compute input 0 usec + compute infer 361 usec + compute output 0 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 1220.4 infer/sec, latency 808 usec\n"
     ]
    }
   ],
   "source": [
    "!perf_analyzer -m dlrm -u localhost:8000 --input-data 1.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-tsunami",
   "metadata": {},
   "source": [
    "### 5.2 Get the inference performance for batchsize=131072 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-crowd",
   "metadata": {},
   "source": [
    "#### 5.2.1. Modify the max_batch_size from 1 to 131072 in $dlrm_model_repo/config.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "broadband-lloyd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /dlrm_infer/model/dlrm/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile $dlrm_model_repo/config.pbtxt\n",
    "name: \"dlrm\"\n",
    "backend: \"hugectr\"\n",
    "max_batch_size:131072,\n",
    "input [\n",
    "   {\n",
    "    name: \"DES\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"CATCOLUMN\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"ROWINDEX\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind : KIND_GPU\n",
    "    gpus:[2]\n",
    "  }\n",
    "]\n",
    "\n",
    "parameters [\n",
    "  {\n",
    "  key: \"config\"\n",
    "  value: { string_value: \"/dlrm_infer/model/dlrm/1/dlrm.json\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"gpucache\"\n",
    "  value: { string_value: \"true\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"hit_rate_threshold\"\n",
    "  value: { string_value: \"0.8\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"gpucacheper\"\n",
    "  value: { string_value: \"0.5\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"label_dim\"\n",
    "  value: { string_value: \"1\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"slots\"\n",
    "  value: { string_value: \"26\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"cat_feature_num\"\n",
    "  value: { string_value: \"26\" }\n",
    "  },\n",
    " {\n",
    "  key: \"des_feature_num\"\n",
    "  value: { string_value: \"13\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"max_nnz\"\n",
    "  value: { string_value: \"2\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"embedding_vector_size\"\n",
    "  value: { string_value: \"128\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"embeddingkey_long_type\"\n",
    "  value: { string_value: \"true\" }\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-productivity",
   "metadata": {},
   "source": [
    "#### 5.2.2. Relaunch Triton server to reload DLRM according to Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-knitting",
   "metadata": {},
   "source": [
    "#### 5.2.3. Generate the input json file with batchsize=131072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "polyphonic-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=131072\n",
    "!python3 criteo2predict.py --src_csv_path=./infer_test.txt --src_config_path=dlrm_input_format.json --dst_path ./$batchsize\".json\" --batch_size=$batchsize --segmentation=','"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "broke-program",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Successfully read data for 1 stream/streams with 1 step/steps.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 27\n",
      "    Throughput: 5.4 infer/sec\n",
      "    Avg latency: 191104 usec (standard deviation 1974 usec)\n",
      "    p50 latency: 190973 usec\n",
      "    p90 latency: 192496 usec\n",
      "    p95 latency: 195875 usec\n",
      "    p99 latency: 197191 usec\n",
      "    Avg HTTP time: 191181 usec (send/recv 66241 usec + response wait 124940 usec)\n",
      "  Server: \n",
      "    Inference count: 32\n",
      "    Execution count: 32\n",
      "    Successful request count: 32\n",
      "    Avg request latency: 112264 usec (overhead 2 usec + queue 13793 usec + compute input 0 usec + compute infer 98469 usec + compute output 0 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 5.4 infer/sec, latency 191104 usec\n"
     ]
    }
   ],
   "source": [
    "!perf_analyzer -m dlrm -u localhost:8000 --input-data 131072.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-arrangement",
   "metadata": {},
   "source": [
    "## If you want to get more inference results with different batchsize, please repeat step 5.2 with new batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-beads",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python376jvsc74a57bd0fcb6e20f148befaa871fea7e513c7d46ef88bfa3833d49f947d9f904ecb48b74",
   "display_name": "Python 3.7.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}