{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 Overview\n",
    "In this notebook, we want to provide a tutorial on how to use standard DLRM model that trained on HugeCTR_DLRM_Training.\n",
    "notebook and deploy the saved model to Triton Inference Server. We could collect the inference benchmark by Triton performance analyzer  tool\n",
    "\n",
    "1. [Overview](#1)\n",
    "2. [Generate the DLRM Deployment Configuration](#2)\n",
    "3. [Load Models on Triton Server](#3)\n",
    "4. [Prepare Inference Input Data](#4) \n",
    "5. [Inference Benchmarm by Triton Performance Tool](#5) "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Generate the DLRM Deployment Configuration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Generate related model folders"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# define some data folder to store the model related files\n",
    "# Standard Libraries\n",
    "import os\n",
    "from time import time\n",
    "import re\n",
    "import shutil\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "BASE_DIR = \"/dlrm_infer\"\n",
    "model_folder  = os.path.join(BASE_DIR, \"model\")\n",
    "dlrm_model_repo= os.path.join(model_folder, \"dlrm\")\n",
    "dlrm_version =os.path.join(dlrm_model_repo, \"1\")\n",
    "\n",
    "if os.path.isdir(model_folder):\n",
    "    shutil.rmtree(model_folder)\n",
    "os.makedirs(model_folder)\n",
    "\n",
    "if os.path.isdir(dlrm_model_repo):\n",
    "    shutil.rmtree(dlrm_model_repo)\n",
    "os.makedirs(dlrm_model_repo)\n",
    "\n",
    "if os.path.isdir(dlrm_version):\n",
    "    shutil.rmtree(dlrm_version)\n",
    "os.makedirs(dlrm_version)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Copy DLRM model files to model repository"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "! cp -r /dlrm_train/dlrm0_sparse_20000.model $dlrm_version/\n",
    "! cp /dlrm_train/dlrm_dense_20000.model $dlrm_version/\n",
    "! cp /dlrm_train/dlrm.json $dlrm_version/\n",
    "!ls -l $dlrm_version"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total 10858568\r\n",
      "-rw-r--r-- 1 root root 11100190464 Mar 30 03:28 0_sparse_20000.model\r\n",
      "-rw-r--r-- 1 root root     9479684 Mar 30 03:28 _dense_20000.model\r\n",
      "-rw-r--r-- 1 root root        2887 Jul  6 11:03 dlrm.json\r\n",
      "drwxr-xr-x 2 root root        4096 Jul  6 11:02 dlrm0_sparse_20000.model\r\n",
      "-rw-r--r-- 1 root root     9479684 Jul  6 11:03 dlrm_dense_20000.model\r\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Generate the Triton configuration for deploying DLRM "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "%%writefile $dlrm_model_repo/config.pbtxt\n",
    "name: \"dlrm\"\n",
    "backend: \"hugectr\"\n",
    "max_batch_size:1,\n",
    "input [\n",
    "   {\n",
    "    name: \"DES\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"CATCOLUMN\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"ROWINDEX\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind : KIND_GPU\n",
    "    gpus:[2]\n",
    "  }\n",
    "]\n",
    "\n",
    "parameters [\n",
    "  {\n",
    "  key: \"config\"\n",
    "  value: { string_value: \"/dlrm_infer/model/dlrm/1/dlrm.json\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"gpucache\"\n",
    "  value: { string_value: \"true\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"hit_rate_threshold\"\n",
    "  value: { string_value: \"0.8\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"gpucacheper\"\n",
    "  value: { string_value: \"0.5\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"label_dim\"\n",
    "  value: { string_value: \"1\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"slots\"\n",
    "  value: { string_value: \"26\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"cat_feature_num\"\n",
    "  value: { string_value: \"26\" }\n",
    "  },\n",
    " {\n",
    "  key: \"des_feature_num\"\n",
    "  value: { string_value: \"13\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"max_nnz\"\n",
    "  value: { string_value: \"2\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"embedding_vector_size\"\n",
    "  value: { string_value: \"128\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"embeddingkey_long_type\"\n",
    "  value: { string_value: \"true\" }\n",
    "  }\n",
    "]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting /dlrm_infer/model/dlrm/config.pbtxt\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Generate the Hugectr Backend parameter server configuration for deploying dlrm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "%%writefile $model_folder/ps.json\n",
    "{\n",
    "    \"supportlonglong\":true,\n",
    "    \"models\":[\n",
    "        {\n",
    "            \"model\":\"dlrm\",\n",
    "            \"sparse_files\":[\"/dlrm_infer/model/dlrm/1/dlrm0_sparse_20000.model\"],\n",
    "            \"dense_file\":\"/dlrm_infer/model/dlrm/1/dlrm_dense_20000.model\",\n",
    "            \"network_file\":\"/dlrm_infer/model/dlrm/1/dlrm.json\"\n",
    "        }\n",
    "    ]  \n",
    "}"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Writing /dlrm_infer/model/ps.json\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "!ls -l $dlrm_version\n",
    "!ls -l $dlrm_model_repo"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total 10858568\n",
      "-rw-r--r-- 1 root root 11100190464 Mar 30 03:28 0_sparse_20000.model\n",
      "-rw-r--r-- 1 root root     9479684 Mar 30 03:28 _dense_20000.model\n",
      "-rw-r--r-- 1 root root        2887 Jul  6 11:03 dlrm.json\n",
      "drwxr-xr-x 2 root root        4096 Jul  6 11:02 dlrm0_sparse_20000.model\n",
      "-rw-r--r-- 1 root root     9479684 Jul  6 11:03 dlrm_dense_20000.model\n",
      "total 8\n",
      "drwxr-xr-x 3 root root 4096 Jul  6 11:02 1\n",
      "-rw-r--r-- 1 root root 1107 Apr  8 08:04 config.pbtxt\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Deploy DLRM on Triton Server\n",
    "At this stage, you should have already launched the Triton Inference Server with the following command:\n",
    "\n",
    "In this tutorial, we will deploy the DLRM to a single V100(32GB)\n",
    "\n",
    "docker run --gpus=all -it -v /dlrm_infer/:/dlrm_infer -v /dlrm_train/:/dlrm_train --net=host nvcr.io/nvidia/merlin/merlin-inference:0.7 /bin/bash\n",
    "\n",
    "After you enter into the container you can launch triton server with the command below:\n",
    "\n",
    "tritonserver --model-repository=/dlrm_infer/model/ --load-model=dlrm \n",
    "    --model-control-mode=explicit \n",
    "    --backend-directory=/usr/local/hugectr/backends \n",
    "    --backend-config=hugectr,ps=/dlrm_infer/model/ps.json \n",
    "    \n",
    "Note: The model-repository path is /dlrm_infer/model/. The path for the dlrm model network json file is /dlrm_infer/model/dlrm/1/dlrm.json. The path for the parameter server configuration file is /dlrm_infer/model/ps.json."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Prepare Inference Input Data "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1 Read validation data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "!ls -l /dlrm_train/dlrm/val"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total 2938676\r\n",
      "-rw-r--r-- 1 root root 704993648 Mar 25 09:13 0.2a6cbfbca91e420cb6f68536656260fb.parquet\r\n",
      "-rw-r--r-- 1 root root 704993648 Mar 25 13:31 0.83ab760d4f4b4505a397e9b90247eb4a.parquet\r\n",
      "-rw-r--r-- 1 root root 128130255 Apr 12 06:15 0.95a75de478af4decb84e22393d0a5205.parquet\r\n",
      "-rw-r--r-- 1 root root 128130255 Apr  7 08:12 0.d7c746376f3743608223bca2cfe4fb8a.parquet\r\n",
      "-rw-r--r-- 1 root root 128130255 Mar 31 13:05 0.e611389223d34a0c9f127d8e1ea4cb60.parquet\r\n",
      "-rw-r--r-- 1 root root 704993648 Mar 25 13:26 0.fafd8818ec0f4f41a3d3ffe689de91c2.parquet\r\n",
      "-rw-r--r-- 1 root root        54 Apr 12 06:15 _file_list.txt\r\n",
      "-rw-r--r-- 1 root root     26328 Apr 12 06:15 _metadata\r\n",
      "-rw-r--r-- 1 root root      1465 Apr 12 06:15 _metadata.json\r\n",
      "drwxr-xr-x 2 root root      4096 Apr 12 06:13 temp-parquet-after-conversion\r\n",
      "-rw-r--r-- 1 root root 509766965 Apr 12 06:09 test.txt\r\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import pandas as pd\n",
    "df=pd.read_parquet('/dlrm_train/dlrm/val/0.83ab760d4f4b4505a397e9b90247eb4a.parquet',engine='pyarrow')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "df.head(2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>...</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>I11</th>\n",
       "      <th>I12</th>\n",
       "      <th>I13</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>687</td>\n",
       "      <td>75</td>\n",
       "      <td>160</td>\n",
       "      <td>134261</td>\n",
       "      <td>62</td>\n",
       "      <td>10</td>\n",
       "      <td>7542</td>\n",
       "      <td>229</td>\n",
       "      <td>3</td>\n",
       "      <td>89976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117298</td>\n",
       "      <td>-0.264598</td>\n",
       "      <td>-0.241559</td>\n",
       "      <td>-0.569952</td>\n",
       "      <td>-0.441110</td>\n",
       "      <td>-0.570183</td>\n",
       "      <td>-0.5111</td>\n",
       "      <td>-0.0848</td>\n",
       "      <td>-0.436591</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>630</td>\n",
       "      <td>156</td>\n",
       "      <td>46</td>\n",
       "      <td>590512</td>\n",
       "      <td>44</td>\n",
       "      <td>23</td>\n",
       "      <td>10501</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>22899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239154</td>\n",
       "      <td>-0.264598</td>\n",
       "      <td>-0.241559</td>\n",
       "      <td>-0.749732</td>\n",
       "      <td>-0.468817</td>\n",
       "      <td>-0.570183</td>\n",
       "      <td>-0.5111</td>\n",
       "      <td>-0.0848</td>\n",
       "      <td>-0.436591</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    C1   C2   C3      C4  C5  C6     C7   C8  C9    C10  ...        I5  \\\n",
       "0  687   75  160  134261  62  10   7542  229   3  89976  ... -0.117298   \n",
       "1  630  156   46  590512  44  23  10501    2   3  22899  ...  0.239154   \n",
       "\n",
       "         I6        I7        I8        I9       I10     I11     I12       I13  \\\n",
       "0 -0.264598 -0.241559 -0.569952 -0.441110 -0.570183 -0.5111 -0.0848 -0.436591   \n",
       "1 -0.264598 -0.241559 -0.749732 -0.468817 -0.570183 -0.5111 -0.0848 -0.436591   \n",
       "\n",
       "   label  \n",
       "0    1.0  \n",
       "1    0.0  \n",
       "\n",
       "[2 rows x 40 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "df.head(200000).to_csv('infer_test.txt', sep='\\t', index=False,header=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 Follow the Triton requirements to generate input data with json format "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "%%writefile ./criteo2predict.py\n",
    "import argparse\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "def parse_config(src_config):\n",
    "    try:\n",
    "        with open(src_config, 'r') as data_json:\n",
    "            j_data = json.load(data_json)\n",
    "            dense_dim = j_data[\"dense\"]\n",
    "            categorical_dim = j_data[\"categorical\"]\n",
    "            slot_size = j_data[\"slot_size\"]\n",
    "        assert(categorical_dim == np.sum(slot_size))\n",
    "        return dense_dim, categorical_dim, slot_size\n",
    "    except:\n",
    "        print(\"Invalid data configuration file!\")\n",
    "\n",
    "def convert(src_csv, src_config, dst, batch_size,segmentation):\n",
    "    dense_dim, categorical_dim, slot_size = parse_config(src_config)\n",
    "    slot_size_array=[4976199, 3289052, 282487, 138210, 11, 2203, 8901, 67, 4, 948, 15, 25419, 5577159, 1385790, 4348882, 178673, 10023, 88, 34, 14705, 7112, 19283, 4, 6391, 1282, 60]\n",
    "    offset = np.insert(np.cumsum(slot_size_array), 0, 0)[:-1]\n",
    "    total_columns = 1 + dense_dim + categorical_dim\n",
    "    df = pd.read_csv(src_csv,  sep='\\t', nrows=batch_size)\n",
    "    cols = df.columns\n",
    "    slot_num = len(slot_size)\n",
    "    row_ptrs = [0 for _ in range(batch_size*slot_num + 1)]\n",
    "    for i in range(1, len(row_ptrs)):\n",
    "        row_ptrs[i] = row_ptrs[i-1] + slot_size[(i-1)%slot_num]\n",
    "    label_df =  pd.DataFrame(df['label'].values.reshape(1,batch_size))\n",
    "    dense_df = pd.DataFrame(df[['I'+str(i+1) for i in range(dense_dim)]].values.reshape(1, batch_size*dense_dim))\n",
    "    embedding_columns_df = pd.DataFrame(df[['C'+str(i+1) for i in range(categorical_dim)]].values.reshape(1, batch_size*categorical_dim))\n",
    "    row_ptrs_df = pd.DataFrame(np.array(row_ptrs).reshape(1, batch_size*slot_num + 1))\n",
    "    with open(dst, 'w') as dst_txt:\n",
    "        dst_txt.write(\"{\\n\\\"data\\\":[\\n{\\n\")\n",
    "        dst_txt.write(\"\\\"DES\\\":\")\n",
    "        dst_txt.write(','.join('%s' %id for id in dense_df.values.tolist()))\n",
    "        dst_txt.write(\",\\n\\\"CATCOLUMN\\\":\")\n",
    "        dst_txt.write(','.join('%s' %id for id in (embedding_columns_df.values.reshape(-1,26)+offset).reshape(1,-1).tolist()))\n",
    "        dst_txt.write(\",\\n\\\"ROWINDEX\\\":\")\n",
    "        dst_txt.write(','.join('%s' %id for id in row_ptrs_df.values.tolist()))\n",
    "        dst_txt.write(\"\\n}\\n]\\n}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    arg_parser = argparse.ArgumentParser(description='Convert Preprocessed Criteo Data to Inference Format')\n",
    "    arg_parser.add_argument('--src_csv_path', type=str, required=True)\n",
    "    arg_parser.add_argument('--src_config_path', type=str, required=True)\n",
    "    arg_parser.add_argument('--dst_path', type=str, required=True)\n",
    "    arg_parser.add_argument('--batch_size', type=int, default=128)\n",
    "    arg_parser.add_argument('--segmentation', type=str, default=' ')\n",
    "    args = arg_parser.parse_args()\n",
    "    src_csv_path = args.src_csv_path\n",
    "    segmentation = args.segmentation\n",
    "    src_config_path = args.src_config_path\n",
    "    dst_path = args.dst_path\n",
    "    batch_size = args.batch_size\n",
    "    convert(src_csv_path, src_config_path, dst_path, batch_size, segmentation)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting ./criteo2predict.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3 Define Inference Input Data Format"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "%%writefile ./dlrm_input_format.json\n",
    "{\n",
    "    \"dense\": 13,\n",
    "    \"categorical\": 26,\n",
    "    \"slot_size\": [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "}"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting ./dlrm_input_format.json\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.4 Generate the input json data with batch size=1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "batchsize=1\n",
    "!python3 criteo2predict.py --src_csv_path=./infer_test.txt --src_config_path=dlrm_input_format.json --dst_path ./$batchsize\".json\" --batch_size=$batchsize --segmentation=','"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.4 Get Triton server status if deploy DLRM successfully in Step3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "!curl -v localhost:8000/v2/health/ready"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "*   Trying 127.0.0.1:8000...\r\n",
      "* TCP_NODELAY set\r\n",
      "* Connected to localhost (127.0.0.1) port 8000 (#0)\r\n",
      "> GET /v2/health/ready HTTP/1.1\r\n",
      "\r\n",
      "> Host: localhost:8000\r\n",
      "\r\n",
      "> User-Agent: curl/7.68.0\r\n",
      "\r\n",
      "> Accept: */*\r\n",
      "\r\n",
      "> \r\n",
      "\r\n",
      "* Mark bundle as not supporting multiuse\r\n",
      "< HTTP/1.1 200 OK\r\n",
      "\r\n",
      "< Content-Length: 0\r\n",
      "\r\n",
      "< Content-Type: text/plain\r\n",
      "\r\n",
      "< \r\n",
      "\r\n",
      "* Connection #0 to host localhost left intact\r\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Get Inference benchmark by Triton Performance Tool "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1 Get the inference performance for batchsize=1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "!perf_analyzer -m dlrm -u localhost:8000 --input-data 1.json --shape CATCOLUMN:26 --shape DES:13 --shape ROWINDEX:27"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Successfully read data for 1 stream/streams with 1 step/steps.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 5552\n",
      "    Throughput: 1110.4 infer/sec\n",
      "    Avg latency: 887 usec (standard deviation 263 usec)\n",
      "    p50 latency: 876 usec\n",
      "    p90 latency: 918 usec\n",
      "    p95 latency: 941 usec\n",
      "    p99 latency: 1043 usec\n",
      "    Avg HTTP time: 877 usec (send/recv 74 usec + response wait 803 usec)\n",
      "  Server: \n",
      "    Inference count: 6691\n",
      "    Execution count: 6691\n",
      "    Successful request count: 6691\n",
      "    Avg request latency: 573 usec (overhead 1 usec + queue 146 usec + compute input 0 usec + compute infer 426 usec + compute output 0 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 1110.4 infer/sec, latency 887 usec\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2 Get the inference performance for batchsize=131072 \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.2.1. Modify the max_batch_size from 1 to 131072 in $dlrm_model_repo/config.pbtxt"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "%%writefile $dlrm_model_repo/config.pbtxt\n",
    "name: \"dlrm\"\n",
    "backend: \"hugectr\"\n",
    "max_batch_size:131072,\n",
    "input [\n",
    "   {\n",
    "    name: \"DES\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"CATCOLUMN\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"ROWINDEX\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind : KIND_GPU\n",
    "    gpus:[2]\n",
    "  }\n",
    "]\n",
    "\n",
    "parameters [\n",
    "  {\n",
    "  key: \"config\"\n",
    "  value: { string_value: \"/dlrm_infer/model/dlrm/1/dlrm.json\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"gpucache\"\n",
    "  value: { string_value: \"true\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"hit_rate_threshold\"\n",
    "  value: { string_value: \"0.8\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"gpucacheper\"\n",
    "  value: { string_value: \"0.5\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"label_dim\"\n",
    "  value: { string_value: \"1\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"slots\"\n",
    "  value: { string_value: \"26\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"cat_feature_num\"\n",
    "  value: { string_value: \"26\" }\n",
    "  },\n",
    " {\n",
    "  key: \"des_feature_num\"\n",
    "  value: { string_value: \"13\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"max_nnz\"\n",
    "  value: { string_value: \"2\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"embedding_vector_size\"\n",
    "  value: { string_value: \"128\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"embeddingkey_long_type\"\n",
    "  value: { string_value: \"true\" }\n",
    "  }\n",
    "]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting /dlrm_infer/model/dlrm/config.pbtxt\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.2.2. Relaunch Triton server to reload DLRM according to Step 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.2.3. Generate the input json file with batchsize=131072"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "batchsize=131072\n",
    "!python3 criteo2predict.py --src_csv_path=./infer_test.txt --src_config_path=dlrm_input_format.json --dst_path ./$batchsize\".json\" --batch_size=$batchsize --segmentation=','"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "!perf_analyzer -m dlrm -u localhost:8000 --input-data 131072.json --shape CATCOLUMN:3407872 --shape DES:1703936 --shape ROWINDEX:3407873"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Successfully read data for 1 stream/streams with 1 step/steps.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 27\n",
      "    Throughput: 5.4 infer/sec\n",
      "    Avg latency: 191104 usec (standard deviation 1974 usec)\n",
      "    p50 latency: 190973 usec\n",
      "    p90 latency: 192496 usec\n",
      "    p95 latency: 195875 usec\n",
      "    p99 latency: 197191 usec\n",
      "    Avg HTTP time: 191181 usec (send/recv 66241 usec + response wait 124940 usec)\n",
      "  Server: \n",
      "    Inference count: 32\n",
      "    Execution count: 32\n",
      "    Successful request count: 32\n",
      "    Avg request latency: 112264 usec (overhead 2 usec + queue 13793 usec + compute input 0 usec + compute infer 98469 usec + compute output 0 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 5.4 infer/sec, latency 191104 usec\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## If you want to get more inference results with different batchsize, please repeat step 5.2 with new batchsize"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}