{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b57e3b7",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# HugeCTR Continuous Training and Inference Demo (Part I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a52be0",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In HugeCTR version 3.3, we finished the whole pipeline of parameter server, including \n",
    "1. The parameter dumping interface from training to kafka.\n",
    "2. CPU cache(Redis Cluster / Hash Map / Parallel Hash Map).\n",
    "3. RocksDB as a persistence storage.\n",
    "4. Embedding cache update mechanism.\n",
    "\n",
    "\n",
    "The purpose of this notebook is to show how to do continuous traning and inference using HugeCTR Hierarchical Parameter Server. \n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "-  [Data Preparation](#1)\n",
    "-  [Data Preprocessing using Pandas](#2)\n",
    "-  [Wide&Deep Training Demo](#3)\n",
    "-  [Wide&Deep Model Inference using Python API](#4)\n",
    "-  [Wide&Deep Model continuous training](#5)\n",
    "-  [Wide&Deep Model continuous inference](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a25de5",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3e9c57",
   "metadata": {},
   "source": [
    "### 1.1 Make a folder to store our data and data processing scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b475d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir criteo_data\n",
    "!mkdir criteo_script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144ca56d",
   "metadata": {},
   "source": [
    "### 1.2 Download Criteo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c4b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://azuremlsampleexperiments.blob.core.windows.net/criteo/day_1.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fedaee",
   "metadata": {},
   "source": [
    "**NOTE**: Replace `1` with a value from [0, 23] to use a different day.\n",
    "\n",
    "During preprocessing, the amount of data, which is used to speed up the preprocessing, fill missing values, and remove the feature values that are considered rare, is further reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2495ca",
   "metadata": {},
   "source": [
    "### 1.3 Write the preprocessing the script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "193c9c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "if [[ $# -lt 3 ]]; then\n",
    "  echo \"Usage: preprocess.sh [DATASET_NO.] [DST_DATA_DIR] [SCRIPT_TYPE] [SCRIPT_TYPE_SPECIFIC_ARGS...]\"\n",
    "  exit 2\n",
    "fi\n",
    "\n",
    "DST_DATA_DIR=$2\n",
    "\n",
    "echo \"Warning: existing $DST_DATA_DIR is erased\"\n",
    "rm -rf $DST_DATA_DIR\n",
    "\n",
    "if [[ $3 == \"nvt\" ]]; then\n",
    "  if [[ $# -ne 6 ]]; then\n",
    "\t\techo \"Usage: preprocess.sh [DATASET_NO.] [DST_DATA_DIR] nvt [IS_PARQUET_FORMAT] [IS_CRITEO_MODE] [IS_FEATURE_CROSSED]\"\n",
    "    exit 2\n",
    "\tfi\n",
    "\techo \"Preprocessing script: NVTabular\"\n",
    "elif [[ $3 == \"perl\" ]]; then\n",
    "  if [[ $# -ne 4 ]]; then\n",
    "\t\techo \"Usage: preprocess.sh [DATASET_NO.] [DST_DATA_DIR] perl [NUM_SLOTS]\"\n",
    "    exit 2\n",
    "\tfi\n",
    "\techo \"Preprocessing script: Perl\"\n",
    "elif [[ $3 == \"pandas\" ]]; then\n",
    "  if [[ $# -lt 5 ]]; then\n",
    "\t\techo \"Usage: preprocess.sh [DATASET_NO.] [DST_DATA_DIR] pandas [IS_DENSE_NORMALIZED] [IS_FEATURE_CROSSED] (FILE_LIST_LENGTH)\"\n",
    "    exit 2\n",
    "\tfi\n",
    "\techo \"Preprocessing script: Pandas\"\n",
    "else\n",
    "\techo \"Error: $3 is an invalid script type. Pick one from {nvt, perl, pandas}.\"\n",
    "\texit 2\n",
    "fi\n",
    "\n",
    "SCRIPT_TYPE=$3\n",
    "\n",
    "echo \"Getting the first few examples from the uncompressed dataset...\"\n",
    "mkdir -p $DST_DATA_DIR/train                         && \\\n",
    "mkdir -p $DST_DATA_DIR/val                           && \\\n",
    "head -n 500000 day_$1 > $DST_DATA_DIR/day_$1_small\n",
    "if [ $? -ne 0 ]; then\n",
    "\techo \"Warning: fallback to find original compressed data day_$1.gz...\"\n",
    "\techo \"Decompressing day_$1.gz...\"\n",
    "\tgzip -d -c day_$1.gz > day_$1\n",
    "\tif [ $? -ne 0 ]; then\n",
    "\t\techo \"Error: failed to decompress the file.\"\n",
    "\t\texit 2\n",
    "\tfi\n",
    "\thead -n 500000 day_$1 > $DST_DATA_DIR/day_$1_small\n",
    "\tif [ $? -ne 0 ]; then\n",
    "\t\techo \"Error: day_$1 file\"\n",
    "\t\texit 2\n",
    "\tfi\n",
    "fi\n",
    "\n",
    "echo \"Counting the number of samples in day_$1 dataset...\"\n",
    "total_count=$(wc -l $DST_DATA_DIR/day_$1_small)\n",
    "total_count=(${total_count})\n",
    "echo \"The first $total_count examples will be used in day_$1 dataset.\"\n",
    "\n",
    "echo \"Shuffling dataset...\"\n",
    "shuf $DST_DATA_DIR/day_$1_small > $DST_DATA_DIR/day_$1_shuf\n",
    "\n",
    "train_count=$(( total_count * 8 / 10))\n",
    "valtest_count=$(( total_count - train_count ))\n",
    "val_count=$(( valtest_count * 5 / 10 ))\n",
    "test_count=$(( valtest_count - val_count  ))\n",
    "\n",
    "split_dataset()\n",
    "{\n",
    "\techo \"Splitting into $train_count-sample training, $val_count-sample val, and $test_count-sample test datasets...\"\n",
    "\thead -n $train_count $DST_DATA_DIR/$1 > $DST_DATA_DIR/train/train.txt          && \\\n",
    "\ttail -n $valtest_count $DST_DATA_DIR/$1 > $DST_DATA_DIR/val/valtest.txt        && \\\n",
    "\thead -n $val_count $DST_DATA_DIR/val/valtest.txt > $DST_DATA_DIR/val/val.txt   && \\\n",
    "\ttail -n $test_count $DST_DATA_DIR/val/valtest.txt > $DST_DATA_DIR/val/test.txt\n",
    "\n",
    "\tif [ $? -ne 0 ]; then\n",
    "\t\texit 2\n",
    "\tfi\n",
    "}\n",
    "\n",
    "echo \"Preprocessing...\"\n",
    "if [[ $SCRIPT_TYPE == \"nvt\" ]]; then\n",
    "\tIS_PARQUET_FORMAT=$4\n",
    "\tIS_CRITEO_MODE=$5\n",
    "\tFEATURE_CROSS_LIST_OPTION=\"\"\n",
    "\tif [[ ( $IS_CRITEO_MODE -eq 0 ) && ( $6 -eq 1 ) ]]; then\n",
    "\t\tFEATURE_CROSS_LIST_OPTION=\"--feature_cross_list C1_C2,C3_C4\"\n",
    "\t\techo $FEATURE_CROSS_LIST_OPTION\n",
    "\tfi\n",
    "  split_dataset day_$1_shuf\n",
    "  python3 criteo_script/preprocess_nvt.py \\\n",
    "\t\t--data_path $DST_DATA_DIR             \\\n",
    "\t\t--out_path $DST_DATA_DIR              \\\n",
    "\t\t--freq_limit 6                        \\\n",
    "\t\t--device_limit_frac 0.5               \\\n",
    "\t\t--device_pool_frac 0.5                \\\n",
    "\t\t--out_files_per_proc 8                \\\n",
    "\t\t--devices \"0\"                         \\\n",
    "\t\t--num_io_threads 2                    \\\n",
    "        --parquet_format=$IS_PARQUET_FORMAT   \\\n",
    "\t\t--criteo_mode=$IS_CRITEO_MODE         \\\n",
    "\t\t$FEATURE_CROSS_LIST_OPTION\n",
    "\n",
    "elif [[ $SCRIPT_TYPE == \"perl\" ]]; then\n",
    "\tNUM_SLOT=$4\n",
    "  split_dataset day_$1_shuf\n",
    "\tperl criteo_script_legacy/preprocess.pl $DST_DATA_DIR/train/train.txt $DST_DATA_DIR/val/val.txt $DST_DATA_DIR/val/test.txt                      && \\\n",
    "\tcriteo2hugectr_legacy $NUM_SLOT $DST_DATA_DIR/train/train.txt.out $DST_DATA_DIR/train/sparse_embedding $DST_DATA_DIR/file_list.txt && \\\n",
    "\tcriteo2hugectr_legacy $NUM_SLOT $DST_DATA_DIR/val/test.txt.out $DST_DATA_DIR/val/sparse_embedding $DST_DATA_DIR/file_list_test.txt\n",
    "\n",
    "elif [[ $SCRIPT_TYPE == \"pandas\" ]]; then\n",
    "\tpython3 criteo_script/preprocess.py                 \\\n",
    "\t\t--src_csv_path=$DST_DATA_DIR/day_$1_shuf          \\\n",
    "\t\t--dst_csv_path=$DST_DATA_DIR/day_$1_shuf.out      \\\n",
    "\t\t--normalize_dense=$4 --feature_cross=$5      &&   \\\n",
    "  split_dataset day_$1_shuf.out\n",
    "\tNUM_WIDE_KEYS=\"\"\n",
    "\tif [[ $5 -ne 0 ]]; then\n",
    "\t\tNUM_WIDE_KEYS=2\n",
    "\tfi\n",
    "\n",
    "  FILE_LIST_LENGTH=\"\"\n",
    "  if [[ $# -gt 5 ]]; then\n",
    "    FILE_LIST_LENGTH=$6\n",
    "\tfi\n",
    "\n",
    "\tcriteo2hugectr $DST_DATA_DIR/train/train.txt $DST_DATA_DIR/train/sparse_embedding $DST_DATA_DIR/file_list.txt $NUM_WIDE_KEYS $FILE_LIST_LENGTH && \\\n",
    "\tcriteo2hugectr $DST_DATA_DIR/val/test.txt $DST_DATA_DIR/val/sparse_embedding $DST_DATA_DIR/file_list_test.txt $NUM_WIDE_KEYS $FILE_LIST_LENGTH\n",
    "fi\n",
    "\n",
    "if [ $? -ne 0 ]; then\n",
    "\texit 2\n",
    "fi\n",
    "\n",
    "echo \"All done!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd49a678",
   "metadata": {},
   "source": [
    "**NOTE**: Here we only read the first 500000 lines of the data to do the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e8df09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting criteo_script/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile criteo_script/preprocess.py\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "from six.moves import urllib\n",
    "import urllib.request \n",
    "\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import logging\n",
    "import concurrent.futures as cf\n",
    "from traceback import print_exc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing as skp\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s')\n",
    "logging.root.setLevel(logging.NOTSET)\n",
    "\n",
    "NUM_INTEGER_COLUMNS = 13\n",
    "NUM_CATEGORICAL_COLUMNS = 26\n",
    "NUM_TOTAL_COLUMNS = 1 + NUM_INTEGER_COLUMNS + NUM_CATEGORICAL_COLUMNS\n",
    "\n",
    "MAX_NUM_WORKERS = NUM_TOTAL_COLUMNS\n",
    "\n",
    "INT_NAN_VALUE = np.iinfo(np.int32).min\n",
    "CAT_NAN_VALUE = '80000000'\n",
    "\n",
    "def idx2key(idx):\n",
    "    if idx == 0:\n",
    "        return 'label'\n",
    "    return 'I' + str(idx) if idx <= NUM_INTEGER_COLUMNS else 'C' + str(idx - NUM_INTEGER_COLUMNS)\n",
    "\n",
    "def _fill_missing_features_and_split(chunk, series_list_dict):\n",
    "    for cid, col in enumerate(chunk.columns):\n",
    "        NAN_VALUE = INT_NAN_VALUE if cid <= NUM_INTEGER_COLUMNS else CAT_NAN_VALUE\n",
    "        result_series = chunk[col].fillna(NAN_VALUE)\n",
    "        series_list_dict[col].append(result_series)\n",
    "\n",
    "def _merge_and_transform_series(src_series_list, col, dense_cols,\n",
    "                                normalize_dense):\n",
    "    result_series = pd.concat(src_series_list)\n",
    "\n",
    "    if col != 'label':\n",
    "        unique_value_counts = result_series.value_counts()\n",
    "        unique_value_counts = unique_value_counts.loc[unique_value_counts >= 6]\n",
    "        unique_value_counts = set(unique_value_counts.index.values)\n",
    "        NAN_VALUE = INT_NAN_VALUE if col.startswith('I') else CAT_NAN_VALUE\n",
    "        result_series = result_series.apply(\n",
    "                lambda x: x if x in unique_value_counts else NAN_VALUE)\n",
    "\n",
    "    if col == 'label' or col in dense_cols:\n",
    "        result_series = result_series.astype(np.int64)\n",
    "        le = skp.LabelEncoder()\n",
    "        result_series = pd.DataFrame(le.fit_transform(result_series))\n",
    "        if col != 'label':\n",
    "            result_series = result_series + 1\n",
    "    else:\n",
    "        oe = skp.OrdinalEncoder(dtype=np.int64)\n",
    "        result_series = pd.DataFrame(oe.fit_transform(pd.DataFrame(result_series)))\n",
    "        result_series = result_series + 1\n",
    "\n",
    "\n",
    "    if normalize_dense != 0:\n",
    "        if col in dense_cols:\n",
    "            mms = skp.MinMaxScaler(feature_range=(0,1))\n",
    "            result_series = pd.DataFrame(mms.fit_transform(result_series))\n",
    "\n",
    "    result_series.columns = [col]\n",
    "\n",
    "    min_max = (np.int64(result_series[col].min()), np.int64(result_series[col].max()))\n",
    "    if col != 'label':\n",
    "        logging.info('column {} [{}, {}]'.format(col, str(min_max[0]),str(min_max[1])))\n",
    "\n",
    "    return [result_series, min_max]\n",
    "\n",
    "def _convert_to_string(series):\n",
    "    return series.astype(str)\n",
    "\n",
    "def _merge_columns_and_feature_cross(series_list, min_max, feature_pairs,\n",
    "                                     feature_cross):\n",
    "    name_to_series = dict()\n",
    "    for series in series_list:\n",
    "        name_to_series[series.columns[0]] = series.iloc[:,0]\n",
    "    df = pd.DataFrame(name_to_series)\n",
    "    cols = [idx2key(idx) for idx in range(0, NUM_TOTAL_COLUMNS)]\n",
    "    df = df.reindex(columns=cols)\n",
    "\n",
    "    offset = np.int64(0)\n",
    "    for col in cols:\n",
    "        if col != 'label' and col.startswith('I') == False:\n",
    "            df[col] += offset\n",
    "            logging.info('column {} offset {}'.format(col, str(offset)))\n",
    "            offset += min_max[col][1]\n",
    "\n",
    "    if feature_cross != 0:\n",
    "        for idx, pair in enumerate(feature_pairs):\n",
    "            col0 = pair[0]\n",
    "            col1 = pair[1]\n",
    "\n",
    "            col1_width = int(min_max[col1][1] - min_max[col1][0] + 1)\n",
    "\n",
    "            crossed_column_series = df[col0] * col1_width + df[col1]\n",
    "            oe = skp.OrdinalEncoder(dtype=np.int64)\n",
    "            crossed_column_series = pd.DataFrame(oe.fit_transform(pd.DataFrame(crossed_column_series)))\n",
    "            crossed_column_series = crossed_column_series + 1\n",
    "\n",
    "            crossed_column = col0 + '_' + col1\n",
    "            df.insert(NUM_INTEGER_COLUMNS + 1 + idx, crossed_column, crossed_column_series)\n",
    "            crossed_column_max_val = np.int64(df[crossed_column].max())\n",
    "            logging.info('column {} [{}, {}]'.format(\n",
    "                crossed_column,\n",
    "                str(df[crossed_column].min()),\n",
    "                str(crossed_column_max_val)))\n",
    "            df[crossed_column] += offset\n",
    "            logging.info('column {} offset {}'.format(crossed_column, str(offset)))\n",
    "            offset += crossed_column_max_val\n",
    "\n",
    "    return df\n",
    "\n",
    "def _wait_futures_and_reset(futures):\n",
    "    for future in futures:\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            print(result)\n",
    "    futures = list()\n",
    "\n",
    "def _process_chunks(executor, chunks_to_process, op, *argv):\n",
    "    futures = list()\n",
    "    for chunk in chunks_to_process:\n",
    "        argv_list = list(argv)\n",
    "        argv_list.insert(0, chunk)\n",
    "        new_argv = tuple(argv_list)\n",
    "        future = executor.submit(op, *new_argv)\n",
    "        futures.append(future)\n",
    "    _wait_futures_and_reset(futures)\n",
    "\n",
    "def preprocess(src_txt_name, dst_txt_name, normalize_dense, feature_cross):\n",
    "    cols = [idx2key(idx) for idx in range(0, NUM_TOTAL_COLUMNS)]\n",
    "    series_list_dict = dict()\n",
    "\n",
    "    with cf.ThreadPoolExecutor(max_workers=MAX_NUM_WORKERS) as executor:\n",
    "        logging.info('read a CSV file')\n",
    "        reader = pd.read_csv(src_txt_name, sep='\\t',\n",
    "                             names=cols,\n",
    "                             chunksize=131072)\n",
    "\n",
    "        logging.info('_fill_missing_features_and_split')\n",
    "        for col in cols:\n",
    "            series_list_dict[col] = list()\n",
    "        _process_chunks(executor, reader, _fill_missing_features_and_split,\n",
    "                        series_list_dict)\n",
    "\n",
    "    with cf.ProcessPoolExecutor(max_workers=MAX_NUM_WORKERS) as executor:\n",
    "        logging.info('_merge_and_transform_series')\n",
    "        futures = list()\n",
    "        dense_cols = [idx2key(idx+1) for idx in range(NUM_INTEGER_COLUMNS)]\n",
    "        dst_series_list = list()\n",
    "        min_max = dict()\n",
    "        for col, src_series_list in series_list_dict.items():\n",
    "            future = executor.submit(_merge_and_transform_series,\n",
    "                                     src_series_list, col, dense_cols,\n",
    "                                     normalize_dense)\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in futures:\n",
    "            col = None\n",
    "            for idx, ret in enumerate(future.result()):\n",
    "                try:\n",
    "                    if idx == 0:\n",
    "                        col = ret.columns[0]\n",
    "                        dst_series_list.append(ret)\n",
    "                    else:\n",
    "                        min_max[col] = ret\n",
    "                except:\n",
    "                    print_exc()\n",
    "        futures = list()\n",
    "\n",
    "        logging.info('_merge_columns_and_feature_cross')\n",
    "        feature_pairs = [('C1', 'C2'), ('C3', 'C4')]\n",
    "        df = _merge_columns_and_feature_cross(dst_series_list, min_max, feature_pairs,\n",
    "                                              feature_cross)\n",
    "\n",
    "        \n",
    "        logging.info('_convert_to_string')\n",
    "        futures = dict()\n",
    "        for col in cols:\n",
    "            future = executor.submit(_convert_to_string, df[col])\n",
    "            futures[col] = future\n",
    "        if feature_cross != 0:\n",
    "            for pair in feature_pairs:\n",
    "                col = pair[0] + '_' + pair[1]\n",
    "                future = executor.submit(_convert_to_string, df[col])\n",
    "                futures[col] = future\n",
    "\n",
    "        logging.info('_store_to_df')\n",
    "        for col, future in futures.items():\n",
    "            ret = future.result()\n",
    "            try:\n",
    "                df[col] = ret\n",
    "            except:\n",
    "                print_exc()\n",
    "        futures = dict()\n",
    "\n",
    "        logging.info('write to a CSV file')\n",
    "        df.to_csv(dst_txt_name, sep=' ', header=False, index=False)\n",
    "\n",
    "        logging.info('done!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    arg_parser = argparse.ArgumentParser(description='Preprocssing Criteo Dataset')\n",
    "\n",
    "    arg_parser.add_argument('--src_csv_path', type=str, required=True)\n",
    "    arg_parser.add_argument('--dst_csv_path', type=str, required=True)\n",
    "    arg_parser.add_argument('--normalize_dense', type=int, default=1)\n",
    "    arg_parser.add_argument('--feature_cross', type=int, default=1)\n",
    "\n",
    "    args = arg_parser.parse_args()\n",
    "\n",
    "    src_csv_path = args.src_csv_path\n",
    "    dst_csv_path = args.dst_csv_path\n",
    "\n",
    "    normalize_dense = args.normalize_dense\n",
    "    feature_cross = args.feature_cross\n",
    "\n",
    "    if os.path.exists(src_csv_path) == False:\n",
    "        sys.exit('ERROR: the file \\'{}\\' doesn\\'t exist'.format(src_csv_path))\n",
    "\n",
    "    if os.path.exists(dst_csv_path) == True:\n",
    "        sys.exit('ERROR: the file \\'{}\\' exists'.format(dst_csv_path))\n",
    "\n",
    "    preprocess(src_csv_path, dst_csv_path, normalize_dense, feature_cross)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee52b921",
   "metadata": {},
   "source": [
    "### 1.4 Run the preprocess script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc9080aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash preprocess.sh 0 criteo_data pandas 1 1 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f5dcee",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTES**: \n",
    "\n",
    "Arguments may vary depend on your setting:\n",
    "- The first argument represents the dataset postfix.  For instance, if `day_1` is used, the postfix is `1`.\n",
    "- The second argument, `criteo_data`, is where the preprocessed data is stored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e512e",
   "metadata": {},
   "source": [
    "### 1.5 Generate data sample for inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0f66b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_table(\"criteo_data/train/train.txt\", header = None, sep= ' ', \\\n",
    "                   names = ['label'] + ['I'+str(i) for i in range(1, 14)] + \\\n",
    "                   ['C1_C2', 'C3_C4'] + ['C'+str(i) for i in range(1, 27)])[:5]\n",
    "left = df.iloc[:,:14].astype(np.float32)\n",
    "right = df.iloc[:, 14:].astype(np.int64)\n",
    "merged = pd.concat([left, right], axis = 1)\n",
    "merged.to_csv(\"infer_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bb491a",
   "metadata": {},
   "source": [
    "## 2. Start the Kafka Broker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2d8962",
   "metadata": {},
   "source": [
    "**Please refer to the README to start the Kafka Broker properly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aa5136",
   "metadata": {},
   "source": [
    "## 3. Wide&Deep Model Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fce5517",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r *model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d50a27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wdl_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wdl_demo.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(model_name = \"wdl\",\n",
    "                              max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False,\n",
    "                              use_cuda_graph = True,\n",
    "                              kafka_brockers = \"10.23.137.25:9093\") #Make sure this is consistent with your Kafka broker.)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"criteo_data/file_list.\"+str(i)+\".txt\" for i in range(2)],\n",
    "                          keyset = [\"criteo_data/file_list.\"+str(i)+\".keyset\" for i in range(2)],\n",
    "                          eval_source = \"criteo_data/file_list.2.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "hc_config = hugectr.CreateHMemCache(2, 0.5, 0)\n",
    "etc = hugectr.CreateETC(ps_types = [hugectr.TrainPSType_t.Staged, hugectr.TrainPSType_t.Cached],\\\n",
    "                        sparse_models = [\"./wdl_0_sparse_model\", \"./wdl_1_sparse_model\"],\\\n",
    "                        local_paths = [\"./\"], hmem_cache_configs = [hc_config])\n",
    "model = hugectr.Model(solver, reader, optimizer, etc)\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"wide_data\", 2, True, 1),\n",
    "                        hugectr.DataReaderSparseParam(\"deep_data\", 1, True, 26)]))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 23,\n",
    "                            embedding_vec_size = 1,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding0\",\n",
    "                            bottom_name = \"wide_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 358,\n",
    "                            embedding_vec_size = 16,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"deep_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=416))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding0\"],\n",
    "                            top_names = [\"reshape2\"],\n",
    "                            leading_dim=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"dense\"], top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"dropout2\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Add,\n",
    "                            bottom_names = [\"fc3\", \"reshape2\"],\n",
    "                            top_names = [\"add1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"add1\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.graph_to_json(graph_config_file = \"wdl.json\")\n",
    "#model.save_params_to_files(\"wdl\")\n",
    "model.fit(num_epochs = 1, display = 500, eval_interval = 1000)\n",
    "\n",
    "model.set_source(source = [\"criteo_data/file_list.\"+str(i)+\".txt\" for i in range(3, 5)], \\\n",
    "                 keyset = [\"criteo_data/file_list.\"+str(i)+\".keyset\" for i in range(3, 5)], \\\n",
    "                 eval_source = \"criteo_data/file_list.9.txt\")\n",
    "\n",
    "model.save_params_to_files(\"wdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afc9424b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUGECTR][03:34:23][INFO][RANK0]: Empty embedding, trained table will be stored in ./wdl_0_sparse_model\n",
      "[HUGECTR][03:34:23][INFO][RANK0]: Empty embedding, trained table will be stored in ./wdl_1_sparse_model\n",
      "HugeCTR Version: 3.2\n",
      "====================================================Model Init=====================================================\n",
      "[HUGECTR][03:34:23][INFO][RANK0]: Initialize model: wdl\n",
      "[HUGECTR][03:34:23][INFO][RANK0]: Global seed is 337017754\n",
      "[HUGECTR][03:34:23][INFO][RANK0]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[HUGECTR][03:34:25][WARNING][RANK0]: Peer-to-peer access cannot be fully enabled.\n",
      "[HUGECTR][03:34:25][INFO][RANK0]: Start all2all warmup\n",
      "[HUGECTR][03:34:25][INFO][RANK0]: End all2all warmup\n",
      "[HUGECTR][03:34:25][INFO][RANK0]: Using All-reduce algorithm: NCCL\n",
      "[HUGECTR][03:34:25][INFO][RANK0]: Device 0: Tesla V100-SXM2-32GB\n",
      "[HUGECTR][03:34:25][DEBUG][RANK0]: Creating Kafka lifetime service.\n",
      "[HUGECTR][03:34:25][INFO][RANK0]: num of DataReader workers: 12\n",
      "[HUGECTR][03:34:25][INFO][RANK0]: max_vocabulary_size_per_gpu_=6029312\n",
      "[HUGECTR][03:34:25][INFO][RANK0]: max_vocabulary_size_per_gpu_=5865472\n",
      "[HUGECTR][03:34:25][INFO][RANK0]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "[HUGECTR][03:34:28][INFO][RANK0]: gpu0 start to init embedding\n",
      "[HUGECTR][03:34:28][INFO][RANK0]: gpu0 init embedding done\n",
      "[HUGECTR][03:34:28][INFO][RANK0]: gpu0 start to init embedding\n",
      "[HUGECTR][03:34:28][INFO][RANK0]: gpu0 init embedding done\n",
      "[HUGECTR][03:34:28][INFO][RANK0]: Enable HMEM-Based Parameter Server\n",
      "[HUGECTR][03:34:28][INFO][RANK0]: ./wdl_0_sparse_model not exist, create and train from scratch\n",
      "[HUGECTR][03:34:28][INFO][RANK0]: Enable HMemCache-Based Parameter Server\n",
      "[HUGECTR][03:34:28][INFO][RANK0]: ./wdl_1_sparse_model/key doesn't exist, created\n",
      "[HUGECTR][03:34:28][INFO][RANK0]: ./wdl_1_sparse_model/emb_vector doesn't exist, created\n",
      "[HUGECTR][03:34:28][INFO][RANK0]: ./wdl_1_sparse_model/Adam.m doesn't exist, created\n",
      "[HUGECTR][03:34:28][INFO][RANK0]: ./wdl_1_sparse_model/Adam.v doesn't exist, created\n",
      "[HUGECTR][03:34:29][INFO][RANK0]: Starting AUC NCCL warm-up\n",
      "[HUGECTR][03:34:29][INFO][RANK0]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "label                                   Dense                         Sparse                        \n",
      "label                                   dense                          wide_data,deep_data           \n",
      "(None, 1)                               (None, 13)                              \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "DistributedSlotSparseEmbeddingHash      wide_data                     sparse_embedding0             (None, 1, 1)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      deep_data                     sparse_embedding1             (None, 26, 16)                \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 416)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding0             reshape2                      (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Concat                                  reshape1                      concat1                       (None, 429)                   \n",
      "                                        dense                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            concat1                       fc1                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout1                      fc2                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc2                           relu2                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu2                         dropout2                      (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout2                      fc3                           (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Add                                     fc3                           add1                          (None, 1)                     \n",
      "                                        reshape2                                                                                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  add1                          loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "[HUGECTR][03:34:29][INFO][RANK0]: Save the model graph to wdl.json successfully\n",
      "=====================================================Model Fit=====================================================\n",
      "[HUGECTR][03:34:29][INFO][RANK0]: Use embedding training cache mode with number of training sources: 2, number of epochs: 1\n",
      "[HUGECTR][03:34:29][INFO][RANK0]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[HUGECTR][03:34:29][INFO][RANK0]: Evaluation interval: 1000, snapshot interval: 10000\n",
      "[HUGECTR][03:34:29][INFO][RANK0]: Dense network trainable: True\n",
      "[HUGECTR][03:34:29][INFO][RANK0]: Sparse embedding sparse_embedding0 trainable: True\n",
      "[HUGECTR][03:34:29][INFO][RANK0]: Sparse embedding sparse_embedding1 trainable: True\n",
      "[HUGECTR][03:34:29][INFO][RANK0]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HUGECTR][03:34:29][INFO][RANK0]: lr: 0.001000, warmup_steps: 1, end_lr: 0.000000\n",
      "[HUGECTR][03:34:29][INFO][RANK0]: decay_start: 0, decay_steps: 1, decay_power: 2.000000\n",
      "[HUGECTR][03:34:29][INFO][RANK0]: Evaluation source file: criteo_data/file_list.2.txt\n",
      "[HUGECTR][03:34:29][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.0.txt--------------------\n",
      "[HUGECTR][03:34:29][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][03:34:29][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 0 %\n",
      "[HUGECTR][03:34:30][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.1.txt--------------------\n",
      "[HUGECTR][03:34:30][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][03:34:30][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 0 %\n",
      "[HUGECTR][03:34:31][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 0 %\n",
      "[HUGECTR][03:34:31][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 76.51 %\n",
      "[HUGECTR][03:34:31][INFO][RANK0]: Updating sparse model in SSD [DONE]\n",
      "[HUGECTR][03:34:32][INFO][RANK0]: Sync blocks from HMEM-Cache to SSD\n",
      " \u001b[38;2;89;255;89m ████████████████████████████████████████▏ \u001b[1m\u001b[31m100.0% \u001b[34m[   2/   2 | 79.0 Hz | 0s<0s]  \u001b[0m\u001b[32m\u001b[0mm\n",
      "[HUGECTR][03:34:32][INFO][RANK0]: Dumping dense weights to file, successful\n",
      "[HUGECTR][03:34:32][INFO][RANK0]: Dumping dense optimizer states to file, successful\n",
      "[HUGECTR][03:34:32][INFO][RANK0]: Dumping untrainable weights to file, successful\n",
      "[HUGECTR][03:34:33][DEBUG][RANK0]: Destroying Kafka lifetime service.\n"
     ]
    }
   ],
   "source": [
    "!python wdl_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57656d9a",
   "metadata": {},
   "source": [
    "## 4. WDL Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ec4d8f",
   "metadata": {},
   "source": [
    "### 4.1 Inference using HugeCTR python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4cedad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wdl_infer’: File exists\n",
      "mkdir: cannot create directory ‘/wdl_infer/rocksdb’: File exists\n"
     ]
    }
   ],
   "source": [
    "#Create a folder for RocksDB\n",
    "!mkdir /wdl_infer\n",
    "!mkdir /wdl_infer/rocksdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65911e49",
   "metadata": {},
   "source": [
    "**Please make sure you have started Redis cluster following the README before you start doing inference.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d90ce3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wdl_predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 'wdl_predict.py'\n",
    "from hugectr.inference import InferenceParams, CreateInferenceSession\n",
    "import hugectr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "def wdl_inference(model_name='wdl', network_file='wdl.json', dense_file='wdl_dense_0.model', \\\n",
    "                  embedding_file_list=['wdl_0_sparse_model', 'wdl_1_sparse_model'], data_file='infer_data.csv',\\\n",
    "                  enable_cache=False, rocksdb_path=\"\"):\n",
    "    CATEGORICAL_COLUMNS=[\"C1_C2\",\"C3_C4\"] + [\"C\" + str(x) for x in range(1, 27)]\n",
    "    CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "    LABEL_COLUMNS = ['label']\n",
    "    test_df=pd.read_csv(data_file,sep=',')\n",
    "    config_file = network_file\n",
    "    row_ptrs = list(range(0, 11, 2)) + list(range(0, 131))\n",
    "    dense_features =  list(test_df[CONTINUOUS_COLUMNS].values.flatten())\n",
    "    test_df[CATEGORICAL_COLUMNS].astype(np.int64)\n",
    "    embedding_columns = list((test_df[CATEGORICAL_COLUMNS]).values.flatten())\n",
    "\n",
    "    cpuMemoryDatabase=hugectr.inference.CPUMemoryDatabaseParams(hugectr.DatabaseType_t.disabled, initial_cache_rate=0.2)\n",
    "    rocksdbdatabase=hugectr.inference.PersistentDatabaseParams(hugectr.DatabaseType_t.rocks_db,path=\"/wdl_infer/rocksdb/\")\n",
    "    redisdatabase=hugectr.inference.DistributedDatabaseParams(hugectr.DatabaseType_t.redis_cluster,address=\"127.0.0.1:7000,127.0.0.1:7001,127.0.0.1:7002\")\n",
    "    \n",
    "    # create parameter server, embedding cache and inference session\n",
    "    inference_params = InferenceParams(model_name = model_name,\n",
    "                                max_batchsize = 64,\n",
    "                                hit_rate_threshold = 0.5,\n",
    "                                dense_model_file = dense_file,\n",
    "                                sparse_model_files = embedding_file_list,\n",
    "                                device_id = 0,\n",
    "                                use_gpu_embedding_cache = enable_cache,\n",
    "                                cache_size_percentage = 0.9,\n",
    "                                i64_input_key = True,\n",
    "                                use_mixed_precision = False,\n",
    "                                cpu_memory_db=cpuMemoryDatabase,\n",
    "                                persistent_db=rocksdbdatabase,\n",
    "                                distributed_db=redisdatabase)\n",
    "    inference_session = CreateInferenceSession(config_file, inference_params)\n",
    "    output = inference_session.predict(dense_features, embedding_columns, row_ptrs)\n",
    "    print(\"WDL multi-embedding table inference result is {}\".format(output))\n",
    "\n",
    "wdl_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "826e9a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUGECTR][03:36:30][INFO][RANK0]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[HUGECTR][03:36:30][INFO][RANK0]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[HUGECTR][03:36:30][INFO][RANK0]: Creating RedisCluster backend...\n",
      "[HUGECTR][03:36:30][INFO][RANK0]: Connecting to Redis cluster via 127.0.0.1:7000 ...\n",
      "[HUGECTR][03:36:30][INFO][RANK0]: Connected to Redis database!\n",
      "[HUGECTR][03:36:30][INFO][RANK0]: Creating RocksDB backend...\n",
      "[HUGECTR][03:36:30][INFO][RANK0]: Connecting to RocksDB database...\n",
      "[HUGECTR][03:36:30][INFO][RANK0]: RocksDB /wdl_infer/rocksdb/, found column family \"default\".\n",
      "[HUGECTR][03:36:30][INFO][RANK0]: RocksDB /wdl_infer/rocksdb/, found column family \"hctr_et.wdl.sparse_embedding0\".\n",
      "[HUGECTR][03:36:30][INFO][RANK0]: RocksDB /wdl_infer/rocksdb/, found column family \"hctr_et.wdl.sparse_embedding1\".\n",
      "[HUGECTR][03:36:31][INFO][RANK0]: Connected to RocksDB database!\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p0/v, query 0: Inserted 5565 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p1/v, query 0: Inserted 5553 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p2/v, query 0: Inserted 5572 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p3/v, query 0: Inserted 5553 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p4/v, query 0: Inserted 5485 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p5/v, query 0: Inserted 5556 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p6/v, query 0: Inserted 5584 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p7/v, query 0: Inserted 5605 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: RedisCluster backend. Table: hctr_et.wdl.sparse_embedding0. Inserted 44473 / 44473 pairs.\n",
      "[HUGECTR][03:36:31][INFO][RANK0]: Table: hctr_et.wdl.sparse_embedding0; cached 44473 / 44473 embeddings in distributed database!\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding0, query 0: Inserted 10000 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding0, query 1: Inserted 10000 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding0, query 2: Inserted 10000 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding0, query 3: Inserted 10000 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding0, query 4: Inserted 4473 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: RocksDB backend. Table: hctr_et.wdl.sparse_embedding0. Inserted 44473 / 44473 pairs.\n",
      "[HUGECTR][03:36:31][INFO][RANK0]: Table: hctr_et.wdl.sparse_embedding0; cached 44473 embeddings in persistent database!\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p0/v, query 0: Inserted 6801 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p1/v, query 0: Inserted 6769 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p2/v, query 0: Inserted 6745 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p3/v, query 0: Inserted 6797 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p4/v, query 0: Inserted 6771 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p5/v, query 0: Inserted 6757 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p6/v, query 0: Inserted 6837 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p7/v, query 0: Inserted 6807 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: RedisCluster backend. Table: hctr_et.wdl.sparse_embedding1. Inserted 54284 / 54284 pairs.\n",
      "[HUGECTR][03:36:31][INFO][RANK0]: Table: hctr_et.wdl.sparse_embedding1; cached 54284 / 54284 embeddings in distributed database!\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding1, query 0: Inserted 10000 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding1, query 1: Inserted 10000 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding1, query 2: Inserted 10000 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding1, query 3: Inserted 10000 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding1, query 4: Inserted 10000 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding1, query 5: Inserted 4284 pairs.\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: RocksDB backend. Table: hctr_et.wdl.sparse_embedding1. Inserted 54284 / 54284 pairs.\n",
      "[HUGECTR][03:36:31][INFO][RANK0]: Table: hctr_et.wdl.sparse_embedding1; cached 54284 embeddings in persistent database!\n",
      "[HUGECTR][03:36:31][DEBUG][RANK0]: Real-time subscribers created!\n",
      "[HUGECTR][03:36:31][INFO][RANK0]: Create embedding cache in device 0.\n",
      "[HUGECTR][03:36:31][INFO][RANK0]: Use GPU embedding cache: False, cache size percentage: 0.900000\n",
      "[HUGECTR][03:36:31][INFO][RANK0]: Configured cache hit rate threshold: 0.500000\n",
      "[HUGECTR][03:36:31][INFO][RANK0]: Global seed is 2566656433\n",
      "[HUGECTR][03:36:31][INFO][RANK0]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[HUGECTR][03:36:32][WARNING][RANK0]: Peer-to-peer access cannot be fully enabled.\n",
      "[HUGECTR][03:36:32][INFO][RANK0]: Start all2all warmup\n",
      "[HUGECTR][03:36:32][INFO][RANK0]: End all2all warmup\n",
      "[HUGECTR][03:36:32][INFO][RANK0]: Model name: wdl\n",
      "[HUGECTR][03:36:32][INFO][RANK0]: Use mixed precision: False\n",
      "[HUGECTR][03:36:32][INFO][RANK0]: Use cuda graph: True\n",
      "[HUGECTR][03:36:32][INFO][RANK0]: Max batchsize: 64\n",
      "[HUGECTR][03:36:32][INFO][RANK0]: Use I64 input key: True\n",
      "[HUGECTR][03:36:32][INFO][RANK0]: start create embedding for inference\n",
      "[HUGECTR][03:36:32][INFO][RANK0]: sparse_input name wide_data\n",
      "[HUGECTR][03:36:32][INFO][RANK0]: sparse_input name deep_data\n",
      "[HUGECTR][03:36:32][INFO][RANK0]: create embedding for inference success\n",
      "[HUGECTR][03:36:32][INFO][RANK0]: Inference stage skip BinaryCrossEntropyLoss layer, replaced by Sigmoid layer\n",
      "[HUGECTR][03:36:33][INFO][RANK0]: Looking up 10 embeddings (each with 1 values)...\n",
      "[HUGECTR][03:36:33][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p1/v, query 0: Fetched 2 keys. Hits 2.\n",
      "[HUGECTR][03:36:33][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p2/v, query 0: Fetched 2 keys. Hits 2.\n",
      "[HUGECTR][03:36:33][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p3/v, query 0: Fetched 2 keys. Hits 2.\n",
      "[HUGECTR][03:36:33][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p4/v, query 0: Fetched 1 keys. Hits 1.\n",
      "[HUGECTR][03:36:33][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p5/v, query 0: Fetched 1 keys. Hits 1.\n",
      "[HUGECTR][03:36:33][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p6/v, query 0: Fetched 1 keys. Hits 1.\n",
      "[HUGECTR][03:36:33][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p7/v, query 0: Fetched 1 keys. Hits 1.\n",
      "[HUGECTR][03:36:33][DEBUG][RANK0]: RedisCluster backend. Table: hctr_et.wdl.sparse_embedding0. Fetched 10 / 10 values.\n",
      "[HUGECTR][03:36:33][DEBUG][RANK0]: RedisCluster: 10 hits, 0 missing!\n",
      "[HUGECTR][03:36:33][DEBUG][RANK0]: RocksDB backend. Table: hctr_et.wdl.sparse_embedding0. Fetched 0 / 0 values.\n",
      "[HUGECTR][03:36:33][DEBUG][RANK0]: RocksDB: 10 hits, 0 missing!\n",
      "[HUGECTR][03:36:33][INFO][RANK0]: Parameter server lookup of 10 / 10 embeddings took 656 us.\n",
      "[HUGECTR][03:36:33][INFO][RANK0]: Looking up 130 embeddings (each with 16 values)...\n",
      "[HUGECTR][03:36:33][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p0/v, query 0: Fetched 10 keys. Hits 10.\n",
      "[HUGECTR][03:36:33][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p1/v, query 0: Fetched 16 keys. Hits 16.\n",
      "[HUGECTR][03:36:33][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p2/v, query 0: Fetched 17 keys. Hits 17.\n",
      "[HUGECTR][03:36:33][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p3/v, query 0: Fetched 16 keys. Hits 16.\n",
      "[HUGECTR][03:36:33][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p4/v, query 0: Fetched 18 keys. Hits 18.\n",
      "[HUGECTR][03:36:33][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p5/v, query 0: Fetched 14 keys. Hits 14.\n",
      "[HUGECTR][03:36:33][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p6/v, query 0: Fetched 21 keys. Hits 21.\n",
      "[HUGECTR][03:36:33][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p7/v, query 0: Fetched 18 keys. Hits 18.\n",
      "[HUGECTR][03:36:33][DEBUG][RANK0]: RedisCluster backend. Table: hctr_et.wdl.sparse_embedding1. Fetched 130 / 130 values.\n",
      "[HUGECTR][03:36:33][DEBUG][RANK0]: RedisCluster: 130 hits, 0 missing!\n",
      "[HUGECTR][03:36:33][DEBUG][RANK0]: RocksDB backend. Table: hctr_et.wdl.sparse_embedding1. Fetched 0 / 0 values.\n",
      "[HUGECTR][03:36:33][DEBUG][RANK0]: RocksDB: 130 hits, 0 missing!\n",
      "[HUGECTR][03:36:33][INFO][RANK0]: Parameter server lookup of 130 / 130 embeddings took 882 us.\n",
      "WDL multi-embedding table inference result is [0.013668588362634182, 0.008148659951984882, 0.06785331666469574, 0.007276115473359823, 0.019930679351091385]\n",
      "[HUGECTR][03:36:33][INFO][RANK0]: Disconnecting from RocksDB database...\n",
      "[HUGECTR][03:36:33][INFO][RANK0]: Disconnected from RocksDB database!\n",
      "[HUGECTR][03:36:33][INFO][RANK0]: Disconnecting from Redis database...\n",
      "[HUGECTR][03:36:33][INFO][RANK0]: Disconnected from Redis database!\n"
     ]
    }
   ],
   "source": [
    "!python wdl_predict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff241a12",
   "metadata": {},
   "source": [
    "### 4.2 Inference using Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f0f1b6",
   "metadata": {},
   "source": [
    "**Please refer to the [HugeCTR_Continuous_Training_and_Inference(Part2)](#inference-using-triton) notebook to start Triton and do the inference.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946f822f",
   "metadata": {},
   "source": [
    "## 5.Continue Training WDL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffe229a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wdl_continue.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wdl_continue.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(model_name = \"wdl\",\n",
    "                              max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False,\n",
    "                              use_cuda_graph = True,\n",
    "                              kafka_brockers = \"10.23.137.25:9093\")\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"criteo_data/file_list.\"+str(i)+\".txt\" for i in range(6, 9)],\n",
    "                          keyset = [\"criteo_data/file_list.\"+str(i)+\".keyset\" for i in range(6, 9)],\n",
    "                          eval_source = \"criteo_data/file_list.9.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "hc_config = hugectr.CreateHMemCache(2, 0.5, 0)\n",
    "etc = hugectr.CreateETC(ps_types = [hugectr.TrainPSType_t.Staged, hugectr.TrainPSType_t.Cached],\\\n",
    "                        sparse_models = [\"./wdl_0_sparse_model\", \"./wdl_1_sparse_model\"],\\\n",
    "                        local_paths = [\"./\"], hmem_cache_configs = [hc_config])\n",
    "model = hugectr.Model(solver, reader, optimizer, etc)\n",
    "model.construct_from_json(graph_config_file = \"wdl.json\", include_dense_network = True)\n",
    "model.compile()\n",
    "model.load_dense_weights(\"wdl_dense_0_model\")\n",
    "model.load_dense_optimizer_states(\"dcn_opt_dense_1000.model\")\n",
    "\n",
    "model.summary()\n",
    "model.graph_to_json(graph_config_file = \"wdl.json\")\n",
    "model.fit(num_epochs = 1, display = 500, eval_interval = 1000)\n",
    "model.dump_incremental_model_2kafka()\n",
    "model.save_params_to_files(\"wdl_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be48d99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUGECTR][03:37:25][INFO][RANK0]: Use existing embedding: ./wdl_0_sparse_model\n",
      "[HUGECTR][03:37:25][INFO][RANK0]: Use existing embedding: ./wdl_1_sparse_model\n",
      "HugeCTR Version: 3.2\n",
      "====================================================Model Init=====================================================\n",
      "[HUGECTR][03:37:25][INFO][RANK0]: Initialize model: wdl\n",
      "[HUGECTR][03:37:25][INFO][RANK0]: Global seed is 2083265859\n",
      "[HUGECTR][03:37:25][INFO][RANK0]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[HUGECTR][03:37:27][WARNING][RANK0]: Peer-to-peer access cannot be fully enabled.\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: Start all2all warmup\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: End all2all warmup\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: Using All-reduce algorithm: NCCL\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: Device 0: Tesla V100-SXM2-32GB\n",
      "[HUGECTR][03:37:27][DEBUG][RANK0]: Creating Kafka lifetime service.\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: num of DataReader workers: 12\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: max_num_frequent_categories is not specified using default: 1\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: max_num_infrequent_samples is not specified using default: -1\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: p_dup_max is not specified using default: 0.010000\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: max_all_reduce_bandwidth is not specified using default: 130000000000.000000\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: max_all_to_all_bandwidth is not specified using default: 190000000000.000000\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: efficiency_bandwidth_ratio is not specified using default: 1.000000\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: use_train_precompute_indices is not specified using default: 0\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: use_eval_precompute_indices is not specified using default: 0\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: communication_type is not specified using default: IB_NVLink\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: hybrid_embedding_type is not specified using default: Distributed\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: max_vocabulary_size_per_gpu_=6029312\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: max_num_frequent_categories is not specified using default: 1\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: max_num_infrequent_samples is not specified using default: -1\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: p_dup_max is not specified using default: 0.010000\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: max_all_reduce_bandwidth is not specified using default: 130000000000.000000\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: max_all_to_all_bandwidth is not specified using default: 190000000000.000000\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: efficiency_bandwidth_ratio is not specified using default: 1.000000\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: use_train_precompute_indices is not specified using default: 0\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: use_eval_precompute_indices is not specified using default: 0\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: communication_type is not specified using default: IB_NVLink\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: hybrid_embedding_type is not specified using default: Distributed\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: max_vocabulary_size_per_gpu_=5865472\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: Load the model graph from wdl.json successfully\n",
      "[HUGECTR][03:37:27][INFO][RANK0]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "[HUGECTR][03:37:30][INFO][RANK0]: gpu0 start to init embedding\n",
      "[HUGECTR][03:37:30][INFO][RANK0]: gpu0 init embedding done\n",
      "[HUGECTR][03:37:30][INFO][RANK0]: gpu0 start to init embedding\n",
      "[HUGECTR][03:37:30][INFO][RANK0]: gpu0 init embedding done\n",
      "[HUGECTR][03:37:30][INFO][RANK0]: Enable HMEM-Based Parameter Server\n",
      "[HUGECTR][03:37:30][INFO][RANK0]: Enable HMemCache-Based Parameter Server\n",
      "[HUGECTR][03:37:31][INFO][RANK0]: Starting AUC NCCL warm-up\n",
      "[HUGECTR][03:37:31][INFO][RANK0]: Warm-up done\n",
      "0. Runtime error: Cannot open dense model file /jershi/HugeCTR_gitlab/hugectr/HugeCTR/pybind/model.cpp:1983 \n",
      "\n",
      "0. Runtime error: Cannot open dense opt states file /jershi/HugeCTR_gitlab/hugectr/HugeCTR/pybind/model.cpp:1934 \n",
      "\n",
      "===================================================Model Summary===================================================\n",
      "label                                   Dense                         Sparse                        \n",
      "label                                   dense                          wide_data,deep_data           \n",
      "(None, 1)                               (None, 13)                              \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "DistributedSlotSparseEmbeddingHash      wide_data                     sparse_embedding0             (None, 1, 1)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      deep_data                     sparse_embedding1             (None, 26, 16)                \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 416)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding0             reshape2                      (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Concat                                  reshape1                      concat1                       (None, 429)                   \n",
      "                                        dense                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            concat1                       fc1                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout1                      fc2                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc2                           relu2                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu2                         dropout2                      (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout2                      fc3                           (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Add                                     fc3                           add1                          (None, 1)                     \n",
      "                                        reshape2                                                                                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  add1                          loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "[HUGECTR][03:37:31][INFO][RANK0]: Save the model graph to wdl.json successfully\n",
      "=====================================================Model Fit=====================================================\n",
      "[HUGECTR][03:37:31][INFO][RANK0]: Use embedding training cache mode with number of training sources: 3, number of epochs: 1\n",
      "[HUGECTR][03:37:31][INFO][RANK0]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[HUGECTR][03:37:31][INFO][RANK0]: Evaluation interval: 1000, snapshot interval: 10000\n",
      "[HUGECTR][03:37:31][INFO][RANK0]: Dense network trainable: True\n",
      "[HUGECTR][03:37:31][INFO][RANK0]: Sparse embedding sparse_embedding0 trainable: True\n",
      "[HUGECTR][03:37:31][INFO][RANK0]: Sparse embedding sparse_embedding1 trainable: True\n",
      "[HUGECTR][03:37:31][INFO][RANK0]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HUGECTR][03:37:31][INFO][RANK0]: lr: 0.001000, warmup_steps: 1, end_lr: 0.000000\n",
      "[HUGECTR][03:37:31][INFO][RANK0]: decay_start: 0, decay_steps: 1, decay_power: 2.000000\n",
      "[HUGECTR][03:37:31][INFO][RANK0]: Evaluation source file: criteo_data/file_list.9.txt\n",
      "[HUGECTR][03:37:31][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.6.txt--------------------\n",
      "[HUGECTR][03:37:31][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][03:37:32][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 0 %\n",
      "[HUGECTR][03:37:32][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.7.txt--------------------\n",
      "[HUGECTR][03:37:32][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][03:37:32][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 90.64 %\n",
      "[HUGECTR][03:37:32][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 75.02 %\n",
      "[HUGECTR][03:37:33][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.8.txt--------------------\n",
      "[HUGECTR][03:37:33][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][03:37:33][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 95.81 %\n",
      "[HUGECTR][03:37:33][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 87.33 %\n",
      "[HUGECTR][03:37:34][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 85.8 %\n",
      "[HUGECTR][03:37:34][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 86.51 %\n",
      "[HUGECTR][03:37:34][INFO][RANK0]: Get updated portion of embedding table [DONE}\n",
      "[HUGECTR][03:37:34][INFO][RANK0]: Dump incremental parameters of hctr_et.wdl.sparse_embedding0 into kafka. Embedding size is 1, num_pairs is 58853 \n",
      "[HUGECTR][03:37:34][INFO][RANK0]: Creating new Kafka topic \"hctr_et.wdl.sparse_embedding0\".\n",
      "[HUGECTR][03:37:34][INFO][RANK0]: Dump incremental parameters of hctr_et.wdl.sparse_embedding1 into kafka. Embedding size is 16, num_pairs is 58383 \n",
      "[HUGECTR][03:37:34][INFO][RANK0]: Creating new Kafka topic \"hctr_et.wdl.sparse_embedding1\".\n",
      "[HUGECTR][03:37:42][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 85.8 %\n",
      "[HUGECTR][03:37:42][INFO][RANK0]: Updating sparse model in SSD [DONE]\n",
      "[HUGECTR][03:37:42][INFO][RANK0]: Sync blocks from HMEM-Cache to SSD\n",
      " \u001b[38;2;89;255;89m ████████████████████████████████████████▏ \u001b[1m\u001b[31m100.0% \u001b[34m[   2/   2 | 62.5 Hz | 0s<0s]  \u001b[0m\u001b[32m\u001b[0mm\n",
      "[HUGECTR][03:37:42][INFO][RANK0]: Dumping dense weights to file, successful\n",
      "[HUGECTR][03:37:43][INFO][RANK0]: Dumping dense optimizer states to file, successful\n",
      "[HUGECTR][03:37:43][INFO][RANK0]: Dumping untrainable weights to file, successful\n",
      "[HUGECTR][03:37:56][DEBUG][RANK0]: Destroying Kafka lifetime service.\n"
     ]
    }
   ],
   "source": [
    "!python wdl_continue.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328ea9b9",
   "metadata": {},
   "source": [
    "## 6. Inference with new model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c05641",
   "metadata": {},
   "source": [
    "### 6.1 Continuous inference using Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dd4bf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUGECTR][03:38:09][INFO][RANK0]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[HUGECTR][03:38:09][INFO][RANK0]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[HUGECTR][03:38:09][INFO][RANK0]: Creating RedisCluster backend...\n",
      "[HUGECTR][03:38:09][INFO][RANK0]: Connecting to Redis cluster via 127.0.0.1:7000 ...\n",
      "[HUGECTR][03:38:09][INFO][RANK0]: Connected to Redis database!\n",
      "[HUGECTR][03:38:09][INFO][RANK0]: Creating RocksDB backend...\n",
      "[HUGECTR][03:38:09][INFO][RANK0]: Connecting to RocksDB database...\n",
      "[HUGECTR][03:38:09][INFO][RANK0]: RocksDB /wdl_infer/rocksdb/, found column family \"default\".\n",
      "[HUGECTR][03:38:09][INFO][RANK0]: RocksDB /wdl_infer/rocksdb/, found column family \"hctr_et.wdl.sparse_embedding0\".\n",
      "[HUGECTR][03:38:09][INFO][RANK0]: RocksDB /wdl_infer/rocksdb/, found column family \"hctr_et.wdl.sparse_embedding1\".\n",
      "[HUGECTR][03:38:09][INFO][RANK0]: Connected to RocksDB database!\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p0/v, query 0: Inserted 10000 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p0/v, query 1: Inserted 243 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p1/v, query 0: Inserted 10000 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p1/v, query 1: Inserted 375 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p2/v, query 0: Inserted 10000 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p2/v, query 1: Inserted 329 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p3/v, query 0: Inserted 10000 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p3/v, query 1: Inserted 303 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p4/v, query 0: Inserted 10000 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p4/v, query 1: Inserted 318 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p5/v, query 0: Inserted 10000 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p5/v, query 1: Inserted 271 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p6/v, query 0: Inserted 10000 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p6/v, query 1: Inserted 310 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p7/v, query 0: Inserted 10000 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p7/v, query 1: Inserted 345 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: RedisCluster backend. Table: hctr_et.wdl.sparse_embedding0. Inserted 82494 / 82494 pairs.\n",
      "[HUGECTR][03:38:09][INFO][RANK0]: Table: hctr_et.wdl.sparse_embedding0; cached 82494 / 82494 embeddings in distributed database!\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding0, query 0: Inserted 10000 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding0, query 1: Inserted 10000 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding0, query 2: Inserted 10000 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding0, query 3: Inserted 10000 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding0, query 4: Inserted 10000 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding0, query 5: Inserted 10000 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding0, query 6: Inserted 10000 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding0, query 7: Inserted 10000 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding0, query 8: Inserted 2494 pairs.\n",
      "[HUGECTR][03:38:09][DEBUG][RANK0]: RocksDB backend. Table: hctr_et.wdl.sparse_embedding0. Inserted 82494 / 82494 pairs.\n",
      "[HUGECTR][03:38:09][INFO][RANK0]: Table: hctr_et.wdl.sparse_embedding0; cached 82494 embeddings in persistent database!\n",
      "[HUGECTR][03:38:10][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p0/v, query 0: Inserted 7628 pairs.\n",
      "[HUGECTR][03:38:10][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p1/v, query 0: Inserted 7631 pairs.\n",
      "[HUGECTR][03:38:10][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p2/v, query 0: Inserted 7629 pairs.\n",
      "[HUGECTR][03:38:10][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p3/v, query 0: Inserted 7628 pairs.\n",
      "[HUGECTR][03:38:10][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p4/v, query 0: Inserted 7628 pairs.\n",
      "[HUGECTR][03:38:10][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p5/v, query 0: Inserted 7629 pairs.\n",
      "[HUGECTR][03:38:10][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p6/v, query 0: Inserted 7627 pairs.\n",
      "[HUGECTR][03:38:10][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p7/v, query 0: Inserted 7635 pairs.\n",
      "[HUGECTR][03:38:10][DEBUG][RANK0]: RedisCluster backend. Table: hctr_et.wdl.sparse_embedding1. Inserted 61035 / 61035 pairs.\n",
      "[HUGECTR][03:38:10][INFO][RANK0]: Table: hctr_et.wdl.sparse_embedding1; cached 61035 / 61035 embeddings in distributed database!\n",
      "[HUGECTR][03:38:10][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding1, query 0: Inserted 10000 pairs.\n",
      "[HUGECTR][03:38:10][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding1, query 1: Inserted 10000 pairs.\n",
      "[HUGECTR][03:38:10][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding1, query 2: Inserted 10000 pairs.\n",
      "[HUGECTR][03:38:10][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding1, query 3: Inserted 10000 pairs.\n",
      "[HUGECTR][03:38:10][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding1, query 4: Inserted 10000 pairs.\n",
      "[HUGECTR][03:38:10][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding1, query 5: Inserted 10000 pairs.\n",
      "[HUGECTR][03:38:10][DEBUG][RANK0]: RocksDB table hctr_et.wdl.sparse_embedding1, query 6: Inserted 1035 pairs.\n",
      "[HUGECTR][03:38:10][DEBUG][RANK0]: RocksDB backend. Table: hctr_et.wdl.sparse_embedding1. Inserted 61035 / 61035 pairs.\n",
      "[HUGECTR][03:38:10][INFO][RANK0]: Table: hctr_et.wdl.sparse_embedding1; cached 61035 embeddings in persistent database!\n",
      "[HUGECTR][03:38:10][DEBUG][RANK0]: Real-time subscribers created!\n",
      "[HUGECTR][03:38:10][INFO][RANK0]: Create embedding cache in device 0.\n",
      "[HUGECTR][03:38:10][INFO][RANK0]: Use GPU embedding cache: False, cache size percentage: 0.900000\n",
      "[HUGECTR][03:38:10][INFO][RANK0]: Configured cache hit rate threshold: 0.500000\n",
      "[HUGECTR][03:38:10][INFO][RANK0]: Global seed is 2362747437\n",
      "[HUGECTR][03:38:10][INFO][RANK0]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[HUGECTR][03:38:11][WARNING][RANK0]: Peer-to-peer access cannot be fully enabled.\n",
      "[HUGECTR][03:38:11][INFO][RANK0]: Start all2all warmup\n",
      "[HUGECTR][03:38:11][INFO][RANK0]: End all2all warmup\n",
      "[HUGECTR][03:38:11][INFO][RANK0]: Model name: wdl\n",
      "[HUGECTR][03:38:11][INFO][RANK0]: Use mixed precision: False\n",
      "[HUGECTR][03:38:11][INFO][RANK0]: Use cuda graph: True\n",
      "[HUGECTR][03:38:11][INFO][RANK0]: Max batchsize: 64\n",
      "[HUGECTR][03:38:11][INFO][RANK0]: Use I64 input key: True\n",
      "[HUGECTR][03:38:11][INFO][RANK0]: start create embedding for inference\n",
      "[HUGECTR][03:38:11][INFO][RANK0]: sparse_input name wide_data\n",
      "[HUGECTR][03:38:11][INFO][RANK0]: sparse_input name deep_data\n",
      "[HUGECTR][03:38:11][INFO][RANK0]: create embedding for inference success\n",
      "[HUGECTR][03:38:11][INFO][RANK0]: Inference stage skip BinaryCrossEntropyLoss layer, replaced by Sigmoid layer\n",
      "[HUGECTR][03:38:12][INFO][RANK0]: Looking up 10 embeddings (each with 1 values)...\n",
      "[HUGECTR][03:38:12][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p1/v, query 0: Fetched 2 keys. Hits 2.\n",
      "[HUGECTR][03:38:12][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p2/v, query 0: Fetched 2 keys. Hits 2.\n",
      "[HUGECTR][03:38:12][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p3/v, query 0: Fetched 2 keys. Hits 2.\n",
      "[HUGECTR][03:38:12][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p4/v, query 0: Fetched 1 keys. Hits 1.\n",
      "[HUGECTR][03:38:12][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p5/v, query 0: Fetched 1 keys. Hits 1.\n",
      "[HUGECTR][03:38:12][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p6/v, query 0: Fetched 1 keys. Hits 1.\n",
      "[HUGECTR][03:38:12][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding0/p7/v, query 0: Fetched 1 keys. Hits 1.\n",
      "[HUGECTR][03:38:12][DEBUG][RANK0]: RedisCluster backend. Table: hctr_et.wdl.sparse_embedding0. Fetched 10 / 10 values.\n",
      "[HUGECTR][03:38:12][DEBUG][RANK0]: RedisCluster: 10 hits, 0 missing!\n",
      "[HUGECTR][03:38:12][DEBUG][RANK0]: RocksDB backend. Table: hctr_et.wdl.sparse_embedding0. Fetched 0 / 0 values.\n",
      "[HUGECTR][03:38:12][DEBUG][RANK0]: RocksDB: 10 hits, 0 missing!\n",
      "[HUGECTR][03:38:12][INFO][RANK0]: Parameter server lookup of 10 / 10 embeddings took 679 us.\n",
      "[HUGECTR][03:38:12][INFO][RANK0]: Looking up 130 embeddings (each with 16 values)...\n",
      "[HUGECTR][03:38:12][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p0/v, query 0: Fetched 10 keys. Hits 10.\n",
      "[HUGECTR][03:38:12][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p1/v, query 0: Fetched 16 keys. Hits 16.\n",
      "[HUGECTR][03:38:12][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p2/v, query 0: Fetched 17 keys. Hits 17.\n",
      "[HUGECTR][03:38:12][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p3/v, query 0: Fetched 16 keys. Hits 16.\n",
      "[HUGECTR][03:38:12][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p4/v, query 0: Fetched 18 keys. Hits 18.\n",
      "[HUGECTR][03:38:12][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p5/v, query 0: Fetched 14 keys. Hits 14.\n",
      "[HUGECTR][03:38:12][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p6/v, query 0: Fetched 21 keys. Hits 21.\n",
      "[HUGECTR][03:38:12][DEBUG][RANK0]: Redis partition hctr_et.wdl.sparse_embedding1/p7/v, query 0: Fetched 18 keys. Hits 18.\n",
      "[HUGECTR][03:38:12][DEBUG][RANK0]: RedisCluster backend. Table: hctr_et.wdl.sparse_embedding1. Fetched 130 / 130 values.\n",
      "[HUGECTR][03:38:12][DEBUG][RANK0]: RedisCluster: 130 hits, 0 missing!\n",
      "[HUGECTR][03:38:12][DEBUG][RANK0]: RocksDB backend. Table: hctr_et.wdl.sparse_embedding1. Fetched 0 / 0 values.\n",
      "[HUGECTR][03:38:12][DEBUG][RANK0]: RocksDB: 130 hits, 0 missing!\n",
      "[HUGECTR][03:38:12][INFO][RANK0]: Parameter server lookup of 130 / 130 embeddings took 712 us.\n",
      "WDL multi-embedding table inference result is [0.0036218352615833282, 0.000900191895198077, 0.0546233244240284, 0.0028622469399124384, 0.005312761757522821]\n",
      "[HUGECTR][03:38:12][INFO][RANK0]: Disconnecting from RocksDB database...\n",
      "[HUGECTR][03:38:12][INFO][RANK0]: Disconnected from RocksDB database!\n",
      "[HUGECTR][03:38:12][INFO][RANK0]: Disconnecting from Redis database...\n",
      "[HUGECTR][03:38:12][INFO][RANK0]: Disconnected from Redis database!\n"
     ]
    }
   ],
   "source": [
    "!python wdl_predict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1fd122",
   "metadata": {},
   "source": [
    "### 6.2 Continuous inference using Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d852ae7",
   "metadata": {},
   "source": [
    "**Please refer to the [HugeCTR_Continuous_Training_and_Inference(Part2)](#inference-using-triton) notebook to do the inference.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
