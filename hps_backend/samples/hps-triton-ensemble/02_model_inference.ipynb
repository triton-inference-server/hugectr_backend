{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea4337c-6afe-49b3-a84d-49ed1a70c929",
   "metadata": {},
   "source": [
    "# Overview\n",
    "The **02_model_inference.ipynb** will cover following tasks\n",
    "  * Configure three backends in Triton format\n",
    "  * Deploy to inference with Triton ensemble mode\n",
    "  * Validate deployed ensemble model with dummy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7b43ab46-8b09-4a9c-b40a-2a1b9592ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tritonhttpclient\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4764ae-c99b-40d2-bb96-d6c52cddd801",
   "metadata": {},
   "source": [
    "## Configure 3 backends in Triton format\n",
    "The 3 backends are:\n",
    "* \"hps_embedding\" backend, HPS Triton backend for embedding lookup serving\n",
    "* \"tf_reshape_dense_model\" backend, Tensorflow Triton backend for dense model serving\n",
    "* \"ensemble_model\" backend, integrates the above two backends and serves as one ensemble service\n",
    "![HPS_Triton_overview](./pic/hps_triton_overview.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3bdd47cc-4307-4301-998e-ee384f3267b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict()\n",
    "args[\"slot_num\"] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea13f464-a972-4573-950b-4a6ae221fbb2",
   "metadata": {},
   "source": [
    "### Prepare Triton Inference Server directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bd2eafea-a703-4427-9f0a-d4a8d6114ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/hps_demo\"\n",
    "\n",
    "def repo_check(repo_path):\n",
    "    if os.path.isdir(repo_path):\n",
    "        shutil.rmtree(repo_path)\n",
    "    os.makedirs(repo_path)\n",
    "\n",
    "hps_embedding_repo    = os.path.join(BASE_DIR, \"hps_embedding\")\n",
    "repo_check(hps_embedding_repo)\n",
    "hps_embedding_version = os.path.join(hps_embedding_repo, \"1\")\n",
    "repo_check(hps_embedding_version)\n",
    "\n",
    "tf_reshape_dense_model_repo    = os.path.join(BASE_DIR, \"tf_reshape_dense_model\")\n",
    "repo_check(tf_reshape_dense_model_repo)\n",
    "tf_reshape_dense_model_version = os.path.join(tf_reshape_dense_model_repo, \"1\")\n",
    "repo_check(tf_reshape_dense_model_version)\n",
    "\n",
    "ensemble_model_repo = os.path.join(BASE_DIR, \"ensemble_model\")\n",
    "repo_check(ensemble_model_repo)\n",
    "ensemble_model_version = os.path.join(ensemble_model_repo, \"1\")\n",
    "repo_check(ensemble_model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b28f8933-395e-4262-b2c5-c884d2ec6e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/hps_demo\u001b[00m\n",
      "├── \u001b[01;34mensemble_model\u001b[00m\n",
      "│   └── \u001b[01;34m1\u001b[00m\n",
      "├── \u001b[01;34mhps_embedding\u001b[00m\n",
      "│   └── \u001b[01;34m1\u001b[00m\n",
      "└── \u001b[01;34mtf_reshape_dense_model\u001b[00m\n",
      "    └── \u001b[01;34m1\u001b[00m\n",
      "\n",
      "6 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "# check created repository \n",
    "!tree /hps_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d752ccf1-d2f3-452b-8918-25b5cb4f5ca2",
   "metadata": {},
   "source": [
    "### Configure \"hps_embedding\" HPS backend\n",
    "For more references of HPS backend building, please check [Hierarchical Parameter Server Demo](../../samples/Hierarchical_Parameter_Server_Deployment.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6d80c478-e183-4872-b5fe-cfefcf4be481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /hps_demo/hps_embedding/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile $hps_embedding_repo/config.pbtxt\n",
    "name: \"hps_embedding\"\n",
    "backend: \"hps\"\n",
    "max_batch_size:1024\n",
    "input [\n",
    "  {\n",
    "    name: \"KEYS\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"NUMKEYS\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "version_policy: {\n",
    "        specific:{versions: 1}\n",
    "},\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind : KIND_GPU\n",
    "    gpus:[0]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9f428c",
   "metadata": {},
   "source": [
    "Generate the HPS configuration for deploying embedding tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "052722ae-6419-45fc-97a2-85327112241f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /hps_demo/hps_embedding/hps_embedding.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile $hps_embedding_repo/hps_embedding.json\n",
    "{\n",
    "    \"supportlonglong\": true,\n",
    "    \"models\": [{\n",
    "        \"model\": \"hps_embedding\",\n",
    "        \"sparse_files\": [\"/hps_demo/hps_embedding/1/naive_dnn_sparse.model\"],\n",
    "        \"num_of_worker_buffer_in_pool\": 3,\n",
    "        \"embedding_table_names\":[\"sparse_embedding1\"],\n",
    "        \"embedding_vecsize_per_table\": [16],\n",
    "        \"maxnum_catfeature_query_per_table_per_sample\": [3],\n",
    "        \"default_value_for_each_table\": [1.0],\n",
    "        \"deployed_device_list\": [0],\n",
    "        \"max_batch_size\": 65536,\n",
    "        \"cache_refresh_percentage_per_iteration\": 0.2,\n",
    "        \"hit_rate_threshold\": 1.0,\n",
    "        \"gpucacheper\": 1.0,\n",
    "        \"gpucache\": true\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b686550f-17f5-401b-96b1-101a7a746e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy sparse(HPS) model to target folder\n",
    "!cp -r ./naive_dnn_sparse.model $hps_embedding_version/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a04fdd7-d8f8-4ab4-90cf-b7bf1366ab13",
   "metadata": {},
   "source": [
    "### Configure \"tf_reshape_dense_model\" Tensorflow backend \n",
    "**Note**: For Triton TensorFlow backend, **platform** must be set to tensorflow_graphdef or **tensorflow_savedmodel**. Optionally 'backend' can be set to tensorflow [[link](https://github.com/triton-inference-server/backend#backends)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ce6e63d6-852d-49d1-8767-270c0304dc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /hps_demo/tf_reshape_dense_model/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile $tf_reshape_dense_model_repo/config.pbtxt\n",
    "name: \"tf_reshape_dense_model\"\n",
    "platform: \"tensorflow_savedmodel\"\n",
    "max_batch_size:0\n",
    "input [\n",
    "  {\n",
    "    name: \"input_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [-1]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [-1,1]\n",
    "  }\n",
    "]\n",
    "version_policy: {\n",
    "        specific:{versions: 1}\n",
    "},\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind : KIND_GPU\n",
    "    gpus:[0]\n",
    "  }\n",
    "]\n",
    "dynamic_batching {\n",
    "    max_queue_delay_microseconds: 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "92c485d4-dbf9-488b-a7a2-f3d41b801129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy reshaped TF dense model to target folder\n",
    "!cp -r ./naive_dnn_reshape_dense.model $tf_reshape_dense_model_version/model.savedmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426ae9c0-00a7-4357-8d08-900bb7212646",
   "metadata": {},
   "source": [
    "### Configure \"ensemble_model\" Triton backend\n",
    "**Note**: the \"**key**\" setting in step block must match the naming configuration in the former HPS and TF backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "86a8536e-b3af-4cd3-901e-8d11b60f4275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /hps_demo/ensemble_model/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile $ensemble_model_repo/config.pbtxt\n",
    "name: \"ensemble_model\"\n",
    "platform: \"ensemble\"\n",
    "max_batch_size: 16\n",
    "input [\n",
    "  {\n",
    "    name: \"EMB_KEY\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"EMB_N_KEY\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"DENSE_OUTPUT\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [1]\n",
    "  }\n",
    "]\n",
    "ensemble_scheduling {\n",
    "  step [\n",
    "    {\n",
    "      model_name: \"hps_embedding\"\n",
    "      model_version: -1\n",
    "      input_map {\n",
    "        key: \"KEYS\"\n",
    "        value: \"EMB_KEY\"\n",
    "      }\n",
    "      input_map {\n",
    "        key: \"NUMKEYS\"\n",
    "        value: \"EMB_N_KEY\"\n",
    "      }\n",
    "      output_map {\n",
    "        key: \"OUTPUT0\"\n",
    "        value: \"LOOKUP_VECTORS\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      model_name: \"tf_reshape_dense_model\"\n",
    "      model_version: -1\n",
    "      input_map {\n",
    "        key: \"input_1\"\n",
    "        value: \"LOOKUP_VECTORS\"\n",
    "      }\n",
    "      output_map {\n",
    "        key: \"output_1\"\n",
    "        value: \"DENSE_OUTPUT\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca76cda-04a2-4359-be30-d7b64ff2773e",
   "metadata": {},
   "source": [
    "### Check the generated directory and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a1bf9df9-3067-4b97-bba4-852cbab67c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/hps_demo\u001b[00m\n",
      "├── \u001b[01;34mensemble_model\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   └── config.pbtxt\n",
      "├── \u001b[01;34mhps_embedding\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   └── \u001b[01;34mnaive_dnn_sparse.model\u001b[00m\n",
      "│   │       ├── emb_vector\n",
      "│   │       └── key\n",
      "│   └── hps_embedding.json\n",
      "└── \u001b[01;34mtf_reshape_dense_model\u001b[00m\n",
      "    └── \u001b[01;34m1\u001b[00m\n",
      "        └── \u001b[01;34mmodel.savedmodel\u001b[00m\n",
      "            ├── \u001b[01;34massets\u001b[00m\n",
      "            ├── keras_metadata.pb\n",
      "            ├── saved_model.pb\n",
      "            └── \u001b[01;34mvariables\u001b[00m\n",
      "                ├── variables.data-00000-of-00001\n",
      "                └── variables.index\n",
      "\n",
      "10 directories, 8 files\n"
     ]
    }
   ],
   "source": [
    "!tree $BASE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f4b7c7-8ef5-4a55-a521-d82e3b4f771c",
   "metadata": {},
   "source": [
    "## Start Triton Inference Server, load 3 backends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4043a1b-130d-447b-ae2d-dcbc7493f252",
   "metadata": {},
   "source": [
    "Now, we assume you have checked your **tritonserver** version and confirmed that can run tritonserver command inside your docker container.\n",
    "\n",
    "For this tutorial, the command to start Triton will be\n",
    "> **tritonserver --model-repository=/hps_demo/ --backend-config=hps,ps=/hps_demo/hps_embedding/hps_embedding.json --backend-config=tensorflow,version=2**\n",
    "\n",
    "A few tips when using Tritonserver:\n",
    "* Tritonserver `--backend-directory` parameter only take one repository, we suggest you link the compiled hps `libtriton_hps.so` file to Triton default backend repository first. For instance, `ln -s /usr/local/hugectr/backends/hps/ /opt/tritonserver/backends/`\n",
    "\n",
    "* Tensorflow backend `libtriton_tensorflow2.so` need to be compiled first, and copy to the Triton default directory\n",
    "\n",
    "* Tritonserver support multiple `--backend-config` setting, e.g., we defined two backends (HPS and TF) in this tutorial\n",
    "\n",
    "* Tritonserver is using `--grpc-port=8000 --http-port=8001 --metrics-port=8002` as default, need to configure if has port conflict \n",
    "\n",
    "* No need to set `--model-control-mode=explicit` setting for Triton ensemble mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0550cab5-0049-43d8-b159-caee59bb85ab",
   "metadata": {},
   "source": [
    "If you successfully started tritonserver, you should see a log similar to following\n",
    "```\n",
    "...\n",
    "+------------------+------+\n",
    "| Repository Agent | Path |\n",
    "+------------------+------+\n",
    "+------------------+------+\n",
    "\n",
    "I0809 10:24:19.230206 53894 server.cc:583] \n",
    "+------------+-----------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+\n",
    "| Backend    | Path                                                            | Config                                                                                                               |\n",
    "+------------+-----------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+\n",
    "| tensorflow | /opt/tritonserver/backends/tensorflow2/libtriton_tensorflow2.so | {\"cmdline\":{\"auto-complete-config\":\"false\",\"backend-directory\":\"/opt/tritonserver/backends\",\"min-compute-capability\" |\n",
    "|            |                                                                 | :\"6.000000\",\"version\":\"2\",\"default-max-batch-size\":\"4\"}}                                                             |\n",
    "| hps        | /opt/tritonserver/backends/hps/libtriton_hps.so                 | {\"cmdline\":{\"auto-complete-config\":\"false\",\"backend-directory\":\"/opt/tritonserver/backends\",\"min-compute-capability\" |\n",
    "|            |                                                                 | :\"6.000000\",\"ps\":\"/hps_demo/hps_embedding.json\",\"default-max-batch-size\":\"4\"}}                                       |\n",
    "+------------+-----------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+\n",
    "\n",
    "I0809 10:24:19.230310 53894 server.cc:626] \n",
    "+--------------------+---------+--------+\n",
    "| Model              | Version | Status |\n",
    "+--------------------+---------+--------+\n",
    "| ensemble_model     | 1       | READY  |\n",
    "| hps_embedding      | 1       | READY  |\n",
    "| tf_new_dense_model | 1       | READY  |\n",
    "+--------------------+---------+--------+\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de058a77-6ffb-4ae6-b668-d66fb51cce1b",
   "metadata": {},
   "source": [
    "## Validate deployed ensemble model with dummy dataset\n",
    "### Step.1 Check Tritonserver health\n",
    "**Note**: if you are using default Tritonserver settings, the default port will be `8000` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1c917cb9-ef26-48bd-a4df-0b0e6b398c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 127.0.0.1:8000...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (127.0.0.1) port 8000 (#0)\n",
      "> GET /v2/health/ready HTTP/1.1\n",
      "> Host: localhost:8000\n",
      "> User-Agent: curl/7.68.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Length: 0\n",
      "< Content-Type: text/plain\n",
      "< \n",
      "* Connection #0 to host localhost left intact\n"
     ]
    }
   ],
   "source": [
    "!curl -v localhost:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "811a7d08-6d4b-4590-91df-be0ec4c45cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client created.\n",
      "GET /v2/health/live, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    triton_client = tritonhttpclient.InferenceServerClient(url=\"localhost:8000\", verbose=True)\n",
    "    print(\"client created.\")\n",
    "except Exception as e:\n",
    "    print(\"channel creation failed: \" + str(e))\n",
    "    \n",
    "triton_client.is_server_live()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b4af6c-014b-4644-91b4-faff1054c0a6",
   "metadata": {},
   "source": [
    "### Step.2 Check loaded backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3de34c95-9bbd-4074-a025-80de25b73656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/index, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '176'}>\n",
      "bytearray(b'[{\"name\":\"ensemble_model\",\"version\":\"1\",\"state\":\"READY\"},{\"name\":\"hps_embedding\",\"version\":\"1\",\"state\":\"READY\"},{\"name\":\"tf_reshape_dense_model\",\"version\":\"1\",\"state\":\"READY\"}]')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'ensemble_model', 'version': '1', 'state': 'READY'},\n",
       " {'name': 'hps_embedding', 'version': '1', 'state': 'READY'},\n",
       " {'name': 'tf_reshape_dense_model', 'version': '1', 'state': 'READY'}]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.get_model_repository_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39202c0e-a00f-43e4-a498-747662ff0762",
   "metadata": {},
   "source": [
    "### Step.3 Prepare mock request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4419f4a6-acef-45b1-a1e9-f85890169d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input key tensor is \n",
      "[[4 1 2]], \n",
      "number of key tensor is \n",
      "[[3]]\n"
     ]
    }
   ],
   "source": [
    "# generate mock requests based on model training settings\n",
    "batch_size = 1\n",
    "key_tensor  = np.random.randint(1,10,(batch_size,args[\"slot_num\"])).astype(np.int64)\n",
    "nkey_tensor = np.full((batch_size, 1), 3).astype(np.int32)\n",
    "print(\"Input key tensor is \\n{}, \\nnumber of key tensor is \\n{}\".format(key_tensor, nkey_tensor))\n",
    "\n",
    "inputs = [\n",
    "    httpclient.InferInput(\"EMB_KEY\", \n",
    "                          key_tensor.shape,\n",
    "                          np_to_triton_dtype(np.int64)),\n",
    "    httpclient.InferInput(\"EMB_N_KEY\", \n",
    "                          nkey_tensor.shape,\n",
    "                          np_to_triton_dtype(np.int32)),\n",
    "]\n",
    "inputs[0].set_data_from_numpy(key_tensor)\n",
    "inputs[1].set_data_from_numpy(nkey_tensor)\n",
    "\n",
    "outputs = [\n",
    "    httpclient.InferRequestedOutput(\"DENSE_OUTPUT\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc5ddd0-a12b-4d18-ba11-94ae03823eec",
   "metadata": {},
   "source": [
    "### Step.4 Send request to Triton server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ca82ebcb-37a0-49e6-85e4-2e3b04411d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction result is [[1918.4756]]\n",
      "Response details:\n",
      "{'model_name': 'ensemble_model', 'model_version': '1', 'parameters': {'sequence_id': 0, 'sequence_start': False, 'sequence_end': False}, 'outputs': [{'name': 'DENSE_OUTPUT', 'datatype': 'FP32', 'shape': [1, 1], 'parameters': {'binary_data_size': 4}}]}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ensemble_model\"\n",
    "\n",
    "with httpclient.InferenceServerClient(\"localhost:8000\") as client:\n",
    "    response = client.infer(model_name,\n",
    "                            inputs,\n",
    "                            outputs=outputs)\n",
    "    result = response.get_response()\n",
    "    \n",
    "    print(\"Prediction result is {}\".format(response.as_numpy(\"DENSE_OUTPUT\")))\n",
    "    print(\"Response details:\\n{}\".format(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
