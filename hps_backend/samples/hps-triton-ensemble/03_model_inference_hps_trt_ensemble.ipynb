{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4d877bc",
   "metadata": {},
   "source": [
    "# Overview\n",
    "The **03_model_inference_hps_trt_ensemble.ipynb** will cover following tasks\n",
    "  * Configure three backends in Triton format\n",
    "  * Deploy to inference with Triton ensemble mode\n",
    "  * Validate deployed ensemble model with dummy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf8a2d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tritonhttpclient/__init__.py:31: DeprecationWarning: The package `tritonhttpclient` is deprecated and will be removed in a future version. Please use instead `tritonclient.http`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tritonhttpclient\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021b741e",
   "metadata": {},
   "source": [
    "## Configure 3 backends in Triton format\n",
    "The 3 backends are:\n",
    "* \"hps_embedding\" backend, HPS Triton backend for embedding lookup serving\n",
    "* \"trt_naive_dnn_dense\" backend, TensorRT Triton backend for dense model serving\n",
    "* \"hps_trt_ensemble\" backend, integrates the above two backends and serves as one ensemble service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254c297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict()\n",
    "args[\"slot_num\"] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6d714b",
   "metadata": {},
   "source": [
    "### Prepare Triton Inference Server directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0bd1606",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/model_repo\"\n",
    "\n",
    "!mkdir -p $BASE_DIR/hps_embedding/1\n",
    "!mkdir -p $BASE_DIR/trt_naive_dnn_dense/1\n",
    "!mkdir -p $BASE_DIR/hps_trt_ensemble/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc4066a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/model_repo\u001b[00m\r\n",
      "├── \u001b[01;34mhps_embedding\u001b[00m\r\n",
      "│   ├── \u001b[01;34m1\u001b[00m\r\n",
      "│   │   └── \u001b[01;34mnaive_dnn_sparse.model\u001b[00m\r\n",
      "│   │       ├── emb_vector\r\n",
      "│   │       └── key\r\n",
      "│   ├── config.pbtxt\r\n",
      "│   └── hps_embedding.json\r\n",
      "├── \u001b[01;34mhps_trt_ensemble\u001b[00m\r\n",
      "│   └── \u001b[01;34m1\u001b[00m\r\n",
      "└── \u001b[01;34mtrt_naive_dnn_dense\u001b[00m\r\n",
      "    └── \u001b[01;34m1\u001b[00m\r\n",
      "\r\n",
      "7 directories, 4 files\r\n"
     ]
    }
   ],
   "source": [
    "# check created repository \n",
    "!tree /model_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26cc82",
   "metadata": {},
   "source": [
    "### Configure \"hps_embedding\" HPS backend\n",
    "For more references of HPS backend building, please check [Hierarchical Parameter Server Demo](../../samples/Hierarchical_Parameter_Server_Deployment.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f34dfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /model_repo/hps_embedding/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile $BASE_DIR/hps_embedding/config.pbtxt\n",
    "name: \"hps_embedding\"\n",
    "backend: \"hps\"\n",
    "max_batch_size:0\n",
    "input [\n",
    "  {\n",
    "    name: \"KEYS\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ -1, -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"NUMKEYS\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ -1, -1]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "version_policy: {\n",
    "        specific:{versions: 1}\n",
    "},\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind : KIND_GPU\n",
    "    gpus:[0]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208317bc",
   "metadata": {},
   "source": [
    "Generate the HPS configuration for deploying embedding tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff83d7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /model_repo/hps_embedding/hps_embedding.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile $BASE_DIR/hps_embedding/hps_embedding.json\n",
    "{\n",
    "    \"supportlonglong\": true,\n",
    "    \"models\": [{\n",
    "        \"model\": \"hps_embedding\",\n",
    "        \"sparse_files\": [\"/model_repo/hps_embedding/1/naive_dnn_sparse.model\"],\n",
    "        \"num_of_worker_buffer_in_pool\": 3,\n",
    "        \"embedding_table_names\":[\"sparse_embedding1\"],\n",
    "        \"embedding_vecsize_per_table\": [16],\n",
    "        \"maxnum_catfeature_query_per_table_per_sample\": [3],\n",
    "        \"default_value_for_each_table\": [1.0],\n",
    "        \"deployed_device_list\": [0],\n",
    "        \"max_batch_size\": 65536,\n",
    "        \"cache_refresh_percentage_per_iteration\": 0.2,\n",
    "        \"hit_rate_threshold\": 1.0,\n",
    "        \"gpucacheper\": 1.0,\n",
    "        \"gpucache\": true\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b054d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r ./naive_dnn_sparse.model /model_repo/hps_embedding/1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592b0162",
   "metadata": {},
   "source": [
    "### Configure \"trt_naive_dnn_dense\" TensorRT backend \n",
    "**Note**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff6fe8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /model_repo/trt_naive_dnn_dense/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile $BASE_DIR/trt_naive_dnn_dense/config.pbtxt\n",
    "platform: \"tensorrt_plan\"\n",
    "default_model_filename: \"naive_dnn_dense.trt\"\n",
    "backend: \"tensorrt\"\n",
    "max_batch_size: 0\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"input_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [49152]\n",
    "    reshape: { shape: [1024, 48] }\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "      name: \"fc_3\"\n",
    "      data_type: TYPE_FP32\n",
    "      dims: [-1,1]\n",
    "  }\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind: KIND_GPU\n",
    "    gpus:[0]\n",
    "\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee963508",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r ./naive_dnn_dense.trt /model_repo/trt_naive_dnn_dense/1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42707dcd",
   "metadata": {},
   "source": [
    "### Configure \"hps_trt_ensemble\" Triton backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "392792e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /model_repo/hps_trt_ensemble/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile $BASE_DIR/hps_trt_ensemble/config.pbtxt\n",
    "name: \"hps_trt_ensemble\"\n",
    "platform: \"ensemble\"\n",
    "max_batch_size: 0\n",
    "input [\n",
    "  {\n",
    "    name: \"EMB_KEY\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [-1,-1]\n",
    "  },\n",
    "  {\n",
    "    name: \"EMB_N_KEY\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [-1,-1]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"DENSE_OUTPUT\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [-1, 1]\n",
    "  }\n",
    "]\n",
    "ensemble_scheduling {\n",
    "  step [\n",
    "    {\n",
    "      model_name: \"hps_embedding\"\n",
    "      model_version: -1\n",
    "      input_map {\n",
    "        key: \"KEYS\"\n",
    "        value: \"EMB_KEY\"\n",
    "      }\n",
    "      input_map {\n",
    "        key: \"NUMKEYS\"\n",
    "        value: \"EMB_N_KEY\"\n",
    "      }\n",
    "      output_map {\n",
    "        key: \"OUTPUT0\"\n",
    "        value: \"LOOKUP_VECTORS\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      model_name: \"trt_naive_dnn_dense\"\n",
    "      model_version: -1\n",
    "      input_map {\n",
    "        key: \"input_1\"\n",
    "        value: \"LOOKUP_VECTORS\"\n",
    "      }\n",
    "      output_map {\n",
    "        key: \"fc_3\"\n",
    "        value: \"DENSE_OUTPUT\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16878a6",
   "metadata": {},
   "source": [
    "### Check the generated directory and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41c40a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/model_repo\u001b[00m\r\n",
      "├── \u001b[01;34mhps_embedding\u001b[00m\r\n",
      "│   ├── \u001b[01;34m1\u001b[00m\r\n",
      "│   │   └── \u001b[01;34mnaive_dnn_sparse.model\u001b[00m\r\n",
      "│   │       ├── emb_vector\r\n",
      "│   │       └── key\r\n",
      "│   ├── config.pbtxt\r\n",
      "│   └── hps_embedding.json\r\n",
      "├── \u001b[01;34mhps_trt_ensemble\u001b[00m\r\n",
      "│   ├── \u001b[01;34m1\u001b[00m\r\n",
      "│   └── config.pbtxt\r\n",
      "└── \u001b[01;34mtrt_naive_dnn_dense\u001b[00m\r\n",
      "    ├── \u001b[01;34m1\u001b[00m\r\n",
      "    │   └── naive_dnn_dense.trt\r\n",
      "    └── config.pbtxt\r\n",
      "\r\n",
      "7 directories, 7 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree /model_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930caea4",
   "metadata": {},
   "source": [
    "## Start Triton Inference Server, load 3 backends\n",
    "\n",
    "Now, we assume you have checked your **tritonserver** version and confirmed that can run tritonserver command inside your docker container.\n",
    "\n",
    "For this tutorial, the command to start Triton will be\n",
    "> **tritonserver --model-repository=/model_repo/ --backend-config=hps,ps=/model_repo/hps_embedding/hps_embedding.json --load-model=hps_trt_ensemble --model-control-mode=explicit**\n",
    "\n",
    "If you successfully started tritonserver, you should see a log similar to following\n",
    "\n",
    "```bash\n",
    "+----------+--------------------------------+--------------------------------+\n",
    "| Backend  | Path                           | Config                         |\n",
    "+----------+--------------------------------+--------------------------------+\n",
    "| tensorrt | /opt/tritonserver/backends/ten | {\"cmdline\":{\"auto-complete-con |\n",
    "|          | sorrt/libtriton_tensorrt.so    | fig\":\"true\",\"min-compute-capab |\n",
    "|          |                                | ility\":\"6.000000\",\"backend-dir |\n",
    "|          |                                | ectory\":\"/opt/tritonserver/bac |\n",
    "|          |                                | kends\",\"default-max-batch-size |\n",
    "|          |                                | \":\"4\"}}                        |\n",
    "|          |                                |                                |\n",
    "| hps      | /opt/tritonserver/backends/hps | {\"cmdline\":{\"auto-complete-con |\n",
    "|          | /libtriton_hps.so              | fig\":\"true\",\"backend-directory |\n",
    "|          |                                | \":\"/opt/tritonserver/backends\" |\n",
    "|          |                                | ,\"min-compute-capability\":\"6.0 |\n",
    "|          |                                | 00000\",\"ps\":\"/model_repo/hps_e |\n",
    "|          |                                | mbedding/hps_embedding.json\",\" |\n",
    "|          |                                | default-max-batch-size\":\"4\"}}  |\n",
    "|          |                                |                                |\n",
    "+----------+--------------------------------+--------------------------------+\n",
    "\n",
    "+---------------------+---------+--------+\n",
    "| Model               | Version | Status |\n",
    "+---------------------+---------+--------+\n",
    "| hps_embedding       | 1       | READY  |\n",
    "| hps_trt_ensemble    | 1       | READY  |\n",
    "| trt_naive_dnn_dense | 1       | READY  |\n",
    "+---------------------+---------+--------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a150e523",
   "metadata": {},
   "source": [
    "## Validate deployed ensemble model with dummy dataset\n",
    "### Step.1 Check Tritonserver health\n",
    "**Note**: if you are using default Tritonserver settings, the default port will be `8000` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eac9d566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 127.0.0.1:8000...\r\n",
      "* TCP_NODELAY set\r\n",
      "* Connected to localhost (127.0.0.1) port 8000 (#0)\r\n",
      "> GET /v2/health/ready HTTP/1.1\r",
      "\r\n",
      "> Host: localhost:8000\r",
      "\r\n",
      "> User-Agent: curl/7.68.0\r",
      "\r\n",
      "> Accept: */*\r",
      "\r\n",
      "> \r",
      "\r\n",
      "* Mark bundle as not supporting multiuse\r\n",
      "< HTTP/1.1 200 OK\r",
      "\r\n",
      "< Content-Length: 0\r",
      "\r\n",
      "< Content-Type: text/plain\r",
      "\r\n",
      "< \r",
      "\r\n",
      "* Connection #0 to host localhost left intact\r\n"
     ]
    }
   ],
   "source": [
    "!curl -v localhost:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d099ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client created.\n",
      "GET /v2/health/live, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    triton_client = tritonhttpclient.InferenceServerClient(url=\"localhost:8000\", verbose=True)\n",
    "    print(\"client created.\")\n",
    "except Exception as e:\n",
    "    print(\"channel creation failed: \" + str(e))\n",
    "    \n",
    "triton_client.is_server_live()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277070d1",
   "metadata": {},
   "source": [
    "### Step.2 Check loaded backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34faae0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/index, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '175'}>\n",
      "bytearray(b'[{\"name\":\"hps_embedding\",\"version\":\"1\",\"state\":\"READY\"},{\"name\":\"hps_trt_ensemble\",\"version\":\"1\",\"state\":\"READY\"},{\"name\":\"trt_naive_dnn_dense\",\"version\":\"1\",\"state\":\"READY\"}]')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'hps_embedding', 'version': '1', 'state': 'READY'},\n",
       " {'name': 'hps_trt_ensemble', 'version': '1', 'state': 'READY'},\n",
       " {'name': 'trt_naive_dnn_dense', 'version': '1', 'state': 'READY'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.get_model_repository_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4fa702",
   "metadata": {},
   "source": [
    "### Step.3 Prepare mock request\n",
    "\n",
    "**Note**: The TensorRT engine for dense network is built with the fixed batch size 1024, thus we can only send requests of this batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb491c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input key tensor is \n",
      "[[1 2 2 ... 3 9 9]], \n",
      "number of key tensor is \n",
      "[[3072]]\n"
     ]
    }
   ],
   "source": [
    "# generate mock requests based on model training settings\n",
    "batch_size = 1024\n",
    "key_tensor  = np.random.randint(1,10,(1, batch_size * args[\"slot_num\"])).astype(np.int64)\n",
    "nkey_tensor = np.full((1, 1), batch_size * 3).astype(np.int32)\n",
    "print(\"Input key tensor is \\n{}, \\nnumber of key tensor is \\n{}\".format(key_tensor, nkey_tensor))\n",
    "\n",
    "inputs = [\n",
    "    httpclient.InferInput(\"EMB_KEY\", \n",
    "                          key_tensor.shape,\n",
    "                          np_to_triton_dtype(np.int64)),\n",
    "    httpclient.InferInput(\"EMB_N_KEY\", \n",
    "                          nkey_tensor.shape,\n",
    "                          np_to_triton_dtype(np.int32)),\n",
    "]\n",
    "inputs[0].set_data_from_numpy(key_tensor)\n",
    "inputs[1].set_data_from_numpy(nkey_tensor)\n",
    "\n",
    "outputs = [\n",
    "    httpclient.InferRequestedOutput(\"DENSE_OUTPUT\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93809b8f",
   "metadata": {},
   "source": [
    "### Step.4 Send request to Triton server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d39ad473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction result is [[2932.007 ]\n",
      " [2992.7078]\n",
      " [3417.2224]\n",
      " ...\n",
      " [2994.281 ]\n",
      " [3024.6218]\n",
      " [3153.9033]]\n",
      "Response details:\n",
      "{'model_name': 'hps_trt_ensemble', 'model_version': '1', 'parameters': {'sequence_id': 0, 'sequence_start': False, 'sequence_end': False}, 'outputs': [{'name': 'DENSE_OUTPUT', 'datatype': 'FP32', 'shape': [1024, 1], 'parameters': {'binary_data_size': 4096}}]}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"hps_trt_ensemble\"\n",
    "\n",
    "with httpclient.InferenceServerClient(\"localhost:8000\") as client:\n",
    "    response = client.infer(model_name,\n",
    "                            inputs,\n",
    "                            outputs=outputs)\n",
    "    result = response.get_response()\n",
    "    \n",
    "    print(\"Prediction result is {}\".format(response.as_numpy(\"DENSE_OUTPUT\")))\n",
    "    print(\"Response details:\\n{}\".format(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
