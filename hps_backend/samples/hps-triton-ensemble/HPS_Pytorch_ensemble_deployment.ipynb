{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de3ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48596917",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24708c93",
   "metadata": {},
   "source": [
    "# 1.Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bf02b9",
   "metadata": {},
   "source": [
    "In this notebook, we want to provide a tutorial about how to use the Hierarchical Parameter Server(HPS) backend to look up the embedding keys for inference service, and combine with pytorch and TernsorRT Triton backend "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439614f2",
   "metadata": {},
   "source": [
    "1. <a href='#section1'>Overview</a> \n",
    "2. [Generate sythetic datasets to train native Pytorch DNN model and deploy the pytorch model using pytorch triton backend](#section2) \n",
    "3. [Separate the trained DNN model graph into two, embedding lookup and dense model graph](#section3)  \n",
    "    3.1 [Deploy dense part model using pytorch Triton backend](#section3.1)  \n",
    "    3.2 [Deploy the embedding part using HPS Triton Backend](#section3.2)   \n",
    "    3.3 [Configure \"ensemble_model\" Triton backend for Embedding and Dense model](#section3.3) \n",
    "4. <a href='#section4'>Use TensorRT to speed up dense model inference and combine with HPS Backend</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fb14d1",
   "metadata": {},
   "source": [
    "<a id='#section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dff752",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3cbaaf-d9f9-40ed-83b0-44b8e56bed4a",
   "metadata": {},
   "source": [
    "# 2. Train Pytorch DNN Model Based on Sythetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffdcbc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch \n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5bcc103",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict()\n",
    "# define model training settings\n",
    "args[\"gpu_num\"] = 4                               # the number of available GPUs\n",
    "args[\"num_sample\"] = 64                           # the number of training sample\n",
    "args[\"iter_num\"] = 20                             # the number of training iteration\n",
    "args[\"embed_vec_size\"] = 32                       # the dimension of embedding vectors\n",
    "args[\"global_batch_size\"] = 32                 # the globally batchsize for all GPUs\n",
    "args[\"max_vocabulary_size\"] = 1000              # the num of embeddings in embedding table\n",
    "args[\"vocabulary_range_per_slot\"] = [[0,1000]]  # the range of embedding keys in embedding table\n",
    "# define data type\n",
    "args[\"np_key_type\"]    = np.int64\n",
    "args[\"np_vector_type\"] = np.float32\n",
    "args[\"tf_key_type\"]    = torch.int64\n",
    "args[\"tf_vector_type\"] = torch.float32\n",
    "\n",
    "# GPU environment configuration for model training\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, range(args[\"gpu_num\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da11c6",
   "metadata": {},
   "source": [
    "## 2.1 Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ae1b530",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_random_samples(num_samples, vocabulary_range_per_slot, key_dtype = np.int64):\n",
    "    \"\"\"\n",
    "    Data generator\n",
    "    \n",
    "    Returns a randomly generated set of values for keys and labels\n",
    "    \"\"\"\n",
    "    keys = list()\n",
    "    for vocab_range in vocabulary_range_per_slot:\n",
    "        keys_per_slot = np.random.randint(low=vocab_range[0], \n",
    "                                          high=vocab_range[1], \n",
    "                                          size=(num_samples, 1), \n",
    "                                          dtype=key_dtype)\n",
    "        keys.append(keys_per_slot)\n",
    "    keys = np.concatenate(np.array(keys), axis = 1)\n",
    "    labels = np.random.randint(low=0, high=2, size=(num_samples, 1))\n",
    "    return keys, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee30d3ad",
   "metadata": {},
   "source": [
    "## 2.2 Define a Naive Pytorch DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71eca596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,user_num,user_dim,layer=[32,16,8]):\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        self.user_Embedding = nn.Embedding(user_num,user_dim)\n",
    "        self.mlp = nn.Sequential()\n",
    "        for id in range(1,len(layer)):\n",
    "            self.mlp.add_module(\"Linear_layer_%d\" % id, nn.Linear(layer[id-1],layer[id]))\n",
    "            self.mlp.add_module(\"Relu_layer_%d\" % id, nn.ReLU(inplace=True))\n",
    "        self.predict =  nn.Sequential(nn.Linear(layer[-1],1),nn.Sigmoid())\n",
    "    \n",
    "    def forward(self,x):\n",
    "        user = self.user_Embedding(x)\n",
    "        user = self.mlp(user)\n",
    "        score = self.predict(user)\n",
    "        return score\n",
    "\n",
    "model = MLP(1000,32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db100b94",
   "metadata": {},
   "source": [
    "## 2.3 Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20b2a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "keys, labels = generate_random_samples(args[\"num_sample\"], args[\"vocabulary_range_per_slot\"], args[\"np_key_type\"])\n",
    "x_train = torch.from_numpy(keys)\n",
    "y_train = torch.from_numpy(labels).float()\n",
    "x_dataloader = DataLoader(x_train, batch_size=args[\"global_batch_size\"], shuffle=True, num_workers=1, pin_memory=False, drop_last=False)\n",
    "y_dataloader = DataLoader(y_train, batch_size=args[\"global_batch_size\"], shuffle=True, num_workers=1, pin_memory=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d22a5bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0+cu117\n",
      "MLP(\n",
      "  (user_Embedding): Embedding(1000, 32)\n",
      "  (mlp): Sequential(\n",
      "    (Linear_layer_1): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (Relu_layer_1): ReLU(inplace=True)\n",
      "    (Linear_layer_2): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (Relu_layer_2): ReLU(inplace=True)\n",
      "  )\n",
      "  (predict): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n",
      "0 tensor(-0., grad_fn=<DivBackward1>)\n",
      "1 tensor(-0., grad_fn=<DivBackward1>)\n",
      "2 tensor(-0., grad_fn=<DivBackward1>)\n",
      "3 tensor(-0., grad_fn=<DivBackward1>)\n",
      "4 tensor(-0., grad_fn=<DivBackward1>)\n",
      "5 tensor(-0., grad_fn=<DivBackward1>)\n",
      "6 tensor(-0., grad_fn=<DivBackward1>)\n",
      "7 tensor(-0., grad_fn=<DivBackward1>)\n",
      "8 tensor(-0., grad_fn=<DivBackward1>)\n",
      "9 tensor(-0., grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(torch.__version__)\n",
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.001)\n",
    "for epoch in range(args[\"iter_num\"]):\n",
    "    iterations_per_epoch = len(x_dataloader)\n",
    "    x_iterator = iter(x_dataloader)\n",
    "    y_iterator = iter(y_dataloader)\n",
    "    for _ in range(iterations_per_epoch):\n",
    "        optimizer.zero_grad()\n",
    "        x_train = next(x_iterator)\n",
    "        y_train = next(y_iterator)\n",
    "        x, y = x_train, y_train\n",
    "        preds = model(x).squeeze(1)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(epoch, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfd41c2",
   "metadata": {},
   "source": [
    "## 2.4 Print model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9db1a6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_Embedding.weight\n",
      "user_Embedding.weight \t torch.Size([1000, 32])\n",
      "mlp.Linear_layer_1.weight\n",
      "mlp.Linear_layer_1.weight \t torch.Size([16, 32])\n",
      "mlp.Linear_layer_1.bias\n",
      "mlp.Linear_layer_1.bias \t torch.Size([16])\n",
      "mlp.Linear_layer_2.weight\n",
      "mlp.Linear_layer_2.weight \t torch.Size([8, 16])\n",
      "mlp.Linear_layer_2.bias\n",
      "mlp.Linear_layer_2.bias \t torch.Size([8])\n",
      "predict.0.weight\n",
      "predict.0.weight \t torch.Size([1, 8])\n",
      "predict.0.bias\n",
      "predict.0.bias \t torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor)\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cdf536",
   "metadata": {},
   "source": [
    "## 2.5 Save model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "badf3b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p model/torch_test/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "420081d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model=torch.jit.script(model)\n",
    "save_model.save(\"model/torch_test/0/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0030e50",
   "metadata": {},
   "source": [
    "## 2.6 Deploye the model using Pytorch Triton Backend\n",
    "Configure \"torch_test\" model with pytorch backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff7f83c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model/torch_test/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/torch_test/config.pbtxt\n",
    "name: \"torch_test\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 32\n",
    "input: [\n",
    "   {\n",
    "      name: \"user_Embedding\"\n",
    "      data_type: TYPE_INT64\n",
    "      dims: [-1]\n",
    "   }\n",
    "]\n",
    "output: [\n",
    "   {\n",
    "      name: \"prediction\"\n",
    "      data_type: TYPE_FP32\n",
    "      dims: [-1]\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7231998c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1110 12:02:36.176673 1599 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7fbdf4000000' with size 268435456\n",
      "I1110 12:02:36.181262 1599 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "I1110 12:02:36.181271 1599 cuda_memory_manager.cc:105] CUDA memory pool is created on device 1 with size 67108864\n",
      "I1110 12:02:36.181274 1599 cuda_memory_manager.cc:105] CUDA memory pool is created on device 2 with size 67108864\n",
      "I1110 12:02:36.181277 1599 cuda_memory_manager.cc:105] CUDA memory pool is created on device 3 with size 67108864\n",
      "I1110 12:02:36.551023 1599 model_repository_manager.cc:1206] loading: torch_test:0\n",
      "I1110 12:02:36.952397 1599 libtorch.cc:1917] TRITONBACKEND_Initialize: pytorch\n",
      "I1110 12:02:36.952420 1599 libtorch.cc:1927] Triton TRITONBACKEND API version: 1.10\n",
      "I1110 12:02:36.952426 1599 libtorch.cc:1933] 'pytorch' TRITONBACKEND API version: 1.10\n",
      "I1110 12:02:36.952448 1599 libtorch.cc:1966] TRITONBACKEND_ModelInitialize: torch_test (version 0)\n",
      "W1110 12:02:36.953408 1599 libtorch.cc:262] skipping model configuration auto-complete for 'torch_test': not supported for pytorch backend\n",
      "I1110 12:02:36.956058 1599 libtorch.cc:291] Optimized execution is enabled for model instance 'torch_test'\n",
      "I1110 12:02:36.956069 1599 libtorch.cc:310] Cache Cleaning is disabled for model instance 'torch_test'\n",
      "I1110 12:02:36.956072 1599 libtorch.cc:327] Inference Mode is disabled for model instance 'torch_test'\n",
      "I1110 12:02:36.956076 1599 libtorch.cc:422] NvFuser is not specified for model instance 'torch_test'\n",
      "I1110 12:02:36.958353 1599 libtorch.cc:2010] TRITONBACKEND_ModelInstanceInitialize: torch_test (GPU device 0)\n",
      "I1110 12:02:37.882305 1599 libtorch.cc:2010] TRITONBACKEND_ModelInstanceInitialize: torch_test (GPU device 1)\n",
      "I1110 12:02:38.603060 1599 libtorch.cc:2010] TRITONBACKEND_ModelInstanceInitialize: torch_test (GPU device 2)\n",
      "I1110 12:02:39.302830 1599 libtorch.cc:2010] TRITONBACKEND_ModelInstanceInitialize: torch_test (GPU device 3)\n",
      "I1110 12:02:40.022732 1599 model_repository_manager.cc:1352] successfully loaded 'torch_test' version 0\n",
      "I1110 12:02:40.022857 1599 server.cc:559] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I1110 12:02:40.022965 1599 server.cc:586] \n",
      "+---------+---------------------------------+---------------------------------+\n",
      "| Backend | Path                            | Config                          |\n",
      "+---------+---------------------------------+---------------------------------+\n",
      "| pytorch | /opt/tritonserver/backends/pyto | {\"cmdline\":{\"auto-complete-conf |\n",
      "|         | rch/libtriton_pytorch.so        | ig\":\"true\",\"min-compute-capabil |\n",
      "|         |                                 | ity\":\"6.000000\",\"backend-direct |\n",
      "|         |                                 | ory\":\"/opt/tritonserver/backend |\n",
      "|         |                                 | s\",\"default-max-batch-size\":\"4\" |\n",
      "|         |                                 | }}                              |\n",
      "|         |                                 |                                 |\n",
      "+---------+---------------------------------+---------------------------------+\n",
      "\n",
      "I1110 12:02:40.023014 1599 server.cc:629] \n",
      "+------------+---------+--------+\n",
      "| Model      | Version | Status |\n",
      "+------------+---------+--------+\n",
      "| torch_test | 0       | READY  |\n",
      "+------------+---------+--------+\n",
      "\n",
      "I1110 12:02:40.110336 1599 metrics.cc:650] Collecting metrics for GPU 0: Tesla V100-SXM2-32GB\n",
      "I1110 12:02:40.110359 1599 metrics.cc:650] Collecting metrics for GPU 1: Tesla V100-SXM2-32GB\n",
      "I1110 12:02:40.110366 1599 metrics.cc:650] Collecting metrics for GPU 2: Tesla V100-SXM2-32GB\n",
      "I1110 12:02:40.110373 1599 metrics.cc:650] Collecting metrics for GPU 3: Tesla V100-SXM2-32GB\n",
      "I1110 12:02:40.113167 1599 tritonserver.cc:2176] \n",
      "+----------------------------------+------------------------------------------+\n",
      "| Option                           | Value                                    |\n",
      "+----------------------------------+------------------------------------------+\n",
      "| server_id                        | triton                                   |\n",
      "| server_version                   | 2.24.0                                   |\n",
      "| server_extensions                | classification sequence model_repository |\n",
      "|                                  |  model_repository(unload_dependents) sch |\n",
      "|                                  | edule_policy model_configuration system_ |\n",
      "|                                  | shared_memory cuda_shared_memory binary_ |\n",
      "|                                  | tensor_data statistics trace             |\n",
      "| model_repository_path[0]         | /hugectr/hugectr_inference_backend/hps_b |\n",
      "|                                  | ackend/samples/hps-triton-ensemble/model |\n",
      "|                                  | /                                        |\n",
      "| model_control_mode               | MODE_EXPLICIT                            |\n",
      "| startup_models_0                 | torch_test                               |\n",
      "| strict_model_config              | 0                                        |\n",
      "| rate_limit                       | OFF                                      |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                 |\n",
      "| cuda_memory_pool_byte_size{1}    | 67108864                                 |\n",
      "| cuda_memory_pool_byte_size{2}    | 67108864                                 |\n",
      "| cuda_memory_pool_byte_size{3}    | 67108864                                 |\n",
      "| response_cache_byte_size         | 0                                        |\n",
      "| min_supported_compute_capability | 6.0                                      |\n",
      "| strict_readiness                 | 1                                        |\n",
      "| exit_timeout                     | 30                                       |\n",
      "+----------------------------------+------------------------------------------+\n",
      "\n",
      "I1110 12:02:40.114413 1599 grpc_server.cc:4608] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I1110 12:02:40.114748 1599 http_server.cc:3312] Started HTTPService at 0.0.0.0:8000\n",
      "I1110 12:02:40.180583 1599 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002\n",
      "^C\n",
      "Signal (2) received.\n",
      "I1110 12:02:48.687714 1599 server.cc:260] Waiting for in-flight requests to complete.\n",
      "I1110 12:02:48.687750 1599 server.cc:276] Timeout 30: Found 0 model versions that have in-flight inferences\n",
      "I1110 12:02:48.687758 1599 model_repository_manager.cc:1230] unloading: torch_test:0\n",
      "I1110 12:02:48.687854 1599 server.cc:291] All models are stopped, unloading models\n",
      "I1110 12:02:48.687865 1599 server.cc:298] Timeout 30: Found 1 live models and 0 in-flight non-inference requests\n",
      "I1110 12:02:48.688276 1599 libtorch.cc:2044] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I1110 12:02:48.689725 1599 libtorch.cc:2044] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I1110 12:02:48.690856 1599 libtorch.cc:2044] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I1110 12:02:48.691909 1599 libtorch.cc:2044] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I1110 12:02:48.692812 1599 libtorch.cc:1989] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I1110 12:02:48.693105 1599 model_repository_manager.cc:1335] successfully unloaded 'torch_test' version 0\n"
     ]
    }
   ],
   "source": [
    "# Launch the Triton Server\n",
    "!tritonserver --model-repository=/hugectr_backend/model/ --load-model=torch_test --model-control-mode=explicit  --allow-gpu-metrics=true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbe7d97",
   "metadata": {},
   "source": [
    "## 2.6 Send the inference request to Triton Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c0584a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1', 'model_name': 'torch_test', 'model_version': '0', 'outputs': [{'name': 'prediction', 'datatype': 'FP32', 'shape': [1, 2, 1], 'parameters': {'binary_data_size': 8}}]}\n",
      "Prediction Result:\n",
      "[[[0.41792953]\n",
      "  [0.43865865]]]\n"
     ]
    }
   ],
   "source": [
    "from tritonclient.utils import *\n",
    "import tritonclient.http  as httpclient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "model_name = 'torch_test'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with httpclient.InferenceServerClient(\"localhost:8000\") as client:\n",
    "    embedding_columns = np.array([[123,456]],dtype='int64')\n",
    "    \n",
    "    inputs = [\n",
    "        httpclient.InferInput(\"user_Embedding\", embedding_columns.shape,\n",
    "                              np_to_triton_dtype(embedding_columns.dtype)),\n",
    "\n",
    "    ]\n",
    "\n",
    "    inputs[0].set_data_from_numpy(embedding_columns)\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"prediction\")\n",
    "    ]\n",
    "\n",
    "    response = client.infer(model_name,\n",
    "                            inputs,\n",
    "                            request_id=str(1),\n",
    "                            outputs=outputs)\n",
    "\n",
    "    result = response.get_response()\n",
    "    print(result)\n",
    "    print(\"Prediction Result:\")\n",
    "    print(response.as_numpy(\"prediction\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38feea0",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f626acf",
   "metadata": {},
   "source": [
    "# 3 Separate the trained navie DNN model graph into  embedding and dense(MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8513ba00",
   "metadata": {},
   "source": [
    "<a id='section3.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac4b4cd",
   "metadata": {},
   "source": [
    "## 3.1 Depoly the Dense Model using Triton Backend\n",
    "### 3.1.1 Define Dense Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46044843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class OnlyMLP(nn.Module):\n",
    "    def __init__(self,user_num,user_dim,layer=[32,16,8]):\n",
    "        \n",
    "        super(OnlyMLP, self).__init__()\n",
    "        #self.user_Embedding = nn.Embedding(user_num,user_dim)\n",
    "        self.mlp = nn.Sequential()\n",
    "        self.emb_dim = user_dim\n",
    "        for id in range(1,len(layer)):\n",
    "            self.mlp.add_module(\"Linear_layer_%d\" % id, nn.Linear(layer[id-1],layer[id]))\n",
    "            self.mlp.add_module(\"Relu_layer_%d\" % id, nn.ReLU(inplace=True))\n",
    "        self.predict =  nn.Sequential(nn.Linear(layer[-1],1),nn.Sigmoid())\n",
    "    \n",
    "    def forward(self,x):\n",
    "        #user = self.user_Embedding(x)\n",
    "        user = x.reshape(-1,self.emb_dim)\n",
    "        user = self.mlp(user)\n",
    "        score = self.predict(user)\n",
    "        return score\n",
    "\n",
    "dense_model = OnlyMLP(1000,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5494a1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp.Linear_layer_1.weight\n",
      "mlp.Linear_layer_1.weight \t torch.Size([16, 32])\n",
      "mlp.Linear_layer_1.bias\n",
      "mlp.Linear_layer_1.bias \t torch.Size([16])\n",
      "mlp.Linear_layer_2.weight\n",
      "mlp.Linear_layer_2.weight \t torch.Size([8, 16])\n",
      "mlp.Linear_layer_2.bias\n",
      "mlp.Linear_layer_2.bias \t torch.Size([8])\n",
      "predict.0.weight\n",
      "predict.0.weight \t torch.Size([1, 8])\n",
      "predict.0.bias\n",
      "predict.0.bias \t torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Print dense model layers\n",
    "for param_tensor in dense_model.state_dict():\n",
    "    print(param_tensor)\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1187741d",
   "metadata": {},
   "source": [
    "### 3.1.2 Load complete pre-trained navie pytorch model( Step 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0643542c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/serialization.py:779: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    }
   ],
   "source": [
    "pretrain_model = torch.load(\"model/torch_test/0/model.pt\")\n",
    "# get pre-trained model state dict\n",
    "pretrain_dict=pretrain_model.state_dict()\n",
    "# get dense model state dict\n",
    "new_dict=  dense_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09be21bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_Embedding.weight\n",
      "torch.Size([1000, 32])\n",
      "mlp.Linear_layer_1.weight\n",
      "torch.Size([16, 32])\n",
      "mlp.Linear_layer_1.bias\n",
      "torch.Size([16])\n",
      "mlp.Linear_layer_2.weight\n",
      "torch.Size([8, 16])\n",
      "mlp.Linear_layer_2.bias\n",
      "torch.Size([8])\n",
      "predict.0.weight\n",
      "torch.Size([1, 8])\n",
      "predict.0.bias\n",
      "torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/serialization.py:779: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    }
   ],
   "source": [
    "pretrain_model = torch.load(\"model/torch_test/0/model.pt\")\n",
    "pretrain_dict=pretrain_model.state_dict()\n",
    "for param_tensor in pretrain_model.state_dict():\n",
    "    print(param_tensor)\n",
    "    print( model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cbff31",
   "metadata": {},
   "source": [
    "##3.3 Remove the embedding layer from pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2745fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_dict = {k:v for k,v in pretrain_dict.items() if k in new_dict}\n",
    "new_dict.update(pretrain_dict)\n",
    "#update dense model\n",
    "dense_model.load_state_dict(new_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a94962d",
   "metadata": {},
   "source": [
    "### 3.1.3 Save the dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "39e473f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p model/dense_test/0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "425ed9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model=torch.jit.script(dense_model)\n",
    "save_model.save(\"model/dense_test/0/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be7bb5",
   "metadata": {},
   "source": [
    "### 3.1.4 Deploye the dense model using Pytorch Triton Backend\n",
    "Configure \"dense_test\" model with pytorch backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74a317df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model/dense_test/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/dense_test/config.pbtxt\n",
    "name: \"dense_test\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 0\n",
    "input: [\n",
    "   {\n",
    "      name: \"mlp.Linear_layer_1\"\n",
    "      data_type: TYPE_FP32\n",
    "      dims: [-1]\n",
    "   }\n",
    "]\n",
    "output: [\n",
    "   {\n",
    "      name: \"prediction\"\n",
    "      data_type: TYPE_FP32\n",
    "      dims: [-1]\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6a6a0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1110 12:09:30.142266 2190 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7f8958000000' with size 268435456\n",
      "I1110 12:09:30.146874 2190 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "I1110 12:09:30.146885 2190 cuda_memory_manager.cc:105] CUDA memory pool is created on device 1 with size 67108864\n",
      "I1110 12:09:30.146888 2190 cuda_memory_manager.cc:105] CUDA memory pool is created on device 2 with size 67108864\n",
      "I1110 12:09:30.146891 2190 cuda_memory_manager.cc:105] CUDA memory pool is created on device 3 with size 67108864\n",
      "I1110 12:09:30.528751 2190 model_repository_manager.cc:1206] loading: dense_test:0\n",
      "I1110 12:09:30.916909 2190 libtorch.cc:1917] TRITONBACKEND_Initialize: pytorch\n",
      "I1110 12:09:30.916938 2190 libtorch.cc:1927] Triton TRITONBACKEND API version: 1.10\n",
      "I1110 12:09:30.916961 2190 libtorch.cc:1933] 'pytorch' TRITONBACKEND API version: 1.10\n",
      "I1110 12:09:30.916984 2190 libtorch.cc:1966] TRITONBACKEND_ModelInitialize: dense_test (version 0)\n",
      "W1110 12:09:30.917972 2190 libtorch.cc:262] skipping model configuration auto-complete for 'dense_test': not supported for pytorch backend\n",
      "I1110 12:09:30.920598 2190 libtorch.cc:291] Optimized execution is enabled for model instance 'dense_test'\n",
      "I1110 12:09:30.920609 2190 libtorch.cc:310] Cache Cleaning is disabled for model instance 'dense_test'\n",
      "I1110 12:09:30.920613 2190 libtorch.cc:327] Inference Mode is disabled for model instance 'dense_test'\n",
      "I1110 12:09:30.920634 2190 libtorch.cc:422] NvFuser is not specified for model instance 'dense_test'\n",
      "I1110 12:09:30.922918 2190 libtorch.cc:2010] TRITONBACKEND_ModelInstanceInitialize: dense_test (GPU device 0)\n",
      "I1110 12:09:31.789084 2190 libtorch.cc:2010] TRITONBACKEND_ModelInstanceInitialize: dense_test (GPU device 1)\n",
      "I1110 12:09:32.509519 2190 libtorch.cc:2010] TRITONBACKEND_ModelInstanceInitialize: dense_test (GPU device 2)\n",
      "I1110 12:09:33.167404 2190 libtorch.cc:2010] TRITONBACKEND_ModelInstanceInitialize: dense_test (GPU device 3)\n",
      "I1110 12:09:33.822859 2190 model_repository_manager.cc:1352] successfully loaded 'dense_test' version 0\n",
      "I1110 12:09:33.823085 2190 server.cc:559] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I1110 12:09:33.823301 2190 server.cc:586] \n",
      "+---------+---------------------------------+---------------------------------+\n",
      "| Backend | Path                            | Config                          |\n",
      "+---------+---------------------------------+---------------------------------+\n",
      "| pytorch | /opt/tritonserver/backends/pyto | {\"cmdline\":{\"auto-complete-conf |\n",
      "|         | rch/libtriton_pytorch.so        | ig\":\"true\",\"min-compute-capabil |\n",
      "|         |                                 | ity\":\"6.000000\",\"backend-direct |\n",
      "|         |                                 | ory\":\"/opt/tritonserver/backend |\n",
      "|         |                                 | s\",\"default-max-batch-size\":\"4\" |\n",
      "|         |                                 | }}                              |\n",
      "|         |                                 |                                 |\n",
      "+---------+---------------------------------+---------------------------------+\n",
      "\n",
      "I1110 12:09:33.823388 2190 server.cc:629] \n",
      "+------------+---------+--------+\n",
      "| Model      | Version | Status |\n",
      "+------------+---------+--------+\n",
      "| dense_test | 0       | READY  |\n",
      "+------------+---------+--------+\n",
      "\n",
      "I1110 12:09:33.905127 2190 metrics.cc:650] Collecting metrics for GPU 0: Tesla V100-SXM2-32GB\n",
      "I1110 12:09:33.905152 2190 metrics.cc:650] Collecting metrics for GPU 1: Tesla V100-SXM2-32GB\n",
      "I1110 12:09:33.905160 2190 metrics.cc:650] Collecting metrics for GPU 2: Tesla V100-SXM2-32GB\n",
      "I1110 12:09:33.905167 2190 metrics.cc:650] Collecting metrics for GPU 3: Tesla V100-SXM2-32GB\n",
      "I1110 12:09:33.907802 2190 tritonserver.cc:2176] \n",
      "+----------------------------------+------------------------------------------+\n",
      "| Option                           | Value                                    |\n",
      "+----------------------------------+------------------------------------------+\n",
      "| server_id                        | triton                                   |\n",
      "| server_version                   | 2.24.0                                   |\n",
      "| server_extensions                | classification sequence model_repository |\n",
      "|                                  |  model_repository(unload_dependents) sch |\n",
      "|                                  | edule_policy model_configuration system_ |\n",
      "|                                  | shared_memory cuda_shared_memory binary_ |\n",
      "|                                  | tensor_data statistics trace             |\n",
      "| model_repository_path[0]         | /hugectr/hugectr_inference_backend/hps_b |\n",
      "|                                  | ackend/samples/hps-triton-ensemble/model |\n",
      "|                                  | /                                        |\n",
      "| model_control_mode               | MODE_EXPLICIT                            |\n",
      "| startup_models_0                 | dense_test                               |\n",
      "| strict_model_config              | 0                                        |\n",
      "| rate_limit                       | OFF                                      |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                 |\n",
      "| cuda_memory_pool_byte_size{1}    | 67108864                                 |\n",
      "| cuda_memory_pool_byte_size{2}    | 67108864                                 |\n",
      "| cuda_memory_pool_byte_size{3}    | 67108864                                 |\n",
      "| response_cache_byte_size         | 0                                        |\n",
      "| min_supported_compute_capability | 6.0                                      |\n",
      "| strict_readiness                 | 1                                        |\n",
      "| exit_timeout                     | 30                                       |\n",
      "+----------------------------------+------------------------------------------+\n",
      "\n",
      "I1110 12:09:33.909444 2190 grpc_server.cc:4608] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I1110 12:09:33.909732 2190 http_server.cc:3312] Started HTTPService at 0.0.0.0:8000\n",
      "I1110 12:09:33.964637 2190 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002\n",
      "^C\n",
      "Signal (2) received.\n",
      "I1110 12:09:41.788400 2190 server.cc:260] Waiting for in-flight requests to complete.\n",
      "I1110 12:09:41.788432 2190 server.cc:276] Timeout 30: Found 0 model versions that have in-flight inferences\n",
      "I1110 12:09:41.788448 2190 model_repository_manager.cc:1230] unloading: dense_test:0\n",
      "I1110 12:09:41.788647 2190 server.cc:291] All models are stopped, unloading models\n",
      "I1110 12:09:41.788661 2190 server.cc:298] Timeout 30: Found 1 live models and 0 in-flight non-inference requests\n",
      "I1110 12:09:41.788976 2190 libtorch.cc:2044] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I1110 12:09:41.790534 2190 libtorch.cc:2044] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I1110 12:09:41.791871 2190 libtorch.cc:2044] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I1110 12:09:41.792987 2190 libtorch.cc:2044] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I1110 12:09:41.794101 2190 libtorch.cc:1989] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I1110 12:09:41.794474 2190 model_repository_manager.cc:1335] successfully unloaded 'dense_test' version 0\n"
     ]
    }
   ],
   "source": [
    "!tritonserver --model-repository=/hugectr_backend/model/ --load-model=dense_test --model-control-mode=explicit --allow-gpu-metrics=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "35b09e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1', 'model_name': 'dense_test', 'model_version': '0', 'outputs': [{'name': 'prediction', 'datatype': 'FP32', 'shape': [1, 1], 'parameters': {'binary_data_size': 4}}]}\n",
      "Prediction Result:\n",
      "[[0.43013266]]\n"
     ]
    }
   ],
   "source": [
    "#send the inference request to dense model\n",
    "from tritonclient.utils import *\n",
    "import tritonclient.http  as httpclient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "model_name = 'dense_test'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with httpclient.InferenceServerClient(\"localhost:8000\") as client:\n",
    "    embedding_columns = np.array([[np.random.uniform(0.0,1) for i in range(32)]],dtype='float32')\n",
    "    \n",
    "    inputs = [\n",
    "        httpclient.InferInput(\"mlp.Linear_layer_1\", embedding_columns.shape,\n",
    "                              np_to_triton_dtype(embedding_columns.dtype)),\n",
    "\n",
    "    ]\n",
    "\n",
    "    inputs[0].set_data_from_numpy(embedding_columns)\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"prediction\")\n",
    "    ]\n",
    "\n",
    "    response = client.infer(model_name,\n",
    "                            inputs,\n",
    "                            request_id=str(1),\n",
    "                            outputs=outputs)\n",
    "\n",
    "    result = response.get_response()\n",
    "    print(result)\n",
    "    print(\"Prediction Result:\")\n",
    "    print(response.as_numpy(\"prediction\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79365d89",
   "metadata": {},
   "source": [
    "<a id='section3.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be241fc",
   "metadata": {},
   "source": [
    "## 3.2 Deploy the embedding part using HPS Triton Backend\n",
    "### 3.2.1 Configure HPS backend for Embedding part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6bfa0432",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p model/hps_test/0/hps_sparse.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0cd522e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model/hps_test/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/hps_test/config.pbtxt\n",
    "name: \"hps_test\"\n",
    "backend: \"hps\"\n",
    "max_batch_size:32,\n",
    "input [\n",
    "  {\n",
    "    name: \"KEYS\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [-1]\n",
    "  },\n",
    "  {\n",
    "    name: \"NUMKEYS\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [-1]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "version_policy: {\n",
    "        specific:{versions: 0}\n",
    "},\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind : KIND_GPU\n",
    "    gpus:[0]\n",
    "  }\n",
    "]\n",
    "\n",
    "parameters [\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1008a30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model/hps.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/hps.json\n",
    "{\n",
    "    \"models\": [{\n",
    "    \"model\": \"hps_test\",\n",
    "    \"sparse_files\": [\"model/hps_test/0/hps_sparse.model\"],\n",
    "    \"num_of_worker_buffer_in_pool\": 3,\n",
    "    \"embedding_table_names\":[\"0\"],\n",
    "    \"num_of_refresher_buffer_in_pool\":1,\n",
    "    \"embedding_vecsize_per_table\":[32],\n",
    "    \"num_of_refresher_buffer_in_pool\":0,\n",
    "    \"maxnum_catfeature_query_per_table_per_sample\":[1],\n",
    "    \"deployed_device_list\":[0],\n",
    "    \"max_batch_size\":32,\n",
    "    \"default_value_for_each_table\":[0.0],\n",
    "    \"cache_refresh_percentage_per_iteration\":0,\n",
    "    \"hit_rate_threshold\":1.1,\n",
    "    \"gpucacheper\":1.0,\n",
    "    \"gpucache\":true\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04173872",
   "metadata": {},
   "source": [
    "### 3.2.2 Conver the torch-format embedding file to HPS-format embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b704f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['user_Embedding.weight', 'mlp.Linear_layer_1.weight', 'mlp.Linear_layer_1.bias', 'mlp.Linear_layer_2.weight', 'mlp.Linear_layer_2.bias', 'predict.0.weight', 'predict.0.bias'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_model = torch.load(\"model/torch_test/0/model.pt\")\n",
    "pretrain_model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20c847c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sparse_model(embeddings_weights, embedding_table_path, embedding_vec_size):\n",
    "    \"\"\"\n",
    "    Convert the lookup part of the model to a format supported by HPS (key-vector pair files),\n",
    "    the embedding weights of the trained dense model will be reloaded.\n",
    "    \n",
    "    Outputs(key-vector pair files) will be saved to defined sparse model path\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(\"{}/key\".format(embedding_table_path), 'wb') as key_file, \\\n",
    "        open(\"{}/emb_vector\".format(embedding_table_path), 'wb') as vec_file:\n",
    "        for key in range(embeddings_weights.shape[0]):\n",
    "            vec = embeddings_weights[key].data.tolist()\n",
    "            key_struct = struct.pack('q', key)\n",
    "            vec_struct = struct.pack(str(embedding_vec_size) + \"f\", *vec)\n",
    "            key_file.write(key_struct)\n",
    "            vec_file.write(vec_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "11ddeb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_sparse_model(pretrain_model.state_dict()['user_Embedding.weight'], \"model/hps_test/0/hps_sparse.model\", 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f23cae",
   "metadata": {},
   "source": [
    "### 3.2.3 Launch Triton Server to verify HPS Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8850f6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1110 12:12:50.076774 2411 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7f1b28000000' with size 268435456\n",
      "I1110 12:12:50.081315 2411 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "I1110 12:12:50.081324 2411 cuda_memory_manager.cc:105] CUDA memory pool is created on device 1 with size 67108864\n",
      "I1110 12:12:50.081327 2411 cuda_memory_manager.cc:105] CUDA memory pool is created on device 2 with size 67108864\n",
      "I1110 12:12:50.081330 2411 cuda_memory_manager.cc:105] CUDA memory pool is created on device 3 with size 67108864\n",
      "I1110 12:12:50.456462 2411 model_repository_manager.cc:1206] loading: hps_test:0\n",
      "I1110 12:12:50.474949 2411 hps.cc:61] TRITONBACKEND_Initialize: hps\n",
      "I1110 12:12:50.474996 2411 hps.cc:68] Triton TRITONBACKEND API version: 1.10\n",
      "I1110 12:12:50.475005 2411 hps.cc:72] 'hps' TRITONBACKEND API version: 1.9\n",
      "I1110 12:12:50.475035 2411 hps.cc:95] The Hierarchical Parameter Server Backend Repository location: /usr/local/hugectr/backends/hps\n",
      "I1110 12:12:50.475063 2411 hps.cc:106] The HPS configuration: {\"cmdline\":{\"auto-complete-config\":\"true\",\"backend-directory\":\"/usr/local/hugectr/backends\",\"min-compute-capability\":\"6.000000\",\"ps\":\"/hugectr/hugectr_inference_backend/hps_backend/samples/hps-triton-ensemble/model/hps.json\",\"default-max-batch-size\":\"4\"}}\n",
      "I1110 12:12:50.475095 2411 backend.cpp:62] *****The Hierarchical Parameter Server is creating... *****\n",
      "I1110 12:12:50.475100 2411 backend.cpp:65] ***** Hierarchical Parameter Server(Int64) is creating... *****\n",
      "=====================================================HPS Parse====================================================\n",
      "[HCTR][12:12:50.475][INFO][RK0][main]: allocation_rate is not specified using default: 268435456\n",
      "[HCTR][12:12:50.475][INFO][RK0][main]: shared_memory_size is not specified using default: 17179869184\n",
      "[HCTR][12:12:50.475][INFO][RK0][main]: shared_memory_name is not specified using default: hctr_mp_hash_map_database\n",
      "[HCTR][12:12:50.475][INFO][RK0][main]: shared_memory_size is not specified using default: 17179869184\n",
      "[HCTR][12:12:50.475][INFO][RK0][main]: shared_memory_name is not specified using default: hctr_mp_hash_map_database\n",
      "[HCTR][12:12:50.475][INFO][RK0][main]: num_node_connections is not specified using default: 5\n",
      "[HCTR][12:12:50.475][INFO][RK0][main]: refresh_time_after_fetch is not specified using default: 0\n",
      "[HCTR][12:12:50.475][INFO][RK0][main]: cache_missed_embeddings is not specified using default: 0\n",
      "[HCTR][12:12:50.475][INFO][RK0][main]: dense_file is not specified using default: \n",
      "[HCTR][12:12:50.475][INFO][RK0][main]: maxnum_des_feature_per_sample is not specified using default: 26\n",
      "[HCTR][12:12:50.475][INFO][RK0][main]: refresh_delay is not specified using default: 0\n",
      "[HCTR][12:12:50.475][INFO][RK0][main]: refresh_interval is not specified using default: 0\n",
      "====================================================HPS Create====================================================\n",
      "[HCTR][12:12:50.475][INFO][RK0][main]: Creating HashMap CPU database backend...\n",
      "[HCTR][12:12:50.475][DEBUG][RK0][main]: Created blank database backend in local memory!\n",
      "[HCTR][12:12:50.475][INFO][RK0][main]: Volatile DB: initial cache rate = 1\n",
      "[HCTR][12:12:50.475][INFO][RK0][main]: Volatile DB: cache missed embeddings = 0\n",
      "[HCTR][12:12:50.475][DEBUG][RK0][main]: Created raw model loader in local memory!\n",
      "[HCTR][12:12:50.475][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][12:12:50.652][INFO][RK0][main]: Table: hps_et.hps_test.0; cached 1000 / 1000 embeddings in volatile database (HashMapBackend); load: 1000 / 80000000 (0.00%).\n",
      "[HCTR][12:12:50.652][DEBUG][RK0][main]: Real-time subscribers created!\n",
      "[HCTR][12:12:50.652][INFO][RK0][main]: Creating embedding cache in device 0.\n",
      "[HCTR][12:12:50.658][INFO][RK0][main]: Model name: hps_test\n",
      "[HCTR][12:12:50.658][INFO][RK0][main]: Number of embedding tables: 1\n",
      "[HCTR][12:12:50.658][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 1.000000\n",
      "[HCTR][12:12:50.658][INFO][RK0][main]: Use I64 input key: True\n",
      "[HCTR][12:12:50.658][INFO][RK0][main]: Configured cache hit rate threshold: 0.900000\n",
      "[HCTR][12:12:50.658][INFO][RK0][main]: The size of thread pool: 80\n",
      "[HCTR][12:12:50.658][INFO][RK0][main]: The size of worker memory pool: 3\n",
      "[HCTR][12:12:50.658][INFO][RK0][main]: The size of refresh memory pool: 0\n",
      "[HCTR][12:12:50.658][INFO][RK0][main]: The refresh percentage : 0.000000\n",
      "I1110 12:12:50.670002 2411 backend.cpp:73] *****The Hierarchaical Prameter Server has been created successfully! *****\n",
      "I1110 12:12:50.670025 2411 hps.cc:169] TRITONBACKEND_ModelInitialize: hps_test (version 0)\n",
      "I1110 12:12:50.670029 2411 hps.cc:182] Repository location: /hugectr/hugectr_inference_backend/hps_backend/samples/hps-triton-ensemble/model/hps_test\n",
      "I1110 12:12:50.670033 2411 hps.cc:197] backend configuration in mode: {\"cmdline\":{\"auto-complete-config\":\"true\",\"backend-directory\":\"/usr/local/hugectr/backends\",\"min-compute-capability\":\"6.000000\",\"ps\":\"/hugectr/hugectr_inference_backend/hps_backend/samples/hps-triton-ensemble/model/hps.json\",\"default-max-batch-size\":\"4\"}}\n",
      "I1110 12:12:50.671099 2411 model_state.cpp:129] Verifying model configuration: {\n",
      "    \"name\": \"hps_test\",\n",
      "    \"platform\": \"\",\n",
      "    \"backend\": \"hps\",\n",
      "    \"version_policy\": {\n",
      "        \"specific\": {\n",
      "            \"versions\": [\n",
      "                0\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"max_batch_size\": 32,\n",
      "    \"input\": [\n",
      "        {\n",
      "            \"name\": \"KEYS\",\n",
      "            \"data_type\": \"TYPE_INT64\",\n",
      "            \"format\": \"FORMAT_NONE\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"is_shape_tensor\": false,\n",
      "            \"allow_ragged_batch\": false,\n",
      "            \"optional\": false\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"NUMKEYS\",\n",
      "            \"data_type\": \"TYPE_INT32\",\n",
      "            \"format\": \"FORMAT_NONE\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"is_shape_tensor\": false,\n",
      "            \"allow_ragged_batch\": false,\n",
      "            \"optional\": false\n",
      "        }\n",
      "    ],\n",
      "    \"output\": [\n",
      "        {\n",
      "            \"name\": \"OUTPUT0\",\n",
      "            \"data_type\": \"TYPE_FP32\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"label_filename\": \"\",\n",
      "            \"is_shape_tensor\": false\n",
      "        }\n",
      "    ],\n",
      "    \"batch_input\": [],\n",
      "    \"batch_output\": [],\n",
      "    \"optimization\": {\n",
      "        \"priority\": \"PRIORITY_DEFAULT\",\n",
      "        \"input_pinned_memory\": {\n",
      "            \"enable\": true\n",
      "        },\n",
      "        \"output_pinned_memory\": {\n",
      "            \"enable\": true\n",
      "        },\n",
      "        \"gather_kernel_buffer_threshold\": 0,\n",
      "        \"eager_batching\": false\n",
      "    },\n",
      "    \"instance_group\": [\n",
      "        {\n",
      "            \"name\": \"hps_test_0\",\n",
      "            \"kind\": \"KIND_GPU\",\n",
      "            \"count\": 1,\n",
      "            \"gpus\": [\n",
      "                0\n",
      "            ],\n",
      "            \"secondary_devices\": [],\n",
      "            \"profile\": [],\n",
      "            \"passive\": false,\n",
      "            \"host_policy\": \"\"\n",
      "        }\n",
      "    ],\n",
      "    \"default_model_filename\": \"\",\n",
      "    \"cc_model_filenames\": {},\n",
      "    \"metric_tags\": {},\n",
      "    \"parameters\": {},\n",
      "    \"model_warmup\": []\n",
      "}\n",
      "I1110 12:12:50.671142 2411 model_state.cpp:210] The model configuration: {\n",
      "    \"name\": \"hps_test\",\n",
      "    \"platform\": \"\",\n",
      "    \"backend\": \"hps\",\n",
      "    \"version_policy\": {\n",
      "        \"specific\": {\n",
      "            \"versions\": [\n",
      "                0\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"max_batch_size\": 32,\n",
      "    \"input\": [\n",
      "        {\n",
      "            \"name\": \"KEYS\",\n",
      "            \"data_type\": \"TYPE_INT64\",\n",
      "            \"format\": \"FORMAT_NONE\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"is_shape_tensor\": false,\n",
      "            \"allow_ragged_batch\": false,\n",
      "            \"optional\": false\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"NUMKEYS\",\n",
      "            \"data_type\": \"TYPE_INT32\",\n",
      "            \"format\": \"FORMAT_NONE\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"is_shape_tensor\": false,\n",
      "            \"allow_ragged_batch\": false,\n",
      "            \"optional\": false\n",
      "        }\n",
      "    ],\n",
      "    \"output\": [\n",
      "        {\n",
      "            \"name\": \"OUTPUT0\",\n",
      "            \"data_type\": \"TYPE_FP32\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"label_filename\": \"\",\n",
      "            \"is_shape_tensor\": false\n",
      "        }\n",
      "    ],\n",
      "    \"batch_input\": [],\n",
      "    \"batch_output\": [],\n",
      "    \"optimization\": {\n",
      "        \"priority\": \"PRIORITY_DEFAULT\",\n",
      "        \"input_pinned_memory\": {\n",
      "            \"enable\": true\n",
      "        },\n",
      "        \"output_pinned_memory\": {\n",
      "            \"enable\": true\n",
      "        },\n",
      "        \"gather_kernel_buffer_threshold\": 0,\n",
      "        \"eager_batching\": false\n",
      "    },\n",
      "    \"instance_group\": [\n",
      "        {\n",
      "            \"name\": \"hps_test_0\",\n",
      "            \"kind\": \"KIND_GPU\",\n",
      "            \"count\": 1,\n",
      "            \"gpus\": [\n",
      "                0\n",
      "            ],\n",
      "            \"secondary_devices\": [],\n",
      "            \"profile\": [],\n",
      "            \"passive\": false,\n",
      "            \"host_policy\": \"\"\n",
      "        }\n",
      "    ],\n",
      "    \"default_model_filename\": \"\",\n",
      "    \"cc_model_filenames\": {},\n",
      "    \"metric_tags\": {},\n",
      "    \"parameters\": {},\n",
      "    \"model_warmup\": []\n",
      "}\n",
      "I1110 12:12:50.671241 2411 model_state.cpp:280] max_batch_size in model config.pbtxt is 32\n",
      "I1110 12:12:50.671248 2411 model_state.cpp:319] ******Creating Embedding Cache for model hps_test in device 0\n",
      "I1110 12:12:50.671254 2411 model_state.cpp:329] ******Creating Embedding Cache for model hps_test successfully\n",
      "I1110 12:12:50.673563 2411 hps.cc:307] TRITONBACKEND_ModelInstanceInitialize: hps_test_0 (device 0)\n",
      "I1110 12:12:50.673573 2411 model_instance_state.cpp:81] Triton Model Instance Initialization on device 0\n",
      "I1110 12:12:50.673595 2411 model_instance_state.cpp:91] Categorical Feature buffer allocation: \n",
      "I1110 12:12:50.673633 2411 model_instance_state.cpp:99] Number of Categorical Feature per Table buffer allocation: \n",
      "I1110 12:12:50.673658 2411 model_instance_state.cpp:109] Look_up result buffer allocation: \n",
      "I1110 12:12:50.673678 2411 hps.cc:320] ******Loading HPS ******\n",
      "I1110 12:12:50.673684 2411 model_instance_state.cpp:140] The model origin json configuration file path is: \n",
      "[HCTR][12:12:50.673][INFO][RK0][main]: Creating lookup session for hps_test on device: 0\n",
      "I1110 12:12:50.673704 2411 model_instance_state.cpp:147] ******Loading HugeCTR lookup session successfully\n",
      "I1110 12:12:50.673861 2411 model_repository_manager.cc:1352] successfully loaded 'hps_test' version 0\n",
      "I1110 12:12:50.674029 2411 server.cc:559] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I1110 12:12:50.674146 2411 server.cc:586] \n",
      "+---------+---------------------------------+---------------------------------+\n",
      "| Backend | Path                            | Config                          |\n",
      "+---------+---------------------------------+---------------------------------+\n",
      "| hps     | /usr/local/hugectr/backends/hps | {\"cmdline\":{\"auto-complete-conf |\n",
      "|         | /libtriton_hps.so               | ig\":\"true\",\"backend-directory\": |\n",
      "|         |                                 | \"/usr/local/hugectr/backends\",\" |\n",
      "|         |                                 | min-compute-capability\":\"6.0000 |\n",
      "|         |                                 | 00\",\"ps\":\"/hugectr/hugectr_infe |\n",
      "|         |                                 | rence_backend/hps_backend/sampl |\n",
      "|         |                                 | es/hps-triton-ensemble/model/hp |\n",
      "|         |                                 | s.json\",\"default-max-batch-size |\n",
      "|         |                                 | \":\"4\"}}                         |\n",
      "|         |                                 |                                 |\n",
      "+---------+---------------------------------+---------------------------------+\n",
      "\n",
      "I1110 12:12:50.674242 2411 server.cc:629] \n",
      "+----------+---------+--------+\n",
      "| Model    | Version | Status |\n",
      "+----------+---------+--------+\n",
      "| hps_test | 0       | READY  |\n",
      "+----------+---------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1110 12:12:50.756500 2411 metrics.cc:650] Collecting metrics for GPU 0: Tesla V100-SXM2-32GB\n",
      "I1110 12:12:50.756523 2411 metrics.cc:650] Collecting metrics for GPU 1: Tesla V100-SXM2-32GB\n",
      "I1110 12:12:50.756530 2411 metrics.cc:650] Collecting metrics for GPU 2: Tesla V100-SXM2-32GB\n",
      "I1110 12:12:50.756538 2411 metrics.cc:650] Collecting metrics for GPU 3: Tesla V100-SXM2-32GB\n",
      "I1110 12:12:50.759481 2411 tritonserver.cc:2176] \n",
      "+----------------------------------+------------------------------------------+\n",
      "| Option                           | Value                                    |\n",
      "+----------------------------------+------------------------------------------+\n",
      "| server_id                        | triton                                   |\n",
      "| server_version                   | 2.24.0                                   |\n",
      "| server_extensions                | classification sequence model_repository |\n",
      "|                                  |  model_repository(unload_dependents) sch |\n",
      "|                                  | edule_policy model_configuration system_ |\n",
      "|                                  | shared_memory cuda_shared_memory binary_ |\n",
      "|                                  | tensor_data statistics trace             |\n",
      "| model_repository_path[0]         | /hugectr/hugectr_inference_backend/hps_b |\n",
      "|                                  | ackend/samples/hps-triton-ensemble/model |\n",
      "|                                  | /                                        |\n",
      "| model_control_mode               | MODE_EXPLICIT                            |\n",
      "| startup_models_0                 | hps_test                                 |\n",
      "| strict_model_config              | 0                                        |\n",
      "| rate_limit                       | OFF                                      |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                 |\n",
      "| cuda_memory_pool_byte_size{1}    | 67108864                                 |\n",
      "| cuda_memory_pool_byte_size{2}    | 67108864                                 |\n",
      "| cuda_memory_pool_byte_size{3}    | 67108864                                 |\n",
      "| response_cache_byte_size         | 0                                        |\n",
      "| min_supported_compute_capability | 6.0                                      |\n",
      "| strict_readiness                 | 1                                        |\n",
      "| exit_timeout                     | 30                                       |\n",
      "+----------------------------------+------------------------------------------+\n",
      "\n",
      "I1110 12:12:50.760694 2411 grpc_server.cc:4608] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I1110 12:12:50.761047 2411 http_server.cc:3312] Started HTTPService at 0.0.0.0:8000\n",
      "I1110 12:12:50.828700 2411 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002\n",
      "^C\n",
      "Signal (2) received.\n",
      "I1110 12:12:56.773158 2411 server.cc:260] Waiting for in-flight requests to complete.\n",
      "I1110 12:12:56.773191 2411 server.cc:276] Timeout 30: Found 0 model versions that have in-flight inferences\n",
      "I1110 12:12:56.773203 2411 model_repository_manager.cc:1230] unloading: hps_test:0\n",
      "I1110 12:12:56.773318 2411 server.cc:291] All models are stopped, unloading models\n",
      "I1110 12:12:56.773328 2411 server.cc:298] Timeout 30: Found 1 live models and 0 in-flight non-inference requests\n",
      "I1110 12:12:56.773621 2411 hps.cc:337] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I1110 12:12:56.773835 2411 hps.cc:268] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I1110 12:12:56.774472 2411 model_state.cpp:112] ******Destorying Embedding Cache for model hps_test successfully\n",
      "I1110 12:12:56.774511 2411 model_repository_manager.cc:1335] successfully unloaded 'hps_test' version 0\n"
     ]
    }
   ],
   "source": [
    "!tritonserver --model-repository=/hugectr_backend/model/ --load-model=hps_test --model-control-mode=explicit --backend-directory=/usr/local/hugectr/backends --backend-config=hps,ps=/hugectr_backend/model/hps.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8cbb2147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1', 'model_name': 'hps_test', 'model_version': '0', 'parameters': {'NumSample': 32, 'DeviceID': 0}, 'outputs': [{'name': 'OUTPUT0', 'datatype': 'FP32', 'shape': [1024], 'parameters': {'binary_data_size': 4096}}]}\n",
      "Prediction Result:\n",
      "[ 0.42441678 -3.7125523   1.2965899  ...  1.5860947  -0.10153087\n",
      " -0.4881016 ]\n",
      "(1024,)\n"
     ]
    }
   ],
   "source": [
    "# send embedding key to HPS backend\n",
    "from tritonclient.utils import *\n",
    "import tritonclient.http  as httpclient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "model_name = 'hps_test'\n",
    "\n",
    "\n",
    "\n",
    "with httpclient.InferenceServerClient(\"localhost:8000\") as client:\n",
    "\n",
    "    embedding_columns = np.array(torch.randint(low=0, high=999, size=(1,32)).numpy().tolist(),dtype='int64')\n",
    "    row_ptrs = np.array([[32]],dtype='int32')\n",
    "\n",
    "    inputs = [\n",
    "        httpclient.InferInput(\"KEYS\", embedding_columns.shape,\n",
    "                              np_to_triton_dtype(embedding_columns.dtype)),\n",
    "        httpclient.InferInput(\"NUMKEYS\", row_ptrs.shape,\n",
    "                              np_to_triton_dtype(row_ptrs.dtype)),\n",
    "\n",
    "    ]\n",
    "\n",
    "    inputs[0].set_data_from_numpy(embedding_columns)\n",
    "    inputs[1].set_data_from_numpy(row_ptrs)\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"OUTPUT0\")\n",
    "    ]\n",
    "\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"OUTPUT0\")\n",
    "    ]\n",
    "\n",
    "    response = client.infer(model_name,\n",
    "                            inputs,\n",
    "                            request_id=str(1),\n",
    "                            outputs=outputs)\n",
    "\n",
    "    result = response.get_response()\n",
    "    print(result)\n",
    "    print(\"Prediction Result:\")\n",
    "    print(response.as_numpy(\"OUTPUT0\"))\n",
    "    print(response.as_numpy(\"OUTPUT0\").shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f394105",
   "metadata": {},
   "source": [
    "<a id='section3.3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97440c8b",
   "metadata": {},
   "source": [
    "## 3.3 Configure \"ensemble_model\" Triton backend for Embedding and Dense model\n",
    "### 3.3.1 Configure \"ensemble_model\" Triton backend for Embedding deployment(HPS backend) and Dense model(Pytorch backend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e7d18a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p model/ensemble_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ad90d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model/ensemble_model/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/ensemble_model/config.pbtxt\n",
    "name: \"ensemble_model\"\n",
    "platform: \"ensemble\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    "  {\n",
    "    name: \"EMB_KEY\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"EMB_N_KEY\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"DENSE_OUTPUT\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [-1]\n",
    "  }\n",
    "]\n",
    "ensemble_scheduling {\n",
    "  step [\n",
    "    {\n",
    "      model_name: \"hps_test\"\n",
    "      model_version: -1\n",
    "      input_map {\n",
    "        key: \"KEYS\"\n",
    "        value: \"EMB_KEY\"\n",
    "      }\n",
    "      input_map {\n",
    "        key: \"NUMKEYS\"\n",
    "        value: \"EMB_N_KEY\"\n",
    "      }\n",
    "      output_map {\n",
    "        key: \"OUTPUT0\"\n",
    "        value: \"LOOKUP_VECTORS\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      model_name: \"dense_test\"\n",
    "      model_version: -1\n",
    "      input_map {\n",
    "        key: \"mlp.Linear_layer_1\"\n",
    "        value: \"LOOKUP_VECTORS\"\n",
    "      }\n",
    "      output_map {\n",
    "        key: \"prediction\"\n",
    "        value: \"DENSE_OUTPUT\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d8d4e0",
   "metadata": {},
   "source": [
    "### 3.3.2 Launch Triton Server to verify ensemble Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e5122180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1110 12:32:25.378918 2862 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7fbf80000000' with size 268435456\n",
      "I1110 12:32:25.383488 2862 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "I1110 12:32:25.383496 2862 cuda_memory_manager.cc:105] CUDA memory pool is created on device 1 with size 67108864\n",
      "I1110 12:32:25.383500 2862 cuda_memory_manager.cc:105] CUDA memory pool is created on device 2 with size 67108864\n",
      "I1110 12:32:25.383502 2862 cuda_memory_manager.cc:105] CUDA memory pool is created on device 3 with size 67108864\n",
      "I1110 12:32:25.761152 2862 model_repository_manager.cc:1206] loading: dense_test:0\n",
      "I1110 12:32:25.761386 2862 model_repository_manager.cc:1206] loading: hps_test:0\n",
      "I1110 12:32:26.162400 2862 libtorch.cc:1917] TRITONBACKEND_Initialize: pytorch\n",
      "I1110 12:32:26.162424 2862 libtorch.cc:1927] Triton TRITONBACKEND API version: 1.10\n",
      "I1110 12:32:26.162428 2862 libtorch.cc:1933] 'pytorch' TRITONBACKEND API version: 1.10\n",
      "I1110 12:32:26.162470 2862 libtorch.cc:1966] TRITONBACKEND_ModelInitialize: dense_test (version 0)\n",
      "W1110 12:32:26.163431 2862 libtorch.cc:262] skipping model configuration auto-complete for 'dense_test': not supported for pytorch backend\n",
      "I1110 12:32:26.166043 2862 libtorch.cc:291] Optimized execution is enabled for model instance 'dense_test'\n",
      "I1110 12:32:26.166054 2862 libtorch.cc:310] Cache Cleaning is disabled for model instance 'dense_test'\n",
      "I1110 12:32:26.166057 2862 libtorch.cc:327] Inference Mode is disabled for model instance 'dense_test'\n",
      "I1110 12:32:26.166061 2862 libtorch.cc:422] NvFuser is not specified for model instance 'dense_test'\n",
      "I1110 12:32:26.185183 2862 hps.cc:61] TRITONBACKEND_Initialize: hps\n",
      "I1110 12:32:26.185197 2862 hps.cc:68] Triton TRITONBACKEND API version: 1.10\n",
      "I1110 12:32:26.185202 2862 hps.cc:72] 'hps' TRITONBACKEND API version: 1.9\n",
      "I1110 12:32:26.185225 2862 hps.cc:95] The Hierarchical Parameter Server Backend Repository location: /opt/tritonserver/backends/hps\n",
      "I1110 12:32:26.185233 2862 hps.cc:106] The HPS configuration: {\"cmdline\":{\"auto-complete-config\":\"true\",\"backend-directory\":\"/opt/tritonserver/backends\",\"min-compute-capability\":\"6.000000\",\"ps\":\"/hugectr/hugectr_inference_backend/hps_backend/samples/hps-triton-ensemble/model/hps.json\",\"default-max-batch-size\":\"4\"}}\n",
      "I1110 12:32:26.185285 2862 backend.cpp:62] *****The Hierarchical Parameter Server is creating... *****\n",
      "I1110 12:32:26.185290 2862 backend.cpp:65] ***** Hierarchical Parameter Server(Int64) is creating... *****\n",
      "=====================================================HPS Parse====================================================\n",
      "[HCTR][12:32:26.185][INFO][RK0][main]: allocation_rate is not specified using default: 268435456\n",
      "[HCTR][12:32:26.185][INFO][RK0][main]: shared_memory_size is not specified using default: 17179869184\n",
      "[HCTR][12:32:26.185][INFO][RK0][main]: shared_memory_name is not specified using default: hctr_mp_hash_map_database\n",
      "[HCTR][12:32:26.185][INFO][RK0][main]: shared_memory_size is not specified using default: 17179869184\n",
      "[HCTR][12:32:26.185][INFO][RK0][main]: shared_memory_name is not specified using default: hctr_mp_hash_map_database\n",
      "[HCTR][12:32:26.185][INFO][RK0][main]: num_node_connections is not specified using default: 5\n",
      "[HCTR][12:32:26.185][INFO][RK0][main]: refresh_time_after_fetch is not specified using default: 0\n",
      "[HCTR][12:32:26.185][INFO][RK0][main]: cache_missed_embeddings is not specified using default: 0\n",
      "[HCTR][12:32:26.185][INFO][RK0][main]: dense_file is not specified using default: \n",
      "[HCTR][12:32:26.185][INFO][RK0][main]: maxnum_des_feature_per_sample is not specified using default: 26\n",
      "[HCTR][12:32:26.185][INFO][RK0][main]: refresh_delay is not specified using default: 0\n",
      "[HCTR][12:32:26.185][INFO][RK0][main]: refresh_interval is not specified using default: 0\n",
      "====================================================HPS Create====================================================\n",
      "[HCTR][12:32:26.185][INFO][RK0][main]: Creating HashMap CPU database backend...\n",
      "[HCTR][12:32:26.186][DEBUG][RK0][main]: Created blank database backend in local memory!\n",
      "[HCTR][12:32:26.186][INFO][RK0][main]: Volatile DB: initial cache rate = 1\n",
      "[HCTR][12:32:26.186][INFO][RK0][main]: Volatile DB: cache missed embeddings = 0\n",
      "[HCTR][12:32:26.186][DEBUG][RK0][main]: Created raw model loader in local memory!\n",
      "[HCTR][12:32:26.186][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][12:32:26.351][INFO][RK0][main]: Table: hps_et.hps_test.0; cached 1000 / 1000 embeddings in volatile database (HashMapBackend); load: 1000 / 80000000 (0.00%).\n",
      "[HCTR][12:32:26.351][DEBUG][RK0][main]: Real-time subscribers created!\n",
      "[HCTR][12:32:26.351][INFO][RK0][main]: Creating embedding cache in device 0.\n",
      "[HCTR][12:32:26.357][INFO][RK0][main]: Model name: hps_test\n",
      "[HCTR][12:32:26.357][INFO][RK0][main]: Number of embedding tables: 1\n",
      "[HCTR][12:32:26.357][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 1.000000\n",
      "[HCTR][12:32:26.357][INFO][RK0][main]: Use I64 input key: True\n",
      "[HCTR][12:32:26.357][INFO][RK0][main]: Configured cache hit rate threshold: 0.900000\n",
      "[HCTR][12:32:26.357][INFO][RK0][main]: The size of thread pool: 80\n",
      "[HCTR][12:32:26.357][INFO][RK0][main]: The size of worker memory pool: 3\n",
      "[HCTR][12:32:26.357][INFO][RK0][main]: The size of refresh memory pool: 0\n",
      "[HCTR][12:32:26.357][INFO][RK0][main]: The refresh percentage : 0.000000\n",
      "I1110 12:32:26.369325 2862 backend.cpp:73] *****The Hierarchaical Prameter Server has been created successfully! *****\n",
      "I1110 12:32:26.369386 2862 hps.cc:169] TRITONBACKEND_ModelInitialize: hps_test (version 0)\n",
      "I1110 12:32:26.369393 2862 hps.cc:182] Repository location: /hugectr/hugectr_inference_backend/hps_backend/samples/hps-triton-ensemble/model/hps_test\n",
      "I1110 12:32:26.369398 2862 hps.cc:197] backend configuration in mode: {\"cmdline\":{\"auto-complete-config\":\"true\",\"backend-directory\":\"/opt/tritonserver/backends\",\"min-compute-capability\":\"6.000000\",\"ps\":\"/hugectr/hugectr_inference_backend/hps_backend/samples/hps-triton-ensemble/model/hps.json\",\"default-max-batch-size\":\"4\"}}\n",
      "I1110 12:32:26.369849 2862 model_state.cpp:129] Verifying model configuration: {\n",
      "    \"name\": \"hps_test\",\n",
      "    \"platform\": \"\",\n",
      "    \"backend\": \"hps\",\n",
      "    \"version_policy\": {\n",
      "        \"specific\": {\n",
      "            \"versions\": [\n",
      "                0\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"max_batch_size\": 32,\n",
      "    \"input\": [\n",
      "        {\n",
      "            \"name\": \"KEYS\",\n",
      "            \"data_type\": \"TYPE_INT64\",\n",
      "            \"format\": \"FORMAT_NONE\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"is_shape_tensor\": false,\n",
      "            \"allow_ragged_batch\": false,\n",
      "            \"optional\": false\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"NUMKEYS\",\n",
      "            \"data_type\": \"TYPE_INT32\",\n",
      "            \"format\": \"FORMAT_NONE\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"is_shape_tensor\": false,\n",
      "            \"allow_ragged_batch\": false,\n",
      "            \"optional\": false\n",
      "        }\n",
      "    ],\n",
      "    \"output\": [\n",
      "        {\n",
      "            \"name\": \"OUTPUT0\",\n",
      "            \"data_type\": \"TYPE_FP32\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"label_filename\": \"\",\n",
      "            \"is_shape_tensor\": false\n",
      "        }\n",
      "    ],\n",
      "    \"batch_input\": [],\n",
      "    \"batch_output\": [],\n",
      "    \"optimization\": {\n",
      "        \"priority\": \"PRIORITY_DEFAULT\",\n",
      "        \"input_pinned_memory\": {\n",
      "            \"enable\": true\n",
      "        },\n",
      "        \"output_pinned_memory\": {\n",
      "            \"enable\": true\n",
      "        },\n",
      "        \"gather_kernel_buffer_threshold\": 0,\n",
      "        \"eager_batching\": false\n",
      "    },\n",
      "    \"instance_group\": [\n",
      "        {\n",
      "            \"name\": \"hps_test_0\",\n",
      "            \"kind\": \"KIND_GPU\",\n",
      "            \"count\": 1,\n",
      "            \"gpus\": [\n",
      "                0\n",
      "            ],\n",
      "            \"secondary_devices\": [],\n",
      "            \"profile\": [],\n",
      "            \"passive\": false,\n",
      "            \"host_policy\": \"\"\n",
      "        }\n",
      "    ],\n",
      "    \"default_model_filename\": \"\",\n",
      "    \"cc_model_filenames\": {},\n",
      "    \"metric_tags\": {},\n",
      "    \"parameters\": {},\n",
      "    \"model_warmup\": []\n",
      "}\n",
      "I1110 12:32:26.369934 2862 model_state.cpp:210] The model configuration: {\n",
      "    \"name\": \"hps_test\",\n",
      "    \"platform\": \"\",\n",
      "    \"backend\": \"hps\",\n",
      "    \"version_policy\": {\n",
      "        \"specific\": {\n",
      "            \"versions\": [\n",
      "                0\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"max_batch_size\": 32,\n",
      "    \"input\": [\n",
      "        {\n",
      "            \"name\": \"KEYS\",\n",
      "            \"data_type\": \"TYPE_INT64\",\n",
      "            \"format\": \"FORMAT_NONE\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"is_shape_tensor\": false,\n",
      "            \"allow_ragged_batch\": false,\n",
      "            \"optional\": false\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"NUMKEYS\",\n",
      "            \"data_type\": \"TYPE_INT32\",\n",
      "            \"format\": \"FORMAT_NONE\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"is_shape_tensor\": false,\n",
      "            \"allow_ragged_batch\": false,\n",
      "            \"optional\": false\n",
      "        }\n",
      "    ],\n",
      "    \"output\": [\n",
      "        {\n",
      "            \"name\": \"OUTPUT0\",\n",
      "            \"data_type\": \"TYPE_FP32\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"label_filename\": \"\",\n",
      "            \"is_shape_tensor\": false\n",
      "        }\n",
      "    ],\n",
      "    \"batch_input\": [],\n",
      "    \"batch_output\": [],\n",
      "    \"optimization\": {\n",
      "        \"priority\": \"PRIORITY_DEFAULT\",\n",
      "        \"input_pinned_memory\": {\n",
      "            \"enable\": true\n",
      "        },\n",
      "        \"output_pinned_memory\": {\n",
      "            \"enable\": true\n",
      "        },\n",
      "        \"gather_kernel_buffer_threshold\": 0,\n",
      "        \"eager_batching\": false\n",
      "    },\n",
      "    \"instance_group\": [\n",
      "        {\n",
      "            \"name\": \"hps_test_0\",\n",
      "            \"kind\": \"KIND_GPU\",\n",
      "            \"count\": 1,\n",
      "            \"gpus\": [\n",
      "                0\n",
      "            ],\n",
      "            \"secondary_devices\": [],\n",
      "            \"profile\": [],\n",
      "            \"passive\": false,\n",
      "            \"host_policy\": \"\"\n",
      "        }\n",
      "    ],\n",
      "    \"default_model_filename\": \"\",\n",
      "    \"cc_model_filenames\": {},\n",
      "    \"metric_tags\": {},\n",
      "    \"parameters\": {},\n",
      "    \"model_warmup\": []\n",
      "}\n",
      "I1110 12:32:26.370013 2862 model_state.cpp:280] max_batch_size in model config.pbtxt is 32\n",
      "I1110 12:32:26.370020 2862 model_state.cpp:319] ******Creating Embedding Cache for model hps_test in device 0\n",
      "I1110 12:32:26.370025 2862 model_state.cpp:329] ******Creating Embedding Cache for model hps_test successfully\n",
      "I1110 12:32:26.370086 2862 libtorch.cc:2010] TRITONBACKEND_ModelInstanceInitialize: dense_test (GPU device 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1110 12:32:27.249859 2862 hps.cc:307] TRITONBACKEND_ModelInstanceInitialize: hps_test_0 (device 0)\n",
      "I1110 12:32:27.249937 2862 model_instance_state.cpp:81] Triton Model Instance Initialization on device 0\n",
      "I1110 12:32:27.249948 2862 model_instance_state.cpp:91] Categorical Feature buffer allocation: \n",
      "I1110 12:32:27.250375 2862 model_instance_state.cpp:99] Number of Categorical Feature per Table buffer allocation: \n",
      "I1110 12:32:27.250487 2862 model_instance_state.cpp:109] Look_up result buffer allocation: \n",
      "I1110 12:32:27.250570 2862 hps.cc:320] ******Loading HPS ******\n",
      "I1110 12:32:27.250577 2862 model_instance_state.cpp:140] The model origin json configuration file path is: \n",
      "[HCTR][12:32:27.250][INFO][RK0][main]: Creating lookup session for hps_test on device: 0\n",
      "I1110 12:32:27.250662 2862 model_instance_state.cpp:147] ******Loading HugeCTR lookup session successfully\n",
      "I1110 12:32:27.250711 2862 libtorch.cc:2010] TRITONBACKEND_ModelInstanceInitialize: dense_test (GPU device 1)\n",
      "I1110 12:32:27.250860 2862 model_repository_manager.cc:1352] successfully loaded 'hps_test' version 0\n",
      "I1110 12:32:27.987660 2862 libtorch.cc:2010] TRITONBACKEND_ModelInstanceInitialize: dense_test (GPU device 2)\n",
      "I1110 12:32:28.651512 2862 libtorch.cc:2010] TRITONBACKEND_ModelInstanceInitialize: dense_test (GPU device 3)\n",
      "I1110 12:32:29.317414 2862 model_repository_manager.cc:1352] successfully loaded 'dense_test' version 0\n",
      "I1110 12:32:29.317917 2862 model_repository_manager.cc:1206] loading: ensemble_model:0\n",
      "I1110 12:32:29.318307 2862 model_repository_manager.cc:1352] successfully loaded 'ensemble_model' version 0\n",
      "I1110 12:32:29.318420 2862 server.cc:559] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I1110 12:32:29.318495 2862 server.cc:586] \n",
      "+---------+---------------------------------+---------------------------------+\n",
      "| Backend | Path                            | Config                          |\n",
      "+---------+---------------------------------+---------------------------------+\n",
      "| pytorch | /opt/tritonserver/backends/pyto | {\"cmdline\":{\"auto-complete-conf |\n",
      "|         | rch/libtriton_pytorch.so        | ig\":\"true\",\"min-compute-capabil |\n",
      "|         |                                 | ity\":\"6.000000\",\"backend-direct |\n",
      "|         |                                 | ory\":\"/opt/tritonserver/backend |\n",
      "|         |                                 | s\",\"default-max-batch-size\":\"4\" |\n",
      "|         |                                 | }}                              |\n",
      "|         |                                 |                                 |\n",
      "| hps     | /opt/tritonserver/backends/hps/ | {\"cmdline\":{\"auto-complete-conf |\n",
      "|         | libtriton_hps.so                | ig\":\"true\",\"backend-directory\": |\n",
      "|         |                                 | \"/opt/tritonserver/backends\",\"m |\n",
      "|         |                                 | in-compute-capability\":\"6.00000 |\n",
      "|         |                                 | 0\",\"ps\":\"/hugectr/hugectr_infer |\n",
      "|         |                                 | ence_backend/hps_backend/sample |\n",
      "|         |                                 | s/hps-triton-ensemble/model/hps |\n",
      "|         |                                 | .json\",\"default-max-batch-size\" |\n",
      "|         |                                 | :\"4\"}}                          |\n",
      "|         |                                 |                                 |\n",
      "+---------+---------------------------------+---------------------------------+\n",
      "\n",
      "I1110 12:32:29.318573 2862 server.cc:629] \n",
      "+----------------+---------+--------+\n",
      "| Model          | Version | Status |\n",
      "+----------------+---------+--------+\n",
      "| dense_test     | 0       | READY  |\n",
      "| ensemble_model | 0       | READY  |\n",
      "| hps_test       | 0       | READY  |\n",
      "+----------------+---------+--------+\n",
      "\n",
      "I1110 12:32:29.399385 2862 metrics.cc:650] Collecting metrics for GPU 0: Tesla V100-SXM2-32GB\n",
      "I1110 12:32:29.399405 2862 metrics.cc:650] Collecting metrics for GPU 1: Tesla V100-SXM2-32GB\n",
      "I1110 12:32:29.399412 2862 metrics.cc:650] Collecting metrics for GPU 2: Tesla V100-SXM2-32GB\n",
      "I1110 12:32:29.399418 2862 metrics.cc:650] Collecting metrics for GPU 3: Tesla V100-SXM2-32GB\n",
      "I1110 12:32:29.401986 2862 tritonserver.cc:2176] \n",
      "+----------------------------------+------------------------------------------+\n",
      "| Option                           | Value                                    |\n",
      "+----------------------------------+------------------------------------------+\n",
      "| server_id                        | triton                                   |\n",
      "| server_version                   | 2.24.0                                   |\n",
      "| server_extensions                | classification sequence model_repository |\n",
      "|                                  |  model_repository(unload_dependents) sch |\n",
      "|                                  | edule_policy model_configuration system_ |\n",
      "|                                  | shared_memory cuda_shared_memory binary_ |\n",
      "|                                  | tensor_data statistics trace             |\n",
      "| model_repository_path[0]         | /hugectr/hugectr_inference_backend/hps_b |\n",
      "|                                  | ackend/samples/hps-triton-ensemble/model |\n",
      "|                                  | /                                        |\n",
      "| model_control_mode               | MODE_EXPLICIT                            |\n",
      "| startup_models_0                 | ensemble_model                           |\n",
      "| strict_model_config              | 0                                        |\n",
      "| rate_limit                       | OFF                                      |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                 |\n",
      "| cuda_memory_pool_byte_size{1}    | 67108864                                 |\n",
      "| cuda_memory_pool_byte_size{2}    | 67108864                                 |\n",
      "| cuda_memory_pool_byte_size{3}    | 67108864                                 |\n",
      "| response_cache_byte_size         | 0                                        |\n",
      "| min_supported_compute_capability | 6.0                                      |\n",
      "| strict_readiness                 | 1                                        |\n",
      "| exit_timeout                     | 30                                       |\n",
      "+----------------------------------+------------------------------------------+\n",
      "\n",
      "I1110 12:32:29.403325 2862 grpc_server.cc:4608] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I1110 12:32:29.403680 2862 http_server.cc:3312] Started HTTPService at 0.0.0.0:8000\n",
      "I1110 12:32:29.472622 2862 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002\n",
      "^C\n",
      "Signal (2) received.\n",
      "I1110 12:32:42.843694 2862 server.cc:260] Waiting for in-flight requests to complete.\n",
      "I1110 12:32:42.843725 2862 server.cc:276] Timeout 30: Found 0 model versions that have in-flight inferences\n",
      "I1110 12:32:42.843742 2862 model_repository_manager.cc:1230] unloading: hps_test:0\n",
      "I1110 12:32:42.843887 2862 model_repository_manager.cc:1230] unloading: ensemble_model:0\n",
      "I1110 12:32:42.843955 2862 model_repository_manager.cc:1230] unloading: dense_test:0\n",
      "I1110 12:32:42.844112 2862 server.cc:291] All models are stopped, unloading models\n",
      "I1110 12:32:42.844144 2862 server.cc:298] Timeout 30: Found 3 live models and 0 in-flight non-inference requests\n",
      "I1110 12:32:42.844196 2862 model_repository_manager.cc:1335] successfully unloaded 'ensemble_model' version 0\n",
      "I1110 12:32:42.844348 2862 hps.cc:337] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I1110 12:32:42.844407 2862 libtorch.cc:2044] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I1110 12:32:42.844912 2862 hps.cc:268] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I1110 12:32:42.846411 2862 libtorch.cc:2044] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I1110 12:32:42.847907 2862 libtorch.cc:2044] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I1110 12:32:42.848519 2862 model_state.cpp:112] ******Destorying Embedding Cache for model hps_test successfully\n",
      "I1110 12:32:42.848580 2862 model_repository_manager.cc:1335] successfully unloaded 'hps_test' version 0\n",
      "I1110 12:32:42.849265 2862 libtorch.cc:2044] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I1110 12:32:42.850352 2862 libtorch.cc:1989] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I1110 12:32:42.850795 2862 model_repository_manager.cc:1335] successfully unloaded 'dense_test' version 0\n"
     ]
    }
   ],
   "source": [
    "!tritonserver --model-repository=/hugectr_backend/model/ --load-model=ensemble_model --model-control-mode=explicit  --allow-gpu-metrics=true --backend-config=hps,ps=/hugectr_backend/model/hps.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d7ac3dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1', 'model_name': 'ensemble_model', 'model_version': '0', 'parameters': {'sequence_id': 0, 'sequence_start': False, 'sequence_end': False}, 'outputs': [{'name': 'DENSE_OUTPUT', 'datatype': 'FP32', 'shape': [1, 1], 'parameters': {'binary_data_size': 4}}]}\n",
      "Prediction Result:\n",
      "[[0.41792953]]\n"
     ]
    }
   ],
   "source": [
    "from tritonclient.utils import *\n",
    "import tritonclient.http  as httpclient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "model_name = 'ensemble_model'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with httpclient.InferenceServerClient(\"localhost:8000\") as client:\n",
    "    embedding_columns = np.array([[123]],dtype='int64')\n",
    "    row_ptrs = np.array([[1]],dtype='int32')\n",
    "\n",
    "    inputs = [\n",
    "        httpclient.InferInput(\"EMB_KEY\", embedding_columns.shape,\n",
    "                              np_to_triton_dtype(embedding_columns.dtype)),\n",
    "        httpclient.InferInput(\"EMB_N_KEY\", row_ptrs.shape,\n",
    "                              np_to_triton_dtype(row_ptrs.dtype)),\n",
    "\n",
    "    ]\n",
    "\n",
    "    inputs[0].set_data_from_numpy(embedding_columns)\n",
    "    inputs[1].set_data_from_numpy(row_ptrs)\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"DENSE_OUTPUT\")\n",
    "    ]\n",
    "\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"DENSE_OUTPUT\")\n",
    "    ]\n",
    "\n",
    "    response = client.infer(model_name,\n",
    "                            inputs,\n",
    "                            request_id=str(1),\n",
    "                            outputs=outputs)\n",
    "\n",
    "    result = response.get_response()\n",
    "    print(result)\n",
    "    print(\"Prediction Result:\")\n",
    "    print(response.as_numpy(\"DENSE_OUTPUT\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692fc5e3",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7790f23b",
   "metadata": {},
   "source": [
    "# 4 Use TensorRT to speed up dense model inference and combine with HPS Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46808665-c08d-4c91-9adf-f16105058918",
   "metadata": {},
   "source": [
    "## 4.1 Convert Dense model to Onnx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a2212af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "dense_model = torch.load(\"model/dense_test/0/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "141ea62c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4337]], grad_fn=<DifferentiableGraphBackward>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "dense_model(torch.from_numpy(np.array([np.random.uniform(0.0,1) for i in range(32)])).float())\n",
    "dense_model(torch.randn(1,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cb4bae3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%vectors : Float(*, 32, strides=[32, 1], requires_grad=0, device=cpu),\n",
      "      %mlp.Linear_layer_1.weight : Float(16, 32, strides=[32, 1], requires_grad=0, device=cpu),\n",
      "      %mlp.Linear_layer_1.bias : Float(16, strides=[1], requires_grad=0, device=cpu),\n",
      "      %mlp.Linear_layer_2.weight : Float(8, 16, strides=[16, 1], requires_grad=0, device=cpu),\n",
      "      %mlp.Linear_layer_2.bias : Float(8, strides=[1], requires_grad=0, device=cpu),\n",
      "      %predict.0.weight : Float(1, 8, strides=[8, 1], requires_grad=0, device=cpu),\n",
      "      %predict.0.bias : Float(1, strides=[1], requires_grad=0, device=cpu)):\n",
      "  %/Constant_output_0 : Long(2, strides=[1], device=cpu) = onnx::Constant[value= -1  32 [ CPULongType{2} ], onnx_name=\"/Constant\"](), scope: OnlyMLP:: # /tmp/ipykernel_128/146538850.py:19:15\n",
      "  %/Reshape_output_0 : Float(*, *, device=cpu) = onnx::Reshape[allowzero=0, onnx_name=\"/Reshape\"](%vectors, %/Constant_output_0), scope: OnlyMLP:: # /tmp/ipykernel_128/146538850.py:19:15\n",
      "  %/mlp/Linear_layer_1/Gemm_output_0 : Float(*, 16, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/mlp/Linear_layer_1/Gemm\"](%/Reshape_output_0, %mlp.Linear_layer_1.weight, %mlp.Linear_layer_1.bias), scope: OnlyMLP::/torch.nn.modules.container.Sequential::mlp/torch.nn.modules.linear.Linear::Linear_layer_1 # /usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:114:15\n",
      "  %/mlp/Relu_layer_1/Relu_output_0 : Float(*, 16, device=cpu) = onnx::Relu[onnx_name=\"/mlp/Relu_layer_1/Relu\"](%/mlp/Linear_layer_1/Gemm_output_0), scope: OnlyMLP::/torch.nn.modules.container.Sequential::mlp/torch.nn.modules.activation.ReLU::Relu_layer_1 # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1455:17\n",
      "  %/mlp/Linear_layer_2/Gemm_output_0 : Float(*, 8, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/mlp/Linear_layer_2/Gemm\"](%/mlp/Relu_layer_1/Relu_output_0, %mlp.Linear_layer_2.weight, %mlp.Linear_layer_2.bias), scope: OnlyMLP::/torch.nn.modules.container.Sequential::mlp/torch.nn.modules.linear.Linear::Linear_layer_2 # /usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:114:15\n",
      "  %/mlp/Relu_layer_2/Relu_output_0 : Float(*, 8, device=cpu) = onnx::Relu[onnx_name=\"/mlp/Relu_layer_2/Relu\"](%/mlp/Linear_layer_2/Gemm_output_0), scope: OnlyMLP::/torch.nn.modules.container.Sequential::mlp/torch.nn.modules.activation.ReLU::Relu_layer_2 # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1455:17\n",
      "  %/predict/0/Gemm_output_0 : Float(*, 1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/predict/0/Gemm\"](%/mlp/Relu_layer_2/Relu_output_0, %predict.0.weight, %predict.0.bias), scope: OnlyMLP::/torch.nn.modules.container.Sequential::predict/torch.nn.modules.linear.Linear::0 # /usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:114:15\n",
      "  %prediction : Float(*, 1, strides=[1, 1], requires_grad=1, device=cpu) = onnx::Sigmoid[onnx_name=\"/predict/1/Sigmoid\"](%/predict/0/Gemm_output_0), scope: OnlyMLP::/torch.nn.modules.container.Sequential::predict/torch.nn.modules.activation.Sigmoid::1 # /usr/local/lib/python3.8/dist-packages/torch/nn/modules/activation.py:294:15\n",
      "  return (%prediction)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:823: UserWarning: no signature found for <torch.ScriptMethod object at 0x7fdf5aae61d0>, skipping _decide_input_format\n",
      "  warnings.warn(f\"{e}, skipping _decide_input_format\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE = 32\n",
    "dummy_input=torch.randn(BATCH_SIZE,32)\n",
    "import torch.onnx\n",
    "torch.onnx.export(dense_model, dummy_input,\"model/dense_onnx_model.onnx\", verbose = True, input_names = [\"vectors\"], output_names = [\"prediction\"],dynamic_axes = {'vectors' : {0 : 'BATCH_SIZE'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e957462f",
   "metadata": {},
   "source": [
    "## 4.2 Conver onnx to tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8cf9576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PATH=/usr/src/tensorrt/bin/:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3a6181b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model/dense_trt/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4b5456c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8501] # trtexec --onnx=model/dense_onnx_model.onnx --saveEngine=model/dense_trt/0/dense_dynamic.trt --optShapes=vectors:32x32 --minShapes=vectors:1x32 --maxShapes=vectors:32x32\n",
      "[11/10/2022-12:38:10] [I] === Model Options ===\n",
      "[11/10/2022-12:38:10] [I] Format: ONNX\n",
      "[11/10/2022-12:38:10] [I] Model: model/dense_onnx_model.onnx\n",
      "[11/10/2022-12:38:10] [I] Output:\n",
      "[11/10/2022-12:38:10] [I] === Build Options ===\n",
      "[11/10/2022-12:38:10] [I] Max batch: explicit batch\n",
      "[11/10/2022-12:38:10] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
      "[11/10/2022-12:38:10] [I] minTiming: 1\n",
      "[11/10/2022-12:38:10] [I] avgTiming: 8\n",
      "[11/10/2022-12:38:10] [I] Precision: FP32\n",
      "[11/10/2022-12:38:10] [I] LayerPrecisions: \n",
      "[11/10/2022-12:38:10] [I] Calibration: \n",
      "[11/10/2022-12:38:10] [I] Refit: Disabled\n",
      "[11/10/2022-12:38:10] [I] Sparsity: Disabled\n",
      "[11/10/2022-12:38:10] [I] Safe mode: Disabled\n",
      "[11/10/2022-12:38:10] [I] DirectIO mode: Disabled\n",
      "[11/10/2022-12:38:10] [I] Restricted mode: Disabled\n",
      "[11/10/2022-12:38:10] [I] Build only: Disabled\n",
      "[11/10/2022-12:38:10] [I] Save engine: model/dense_trt/0/dense_dynamic.trt\n",
      "[11/10/2022-12:38:10] [I] Load engine: \n",
      "[11/10/2022-12:38:10] [I] Profiling verbosity: 0\n",
      "[11/10/2022-12:38:10] [I] Tactic sources: Using default tactic sources\n",
      "[11/10/2022-12:38:10] [I] timingCacheMode: local\n",
      "[11/10/2022-12:38:10] [I] timingCacheFile: \n",
      "[11/10/2022-12:38:10] [I] Heuristic: Disabled\n",
      "[11/10/2022-12:38:10] [I] Preview Features: Use default preview flags.\n",
      "[11/10/2022-12:38:10] [I] Input(s)s format: fp32:CHW\n",
      "[11/10/2022-12:38:10] [I] Output(s)s format: fp32:CHW\n",
      "[11/10/2022-12:38:10] [I] Input build shape: vectors=1x32+32x32+32x32\n",
      "[11/10/2022-12:38:10] [I] Input calibration shapes: model\n",
      "[11/10/2022-12:38:10] [I] === System Options ===\n",
      "[11/10/2022-12:38:10] [I] Device: 0\n",
      "[11/10/2022-12:38:10] [I] DLACore: \n",
      "[11/10/2022-12:38:10] [I] Plugins:\n",
      "[11/10/2022-12:38:10] [I] === Inference Options ===\n",
      "[11/10/2022-12:38:10] [I] Batch: Explicit\n",
      "[11/10/2022-12:38:10] [I] Input inference shape: vectors=32x32\n",
      "[11/10/2022-12:38:10] [I] Iterations: 10\n",
      "[11/10/2022-12:38:10] [I] Duration: 3s (+ 200ms warm up)\n",
      "[11/10/2022-12:38:10] [I] Sleep time: 0ms\n",
      "[11/10/2022-12:38:10] [I] Idle time: 0ms\n",
      "[11/10/2022-12:38:10] [I] Streams: 1\n",
      "[11/10/2022-12:38:10] [I] ExposeDMA: Disabled\n",
      "[11/10/2022-12:38:10] [I] Data transfers: Enabled\n",
      "[11/10/2022-12:38:10] [I] Spin-wait: Disabled\n",
      "[11/10/2022-12:38:10] [I] Multithreading: Disabled\n",
      "[11/10/2022-12:38:10] [I] CUDA Graph: Disabled\n",
      "[11/10/2022-12:38:10] [I] Separate profiling: Disabled\n",
      "[11/10/2022-12:38:10] [I] Time Deserialize: Disabled\n",
      "[11/10/2022-12:38:10] [I] Time Refit: Disabled\n",
      "[11/10/2022-12:38:10] [I] NVTX verbosity: 0\n",
      "[11/10/2022-12:38:10] [I] Persistent Cache Ratio: 0\n",
      "[11/10/2022-12:38:10] [I] Inputs:\n",
      "[11/10/2022-12:38:10] [I] === Reporting Options ===\n",
      "[11/10/2022-12:38:10] [I] Verbose: Disabled\n",
      "[11/10/2022-12:38:10] [I] Averages: 10 inferences\n",
      "[11/10/2022-12:38:10] [I] Percentiles: 90,95,99\n",
      "[11/10/2022-12:38:10] [I] Dump refittable layers:Disabled\n",
      "[11/10/2022-12:38:10] [I] Dump output: Disabled\n",
      "[11/10/2022-12:38:10] [I] Profile: Disabled\n",
      "[11/10/2022-12:38:10] [I] Export timing to JSON file: \n",
      "[11/10/2022-12:38:10] [I] Export output to JSON file: \n",
      "[11/10/2022-12:38:10] [I] Export profile to JSON file: \n",
      "[11/10/2022-12:38:10] [I] \n",
      "[11/10/2022-12:38:10] [I] === Device Information ===\n",
      "[11/10/2022-12:38:10] [I] Selected Device: Tesla V100-SXM2-32GB\n",
      "[11/10/2022-12:38:10] [I] Compute Capability: 7.0\n",
      "[11/10/2022-12:38:10] [I] SMs: 80\n",
      "[11/10/2022-12:38:10] [I] Compute Clock Rate: 1.53 GHz\n",
      "[11/10/2022-12:38:10] [I] Device Global Memory: 32510 MiB\n",
      "[11/10/2022-12:38:10] [I] Shared Memory per SM: 96 KiB\n",
      "[11/10/2022-12:38:10] [I] Memory Bus Width: 4096 bits (ECC enabled)\n",
      "[11/10/2022-12:38:10] [I] Memory Clock Rate: 0.877 GHz\n",
      "[11/10/2022-12:38:10] [I] \n",
      "[11/10/2022-12:38:10] [I] TensorRT version: 8.5.1\n",
      "[11/10/2022-12:38:10] [I] [TRT] [MemUsageChange] Init CUDA: CPU +268, GPU +0, now: CPU 280, GPU 490 (MiB)\n",
      "[11/10/2022-12:38:12] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +170, GPU +46, now: CPU 505, GPU 536 (MiB)\n",
      "[11/10/2022-12:38:12] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
      "[11/10/2022-12:38:12] [I] Start parsing network model\n",
      "[11/10/2022-12:38:12] [I] [TRT] ----------------------------------------------------------------\n",
      "[11/10/2022-12:38:12] [I] [TRT] Input filename:   model/dense_onnx_model.onnx\n",
      "[11/10/2022-12:38:12] [I] [TRT] ONNX IR version:  0.0.7\n",
      "[11/10/2022-12:38:12] [I] [TRT] Opset version:    14\n",
      "[11/10/2022-12:38:12] [I] [TRT] Producer name:    pytorch\n",
      "[11/10/2022-12:38:12] [I] [TRT] Producer version: 1.13.0\n",
      "[11/10/2022-12:38:12] [I] [TRT] Domain:           \n",
      "[11/10/2022-12:38:12] [I] [TRT] Model version:    0\n",
      "[11/10/2022-12:38:12] [I] [TRT] Doc string:       \n",
      "[11/10/2022-12:38:12] [I] [TRT] ----------------------------------------------------------------\n",
      "[11/10/2022-12:38:12] [W] [TRT] onnx2trt_utils.cpp:377: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[11/10/2022-12:38:12] [I] Finish parsing network model\n",
      "[11/10/2022-12:38:13] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +370, GPU +170, now: CPU 875, GPU 706 (MiB)\n",
      "[11/10/2022-12:38:13] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +118, GPU +54, now: CPU 993, GPU 760 (MiB)\n",
      "[11/10/2022-12:38:13] [W] [TRT] TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.4.1\n",
      "[11/10/2022-12:38:13] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[11/10/2022-12:38:18] [I] [TRT] Total Activation Memory: 34089734656\n",
      "[11/10/2022-12:38:18] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[11/10/2022-12:38:18] [I] [TRT] Total Host Persistent Memory: 18912\n",
      "[11/10/2022-12:38:18] [I] [TRT] Total Device Persistent Memory: 0\n",
      "[11/10/2022-12:38:18] [I] [TRT] Total Scratch Memory: 0\n",
      "[11/10/2022-12:38:18] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 4 MiB\n",
      "[11/10/2022-12:38:18] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 5 steps to complete.\n",
      "[11/10/2022-12:38:18] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 0.014528ms to assign 2 blocks to 5 nodes requiring 3072 bytes.\n",
      "[11/10/2022-12:38:18] [I] [TRT] Total Activation Memory: 3072\n",
      "[11/10/2022-12:38:18] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +4, now: CPU 0, GPU 4 (MiB)\n",
      "[11/10/2022-12:38:18] [I] Engine built in 7.93346 sec.\n",
      "[11/10/2022-12:38:18] [I] [TRT] Loaded engine size: 0 MiB\n",
      "[11/10/2022-12:38:18] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)\n",
      "[11/10/2022-12:38:18] [I] Engine deserialized in 0.00622754 sec.\n",
      "[11/10/2022-12:38:18] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)\n",
      "[11/10/2022-12:38:18] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
      "[11/10/2022-12:38:18] [I] Setting persistentCacheLimit to 0 bytes.\n",
      "[11/10/2022-12:38:18] [I] Using random values for input vectors\n",
      "[11/10/2022-12:38:18] [I] Created input binding for vectors with dimensions 32x32\n",
      "[11/10/2022-12:38:18] [I] Using random values for output prediction\n",
      "[11/10/2022-12:38:18] [I] Created output binding for prediction with dimensions 32x1\n",
      "[11/10/2022-12:38:18] [I] Starting inference\n",
      "[11/10/2022-12:38:21] [I] Warmup completed 4149 queries over 200 ms\n",
      "[11/10/2022-12:38:21] [I] Timing trace has 61598 queries over 3.0001 s\n",
      "[11/10/2022-12:38:21] [I] \n",
      "[11/10/2022-12:38:21] [I] === Trace details ===\n",
      "[11/10/2022-12:38:21] [I] Trace averages of 10 runs:\n",
      "[11/10/2022-12:38:21] [I] Average on 10 runs - GPU latency: 0.0183182 ms - Host latency: 0.0319962 ms (enqueue 0.0166595 ms)\n",
      "[11/10/2022-12:38:21] [I] Average on 10 runs - GPU latency: 0.0171524 ms - Host latency: 0.029097 ms (enqueue 0.014328 ms)\n",
      "[11/10/2022-12:38:21] [I] Average on 10 runs - GPU latency: 0.0170898 ms - Host latency: 0.0290283 ms (enqueue 0.014563 ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/10/2022-12:38:21] [I] \r\n",
      "[11/10/2022-12:38:21] [I] === Performance summary ===\r\n",
      "[11/10/2022-12:38:21] [I] Throughput: 20532 qps\r\n",
      "[11/10/2022-12:38:21] [I] Latency: min = 0.0257568 ms, max = 4.00183 ms, mean = 0.0295817 ms, median = 0.0289307 ms, percentile(90%) = 0.0292969 ms, percentile(95%) = 0.029541 ms, percentile(99%) = 0.0423584 ms\r\n",
      "[11/10/2022-12:38:21] [I] Enqueue Time: min = 0.0134277 ms, max = 3.35352 ms, mean = 0.0149384 ms, median = 0.0144043 ms, percentile(90%) = 0.0150146 ms, percentile(95%) = 0.0153809 ms, percentile(99%) = 0.031311 ms\r\n",
      "[11/10/2022-12:38:21] [I] H2D Latency: min = 0.00378418 ms, max = 3.95569 ms, mean = 0.00693778 ms, median = 0.0067749 ms, percentile(90%) = 0.00695801 ms, percentile(95%) = 0.00708008 ms, percentile(99%) = 0.00726318 ms\r\n",
      "[11/10/2022-12:38:21] [I] GPU Compute Time: min = 0.0166016 ms, max = 3.34741 ms, mean = 0.0174177 ms, median = 0.0170898 ms, percentile(90%) = 0.017334 ms, percentile(95%) = 0.0175781 ms, percentile(99%) = 0.027832 ms\r\n",
      "[11/10/2022-12:38:21] [I] D2H Latency: min = 0.00488281 ms, max = 2.73523 ms, mean = 0.00522665 ms, median = 0.00512695 ms, percentile(90%) = 0.00524902 ms, percentile(95%) = 0.00537109 ms, percentile(99%) = 0.0065918 ms\r\n",
      "[11/10/2022-12:38:21] [I] Total Host Walltime: 3.0001 s\r\n",
      "[11/10/2022-12:38:21] [I] Total GPU Compute Time: 1.07289 s\r\n",
      "[11/10/2022-12:38:21] [W] * Throughput may be bound by Enqueue Time rather than GPU Compute and the GPU may be under-utilized.\r\n",
      "[11/10/2022-12:38:21] [W]   If not already in use, --useCudaGraph (utilize CUDA graphs where possible) may increase the throughput.\r\n",
      "[11/10/2022-12:38:21] [W] * GPU compute time is unstable, with coefficient of variance = 98.4293%.\r\n",
      "[11/10/2022-12:38:21] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\r\n",
      "[11/10/2022-12:38:21] [I] Explanations of the performance metrics are printed in the verbose logs.\r\n",
      "[11/10/2022-12:38:21] [I] \r\n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8501] # trtexec --onnx=model/dense_onnx_model.onnx --saveEngine=model/dense_trt/0/dense_dynamic.trt --optShapes=vectors:32x32 --minShapes=vectors:1x32 --maxShapes=vectors:32x32\r\n"
     ]
    }
   ],
   "source": [
    "!trtexec --onnx=model/dense_onnx_model.onnx  --saveEngine=model/dense_trt/0/dense_dynamic.trt --optShapes=vectors:32x32 --minShapes=vectors:1x32 --maxShapes=vectors:32x32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd7d580",
   "metadata": {},
   "source": [
    "### If the tensorrt is not installed in the merlin container, you need to run the following command in the host to get the tensorrt engine of dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003f2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run --runtime=nvidia --cap-add SYS_NICE --gpus=all --net=host -u root -v $(pwd):/hugectr_backend -w /hugectr_backend nvcr.io/nvidia/tensorrt:22.11-py3 trtexec --onnx=/hugectr_backend/model/dense_onnx_model.onnx  --saveEngine=/hugectr_backend/model/dense_trt/0/dense_dynamic.trt --optShapes=vectors:32x32 --minShapes=vectors:1x32 --maxShapes=vectors:32x32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efba41f3",
   "metadata": {},
   "source": [
    "### Create the Tensorrt backend configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e335d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model/dense_trt/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/dense_trt/config.pbtxt\n",
    "platform: \"tensorrt_plan\"\n",
    "default_model_filename: \"dense_dynamic.trt\"\n",
    "backend: \"tensorrt\"\n",
    "max_batch_size: 0\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"vectors\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [1024]\n",
    "    reshape: { shape: [32,32] }\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "      name: \"prediction\"\n",
    "      data_type: TYPE_FP32\n",
    "      dims: [-1, 1]\n",
    "  }\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {\n",
    "    kind: KIND_GPU\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107e37af",
   "metadata": {},
   "source": [
    "# HPS + TensorRT ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e47670f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model/ensemble_model/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/ensemble_model/config.pbtxt\n",
    "name: \"ensemble_model\"\n",
    "platform: \"ensemble\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    "  {\n",
    "    name: \"EMB_KEY\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"EMB_N_KEY\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"DENSE_OUTPUT\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [-1]\n",
    "  }\n",
    "]\n",
    "ensemble_scheduling {\n",
    "  step [\n",
    "    {\n",
    "      model_name: \"hps_test\"\n",
    "      model_version: -1\n",
    "      input_map {\n",
    "        key: \"KEYS\"\n",
    "        value: \"EMB_KEY\"\n",
    "      }\n",
    "      input_map {\n",
    "        key: \"NUMKEYS\"\n",
    "        value: \"EMB_N_KEY\"\n",
    "      }\n",
    "      output_map {\n",
    "        key: \"OUTPUT0\"\n",
    "        value: \"LOOKUP_VECTORS\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      model_name: \"dense_trt\"\n",
    "      model_version: -1\n",
    "      input_map {\n",
    "        key: \"vectors\"\n",
    "        value: \"LOOKUP_VECTORS\"\n",
    "      }\n",
    "      output_map {\n",
    "        key: \"prediction\"\n",
    "        value: \"DENSE_OUTPUT\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "60898258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1110 12:47:38.008801 3326 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7f8e1e000000' with size 268435456\n",
      "I1110 12:47:38.013449 3326 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "I1110 12:47:38.013458 3326 cuda_memory_manager.cc:105] CUDA memory pool is created on device 1 with size 67108864\n",
      "I1110 12:47:38.013461 3326 cuda_memory_manager.cc:105] CUDA memory pool is created on device 2 with size 67108864\n",
      "I1110 12:47:38.013464 3326 cuda_memory_manager.cc:105] CUDA memory pool is created on device 3 with size 67108864\n",
      "I1110 12:47:38.399856 3326 model_repository_manager.cc:1206] loading: dense_trt:0\n",
      "I1110 12:47:38.399985 3326 model_repository_manager.cc:1206] loading: hps_test:0\n",
      "I1110 12:47:38.425984 3326 tensorrt.cc:5427] TRITONBACKEND_Initialize: tensorrt\n",
      "I1110 12:47:38.426016 3326 tensorrt.cc:5437] Triton TRITONBACKEND API version: 1.10\n",
      "I1110 12:47:38.426023 3326 tensorrt.cc:5443] 'tensorrt' TRITONBACKEND API version: 1.10\n",
      "I1110 12:47:38.426169 3326 tensorrt.cc:5486] backend configuration:\n",
      "{\"cmdline\":{\"auto-complete-config\":\"true\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}}\n",
      "I1110 12:47:38.426227 3326 tensorrt.cc:5538] TRITONBACKEND_ModelInitialize: dense_trt (version 0)\n",
      "I1110 12:47:38.724854 3326 logging.cc:49] Loaded engine size: 0 MiB\n",
      "I1110 12:47:38.823401 3326 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)\n",
      "W1110 12:47:38.823444 3326 tensorrt.cc:692] The specified dimensions in model config for dense_trt hints that batching is unavailable\n",
      "I1110 12:47:38.846228 3326 hps.cc:61] TRITONBACKEND_Initialize: hps\n",
      "I1110 12:47:38.846240 3326 hps.cc:68] Triton TRITONBACKEND API version: 1.10\n",
      "I1110 12:47:38.846244 3326 hps.cc:72] 'hps' TRITONBACKEND API version: 1.9\n",
      "I1110 12:47:38.846247 3326 hps.cc:95] The Hierarchical Parameter Server Backend Repository location: /opt/tritonserver/backends/hps\n",
      "I1110 12:47:38.846272 3326 hps.cc:106] The HPS configuration: {\"cmdline\":{\"auto-complete-config\":\"true\",\"backend-directory\":\"/opt/tritonserver/backends\",\"min-compute-capability\":\"6.000000\",\"ps\":\"/hugectr/hugectr_inference_backend/hps_backend/samples/hps-triton-ensemble/model/hps.json\",\"default-max-batch-size\":\"4\"}}\n",
      "I1110 12:47:38.846328 3326 backend.cpp:62] *****The Hierarchical Parameter Server is creating... *****\n",
      "I1110 12:47:38.846333 3326 backend.cpp:65] ***** Hierarchical Parameter Server(Int64) is creating... *****\n",
      "=====================================================HPS Parse====================================================\n",
      "[HCTR][12:47:38.846][INFO][RK0][main]: allocation_rate is not specified using default: 268435456\n",
      "[HCTR][12:47:38.846][INFO][RK0][main]: shared_memory_size is not specified using default: 17179869184\n",
      "[HCTR][12:47:38.846][INFO][RK0][main]: shared_memory_name is not specified using default: hctr_mp_hash_map_database\n",
      "[HCTR][12:47:38.846][INFO][RK0][main]: shared_memory_size is not specified using default: 17179869184\n",
      "[HCTR][12:47:38.846][INFO][RK0][main]: shared_memory_name is not specified using default: hctr_mp_hash_map_database\n",
      "[HCTR][12:47:38.846][INFO][RK0][main]: num_node_connections is not specified using default: 5\n",
      "[HCTR][12:47:38.846][INFO][RK0][main]: refresh_time_after_fetch is not specified using default: 0\n",
      "[HCTR][12:47:38.846][INFO][RK0][main]: cache_missed_embeddings is not specified using default: 0\n",
      "[HCTR][12:47:38.846][INFO][RK0][main]: dense_file is not specified using default: \n",
      "[HCTR][12:47:38.846][INFO][RK0][main]: maxnum_des_feature_per_sample is not specified using default: 26\n",
      "[HCTR][12:47:38.846][INFO][RK0][main]: refresh_delay is not specified using default: 0\n",
      "[HCTR][12:47:38.847][INFO][RK0][main]: refresh_interval is not specified using default: 0\n",
      "====================================================HPS Create====================================================\n",
      "[HCTR][12:47:38.847][INFO][RK0][main]: Creating HashMap CPU database backend...\n",
      "[HCTR][12:47:38.847][DEBUG][RK0][main]: Created blank database backend in local memory!\n",
      "[HCTR][12:47:38.847][INFO][RK0][main]: Volatile DB: initial cache rate = 1\n",
      "[HCTR][12:47:38.847][INFO][RK0][main]: Volatile DB: cache missed embeddings = 0\n",
      "[HCTR][12:47:38.847][DEBUG][RK0][main]: Created raw model loader in local memory!\n",
      "[HCTR][12:47:38.847][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][12:47:39.014][INFO][RK0][main]: Table: hps_et.hps_test.0; cached 1000 / 1000 embeddings in volatile database (HashMapBackend); load: 1000 / 80000000 (0.00%).\n",
      "[HCTR][12:47:39.014][DEBUG][RK0][main]: Real-time subscribers created!\n",
      "[HCTR][12:47:39.014][INFO][RK0][main]: Creating embedding cache in device 0.\n",
      "[HCTR][12:47:39.020][INFO][RK0][main]: Model name: hps_test\n",
      "[HCTR][12:47:39.020][INFO][RK0][main]: Number of embedding tables: 1\n",
      "[HCTR][12:47:39.020][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 1.000000\n",
      "[HCTR][12:47:39.020][INFO][RK0][main]: Use I64 input key: True\n",
      "[HCTR][12:47:39.020][INFO][RK0][main]: Configured cache hit rate threshold: 0.900000\n",
      "[HCTR][12:47:39.020][INFO][RK0][main]: The size of thread pool: 80\n",
      "[HCTR][12:47:39.020][INFO][RK0][main]: The size of worker memory pool: 3\n",
      "[HCTR][12:47:39.020][INFO][RK0][main]: The size of refresh memory pool: 0\n",
      "[HCTR][12:47:39.020][INFO][RK0][main]: The refresh percentage : 0.000000\n",
      "I1110 12:47:39.032015 3326 backend.cpp:73] *****The Hierarchaical Prameter Server has been created successfully! *****\n",
      "I1110 12:47:39.032055 3326 hps.cc:169] TRITONBACKEND_ModelInitialize: hps_test (version 0)\n",
      "I1110 12:47:39.032059 3326 hps.cc:182] Repository location: /hugectr/hugectr_inference_backend/hps_backend/samples/hps-triton-ensemble/model/hps_test\n",
      "I1110 12:47:39.032064 3326 hps.cc:197] backend configuration in mode: {\"cmdline\":{\"auto-complete-config\":\"true\",\"backend-directory\":\"/opt/tritonserver/backends\",\"min-compute-capability\":\"6.000000\",\"ps\":\"/hugectr/hugectr_inference_backend/hps_backend/samples/hps-triton-ensemble/model/hps.json\",\"default-max-batch-size\":\"4\"}}\n",
      "I1110 12:47:39.032438 3326 model_state.cpp:129] Verifying model configuration: {\n",
      "    \"name\": \"hps_test\",\n",
      "    \"platform\": \"\",\n",
      "    \"backend\": \"hps\",\n",
      "    \"version_policy\": {\n",
      "        \"specific\": {\n",
      "            \"versions\": [\n",
      "                0\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"max_batch_size\": 32,\n",
      "    \"input\": [\n",
      "        {\n",
      "            \"name\": \"KEYS\",\n",
      "            \"data_type\": \"TYPE_INT64\",\n",
      "            \"format\": \"FORMAT_NONE\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"is_shape_tensor\": false,\n",
      "            \"allow_ragged_batch\": false,\n",
      "            \"optional\": false\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"NUMKEYS\",\n",
      "            \"data_type\": \"TYPE_INT32\",\n",
      "            \"format\": \"FORMAT_NONE\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"is_shape_tensor\": false,\n",
      "            \"allow_ragged_batch\": false,\n",
      "            \"optional\": false\n",
      "        }\n",
      "    ],\n",
      "    \"output\": [\n",
      "        {\n",
      "            \"name\": \"OUTPUT0\",\n",
      "            \"data_type\": \"TYPE_FP32\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"label_filename\": \"\",\n",
      "            \"is_shape_tensor\": false\n",
      "        }\n",
      "    ],\n",
      "    \"batch_input\": [],\n",
      "    \"batch_output\": [],\n",
      "    \"optimization\": {\n",
      "        \"priority\": \"PRIORITY_DEFAULT\",\n",
      "        \"input_pinned_memory\": {\n",
      "            \"enable\": true\n",
      "        },\n",
      "        \"output_pinned_memory\": {\n",
      "            \"enable\": true\n",
      "        },\n",
      "        \"gather_kernel_buffer_threshold\": 0,\n",
      "        \"eager_batching\": false\n",
      "    },\n",
      "    \"instance_group\": [\n",
      "        {\n",
      "            \"name\": \"hps_test_0\",\n",
      "            \"kind\": \"KIND_GPU\",\n",
      "            \"count\": 1,\n",
      "            \"gpus\": [\n",
      "                0\n",
      "            ],\n",
      "            \"secondary_devices\": [],\n",
      "            \"profile\": [],\n",
      "            \"passive\": false,\n",
      "            \"host_policy\": \"\"\n",
      "        }\n",
      "    ],\n",
      "    \"default_model_filename\": \"\",\n",
      "    \"cc_model_filenames\": {},\n",
      "    \"metric_tags\": {},\n",
      "    \"parameters\": {},\n",
      "    \"model_warmup\": []\n",
      "}\n",
      "I1110 12:47:39.032484 3326 model_state.cpp:210] The model configuration: {\n",
      "    \"name\": \"hps_test\",\n",
      "    \"platform\": \"\",\n",
      "    \"backend\": \"hps\",\n",
      "    \"version_policy\": {\n",
      "        \"specific\": {\n",
      "            \"versions\": [\n",
      "                0\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"max_batch_size\": 32,\n",
      "    \"input\": [\n",
      "        {\n",
      "            \"name\": \"KEYS\",\n",
      "            \"data_type\": \"TYPE_INT64\",\n",
      "            \"format\": \"FORMAT_NONE\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"is_shape_tensor\": false,\n",
      "            \"allow_ragged_batch\": false,\n",
      "            \"optional\": false\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"NUMKEYS\",\n",
      "            \"data_type\": \"TYPE_INT32\",\n",
      "            \"format\": \"FORMAT_NONE\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"is_shape_tensor\": false,\n",
      "            \"allow_ragged_batch\": false,\n",
      "            \"optional\": false\n",
      "        }\n",
      "    ],\n",
      "    \"output\": [\n",
      "        {\n",
      "            \"name\": \"OUTPUT0\",\n",
      "            \"data_type\": \"TYPE_FP32\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"label_filename\": \"\",\n",
      "            \"is_shape_tensor\": false\n",
      "        }\n",
      "    ],\n",
      "    \"batch_input\": [],\n",
      "    \"batch_output\": [],\n",
      "    \"optimization\": {\n",
      "        \"priority\": \"PRIORITY_DEFAULT\",\n",
      "        \"input_pinned_memory\": {\n",
      "            \"enable\": true\n",
      "        },\n",
      "        \"output_pinned_memory\": {\n",
      "            \"enable\": true\n",
      "        },\n",
      "        \"gather_kernel_buffer_threshold\": 0,\n",
      "        \"eager_batching\": false\n",
      "    },\n",
      "    \"instance_group\": [\n",
      "        {\n",
      "            \"name\": \"hps_test_0\",\n",
      "            \"kind\": \"KIND_GPU\",\n",
      "            \"count\": 1,\n",
      "            \"gpus\": [\n",
      "                0\n",
      "            ],\n",
      "            \"secondary_devices\": [],\n",
      "            \"profile\": [],\n",
      "            \"passive\": false,\n",
      "            \"host_policy\": \"\"\n",
      "        }\n",
      "    ],\n",
      "    \"default_model_filename\": \"\",\n",
      "    \"cc_model_filenames\": {},\n",
      "    \"metric_tags\": {},\n",
      "    \"parameters\": {},\n",
      "    \"model_warmup\": []\n",
      "}\n",
      "I1110 12:47:39.032565 3326 model_state.cpp:280] max_batch_size in model config.pbtxt is 32\n",
      "I1110 12:47:39.032583 3326 model_state.cpp:319] ******Creating Embedding Cache for model hps_test in device 0\n",
      "I1110 12:47:39.032588 3326 model_state.cpp:329] ******Creating Embedding Cache for model hps_test successfully\n",
      "I1110 12:47:39.032660 3326 tensorrt.cc:5587] TRITONBACKEND_ModelInstanceInitialize: dense_trt_0 (GPU device 0)\n",
      "I1110 12:47:39.036158 3326 logging.cc:49] Loaded engine size: 0 MiB\n",
      "I1110 12:47:39.042208 3326 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)\n",
      "I1110 12:47:39.044446 3326 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)\n",
      "W1110 12:47:39.044458 3326 logging.cc:46] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
      "I1110 12:47:39.044595 3326 tensorrt.cc:1541] Created instance dense_trt_0 on GPU 0 with stream priority 0 and optimization profile default[0];\n",
      "I1110 12:47:39.044685 3326 hps.cc:307] TRITONBACKEND_ModelInstanceInitialize: hps_test_0 (device 0)\n",
      "I1110 12:47:39.044739 3326 model_instance_state.cpp:81] Triton Model Instance Initialization on device 0\n",
      "I1110 12:47:39.044746 3326 model_instance_state.cpp:91] Categorical Feature buffer allocation: \n",
      "I1110 12:47:39.044905 3326 model_instance_state.cpp:99] Number of Categorical Feature per Table buffer allocation: \n",
      "I1110 12:47:39.044951 3326 model_instance_state.cpp:109] Look_up result buffer allocation: \n",
      "I1110 12:47:39.044981 3326 hps.cc:320] ******Loading HPS ******\n",
      "I1110 12:47:39.044987 3326 model_instance_state.cpp:140] The model origin json configuration file path is: \n",
      "[HCTR][12:47:39.044][INFO][RK0][main]: Creating lookup session for hps_test on device: 0\n",
      "I1110 12:47:39.045014 3326 model_instance_state.cpp:147] ******Loading HugeCTR lookup session successfully\n",
      "I1110 12:47:39.045026 3326 tensorrt.cc:5587] TRITONBACKEND_ModelInstanceInitialize: dense_trt_0 (GPU device 1)\n",
      "I1110 12:47:39.045198 3326 model_repository_manager.cc:1352] successfully loaded 'hps_test' version 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1110 12:47:39.261674 3326 logging.cc:49] Loaded engine size: 0 MiB\n",
      "I1110 12:47:39.267730 3326 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)\n",
      "I1110 12:47:39.270010 3326 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)\n",
      "W1110 12:47:39.270021 3326 logging.cc:46] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
      "I1110 12:47:39.270519 3326 tensorrt.cc:1541] Created instance dense_trt_0 on GPU 1 with stream priority 0 and optimization profile default[0];\n",
      "I1110 12:47:39.270736 3326 tensorrt.cc:5587] TRITONBACKEND_ModelInstanceInitialize: dense_trt_0 (GPU device 2)\n",
      "I1110 12:47:39.485668 3326 logging.cc:49] Loaded engine size: 0 MiB\n",
      "I1110 12:47:39.491818 3326 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)\n",
      "I1110 12:47:39.494035 3326 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)\n",
      "W1110 12:47:39.494046 3326 logging.cc:46] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
      "I1110 12:47:39.494246 3326 tensorrt.cc:1541] Created instance dense_trt_0 on GPU 2 with stream priority 0 and optimization profile default[0];\n",
      "I1110 12:47:39.494427 3326 tensorrt.cc:5587] TRITONBACKEND_ModelInstanceInitialize: dense_trt_0 (GPU device 3)\n",
      "I1110 12:47:39.718042 3326 logging.cc:49] Loaded engine size: 0 MiB\n",
      "I1110 12:47:39.724121 3326 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)\n",
      "I1110 12:47:39.726368 3326 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)\n",
      "W1110 12:47:39.726379 3326 logging.cc:46] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
      "I1110 12:47:39.726743 3326 tensorrt.cc:1541] Created instance dense_trt_0 on GPU 3 with stream priority 0 and optimization profile default[0];\n",
      "I1110 12:47:39.726956 3326 model_repository_manager.cc:1352] successfully loaded 'dense_trt' version 0\n",
      "I1110 12:47:39.727572 3326 model_repository_manager.cc:1206] loading: ensemble_model:0\n",
      "I1110 12:47:39.727805 3326 model_repository_manager.cc:1352] successfully loaded 'ensemble_model' version 0\n",
      "I1110 12:47:39.727879 3326 server.cc:559] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I1110 12:47:39.727930 3326 server.cc:586] \n",
      "+----------+--------------------------------+--------------------------------+\n",
      "| Backend  | Path                           | Config                         |\n",
      "+----------+--------------------------------+--------------------------------+\n",
      "| tensorrt | /opt/tritonserver/backends/ten | {\"cmdline\":{\"auto-complete-con |\n",
      "|          | sorrt/libtriton_tensorrt.so    | fig\":\"true\",\"min-compute-capab |\n",
      "|          |                                | ility\":\"6.000000\",\"backend-dir |\n",
      "|          |                                | ectory\":\"/opt/tritonserver/bac |\n",
      "|          |                                | kends\",\"default-max-batch-size |\n",
      "|          |                                | \":\"4\"}}                        |\n",
      "|          |                                |                                |\n",
      "| hps      | /opt/tritonserver/backends/hps | {\"cmdline\":{\"auto-complete-con |\n",
      "|          | /libtriton_hps.so              | fig\":\"true\",\"backend-directory |\n",
      "|          |                                | \":\"/opt/tritonserver/backends\" |\n",
      "|          |                                | ,\"min-compute-capability\":\"6.0 |\n",
      "|          |                                | 00000\",\"ps\":\"/hugectr/hugectr_ |\n",
      "|          |                                | inference_backend/hps_backend/ |\n",
      "|          |                                | samples/hps-triton-ensemble/mo |\n",
      "|          |                                | del/hps.json\",\"default-max-bat |\n",
      "|          |                                | ch-size\":\"4\"}}                 |\n",
      "|          |                                |                                |\n",
      "+----------+--------------------------------+--------------------------------+\n",
      "\n",
      "I1110 12:47:39.727982 3326 server.cc:629] \n",
      "+----------------+---------+--------+\n",
      "| Model          | Version | Status |\n",
      "+----------------+---------+--------+\n",
      "| dense_trt      | 0       | READY  |\n",
      "| ensemble_model | 0       | READY  |\n",
      "| hps_test       | 0       | READY  |\n",
      "+----------------+---------+--------+\n",
      "\n",
      "I1110 12:47:39.809624 3326 metrics.cc:650] Collecting metrics for GPU 0: Tesla V100-SXM2-32GB\n",
      "I1110 12:47:39.809690 3326 metrics.cc:650] Collecting metrics for GPU 1: Tesla V100-SXM2-32GB\n",
      "I1110 12:47:39.809734 3326 metrics.cc:650] Collecting metrics for GPU 2: Tesla V100-SXM2-32GB\n",
      "I1110 12:47:39.809761 3326 metrics.cc:650] Collecting metrics for GPU 3: Tesla V100-SXM2-32GB\n",
      "I1110 12:47:39.812492 3326 tritonserver.cc:2176] \n",
      "+----------------------------------+------------------------------------------+\n",
      "| Option                           | Value                                    |\n",
      "+----------------------------------+------------------------------------------+\n",
      "| server_id                        | triton                                   |\n",
      "| server_version                   | 2.24.0                                   |\n",
      "| server_extensions                | classification sequence model_repository |\n",
      "|                                  |  model_repository(unload_dependents) sch |\n",
      "|                                  | edule_policy model_configuration system_ |\n",
      "|                                  | shared_memory cuda_shared_memory binary_ |\n",
      "|                                  | tensor_data statistics trace             |\n",
      "| model_repository_path[0]         | /hugectr/hugectr_inference_backend/hps_b |\n",
      "|                                  | ackend/samples/hps-triton-ensemble/model |\n",
      "|                                  | /                                        |\n",
      "| model_control_mode               | MODE_EXPLICIT                            |\n",
      "| startup_models_0                 | ensemble_model                           |\n",
      "| strict_model_config              | 0                                        |\n",
      "| rate_limit                       | OFF                                      |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                 |\n",
      "| cuda_memory_pool_byte_size{1}    | 67108864                                 |\n",
      "| cuda_memory_pool_byte_size{2}    | 67108864                                 |\n",
      "| cuda_memory_pool_byte_size{3}    | 67108864                                 |\n",
      "| response_cache_byte_size         | 0                                        |\n",
      "| min_supported_compute_capability | 6.0                                      |\n",
      "| strict_readiness                 | 1                                        |\n",
      "| exit_timeout                     | 30                                       |\n",
      "+----------------------------------+------------------------------------------+\n",
      "\n",
      "I1110 12:47:39.814378 3326 grpc_server.cc:4608] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I1110 12:47:39.814812 3326 http_server.cc:3312] Started HTTPService at 0.0.0.0:8000\n",
      "I1110 12:47:39.864614 3326 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002\n",
      "^C\n",
      "Signal (2) received.\n",
      "I1110 12:47:46.975269 3326 server.cc:260] Waiting for in-flight requests to complete.\n",
      "I1110 12:47:46.975304 3326 server.cc:276] Timeout 30: Found 0 model versions that have in-flight inferences\n",
      "I1110 12:47:46.975313 3326 model_repository_manager.cc:1230] unloading: hps_test:0\n",
      "I1110 12:47:46.975418 3326 model_repository_manager.cc:1230] unloading: ensemble_model:0\n",
      "I1110 12:47:46.975534 3326 model_repository_manager.cc:1230] unloading: dense_trt:0\n",
      "I1110 12:47:46.975611 3326 server.cc:291] All models are stopped, unloading models\n",
      "I1110 12:47:46.975642 3326 server.cc:298] Timeout 30: Found 3 live models and 0 in-flight non-inference requests\n",
      "I1110 12:47:46.975777 3326 model_repository_manager.cc:1335] successfully unloaded 'ensemble_model' version 0\n",
      "I1110 12:47:46.975777 3326 hps.cc:337] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I1110 12:47:46.975910 3326 tensorrt.cc:5625] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I1110 12:47:46.976918 3326 hps.cc:268] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I1110 12:47:46.977840 3326 tensorrt.cc:5625] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I1110 12:47:46.980440 3326 tensorrt.cc:5625] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I1110 12:47:46.982982 3326 tensorrt.cc:5625] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I1110 12:47:46.984826 3326 tensorrt.cc:5564] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I1110 12:47:46.984947 3326 model_repository_manager.cc:1335] successfully unloaded 'dense_trt' version 0\n",
      "I1110 12:47:46.985143 3326 model_state.cpp:112] ******Destorying Embedding Cache for model hps_test successfully\n",
      "I1110 12:47:46.985188 3326 model_repository_manager.cc:1335] successfully unloaded 'hps_test' version 0\n"
     ]
    }
   ],
   "source": [
    "!tritonserver --model-repository=/hugectr_backend/model/ --load-model=ensemble_model --model-control-mode=explicit  --allow-gpu-metrics=true --backend-config=hps,ps=/hugectr_backend/hps.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8f87df2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1', 'model_name': 'ensemble_model', 'model_version': '0', 'parameters': {'sequence_id': 0, 'sequence_start': False, 'sequence_end': False}, 'outputs': [{'name': 'DENSE_OUTPUT', 'datatype': 'FP32', 'shape': [32, 1], 'parameters': {'binary_data_size': 128}}]}\n",
      "Prediction Result:\n",
      "[[0.42981684]\n",
      " [0.4249443 ]\n",
      " [0.42793524]\n",
      " [0.4317903 ]\n",
      " [0.43148506]\n",
      " [0.4266228 ]\n",
      " [0.42696753]\n",
      " [0.42348012]\n",
      " [0.39775002]\n",
      " [0.432141  ]\n",
      " [0.43840668]\n",
      " [0.4015409 ]\n",
      " [0.43687832]\n",
      " [0.43275222]\n",
      " [0.4350475 ]\n",
      " [0.42040417]\n",
      " [0.4362736 ]\n",
      " [0.43107143]\n",
      " [0.43180412]\n",
      " [0.41212332]\n",
      " [0.40849888]\n",
      " [0.43725652]\n",
      " [0.42471766]\n",
      " [0.4306074 ]\n",
      " [0.41305465]\n",
      " [0.3939272 ]\n",
      " [0.43355086]\n",
      " [0.40327936]\n",
      " [0.42544615]\n",
      " [0.39968377]\n",
      " [0.4147291 ]\n",
      " [0.39775002]]\n"
     ]
    }
   ],
   "source": [
    "from tritonclient.utils import *\n",
    "import tritonclient.http  as httpclient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "model_name = 'ensemble_model'\n",
    "\n",
    "with httpclient.InferenceServerClient(\"localhost:8000\") as client:\n",
    "    embedding_columns = np.array(torch.randint(low=0, high=999, size=(1,32)).numpy().tolist(),dtype='int64')\n",
    "    row_ptrs = np.array([[32]],dtype='int32')\n",
    "\n",
    "    inputs = [\n",
    "        httpclient.InferInput(\"EMB_KEY\", embedding_columns.shape,\n",
    "                              np_to_triton_dtype(embedding_columns.dtype)),\n",
    "        httpclient.InferInput(\"EMB_N_KEY\", row_ptrs.shape,\n",
    "                              np_to_triton_dtype(row_ptrs.dtype)),\n",
    "\n",
    "    ]\n",
    "\n",
    "    inputs[0].set_data_from_numpy(embedding_columns)\n",
    "    inputs[1].set_data_from_numpy(row_ptrs)\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"DENSE_OUTPUT\")\n",
    "    ]\n",
    "\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"DENSE_OUTPUT\")\n",
    "    ]\n",
    "\n",
    "    response = client.infer(model_name,\n",
    "                            inputs,\n",
    "                            request_id=str(1),\n",
    "                            outputs=outputs)\n",
    "\n",
    "    result = response.get_response()\n",
    "    print(result)\n",
    "    print(\"Prediction Result:\")\n",
    "    print(response.as_numpy(\"DENSE_OUTPUT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41427cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
