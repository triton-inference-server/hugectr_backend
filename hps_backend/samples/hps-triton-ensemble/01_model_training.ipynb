{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ca5b0e9-8a5a-406d-8352-34a8b9c222a9",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Hierarchical Parameter Server (HPS) is a distributed recommendation inference framework, which combines a high-performance GPU embedding cache with an hierarchical storage architecture, to realize low-latency retrieval of embeddings for inference tasks. It is provided as a Python toolkit and can be easily integrated into the TensorFlow (TF) model graph.\n",
    "\n",
    "This tutorial will show you how to integrate HPS backend and Tensorflow backend via Triton ensemble mode. By leveraging HPS, trained Tensorflow DNN models with large embedding tables can be efficiently deployed through the Triton Inference Server. For more details about HPS, please refer to [HugeCTR Hierarchical Parameter Server (HPS)](https://nvidia-merlin.github.io/HugeCTR/master/hugectr_parameter_server.html#hugectr-hierarchical-parameter-server-database-backend).\n",
    "\n",
    "The **01_model_training.ipynb** will cover following tasks\n",
    "  * Generate mock datasets that meet the HPS input format\n",
    "  * Train native Tensorflow DNN model\n",
    "  * Separate the trained DNN model graph into two, embedding lookup and dense model graph\n",
    "  * Reconstruct the dense model graph\n",
    "  * Construct HPS lookup model, get DNN model weights and transfer to HPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3cbaaf-d9f9-40ed-83b0-44b8e56bed4a",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffdcbc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 03:26:59.182685: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-22 03:27:00.099052: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/compat/lib.real:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib:/repos/dist/lib:/opt/tritonserver/lib:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib:/repos/dist/lib:/usr/lib/jvm/default-java/lib:/usr/lib/jvm/default-java/lib/server:/usr/local/hugectr/lib\n",
      "2022-11-22 03:27:00.099209: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/compat/lib.real:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib:/repos/dist/lib:/opt/tritonserver/lib:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib:/repos/dist/lib:/usr/lib/jvm/default-java/lib:/usr/lib/jvm/default-java/lib/server:/usr/local/hugectr/lib\n",
      "2022-11-22 03:27:00.099219: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e51ced60-fd3d-402e-bc96-fb7c7259a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict()\n",
    "# define model training settings\n",
    "args[\"gpu_num\"] = 4                               # the number of available GPUs\n",
    "args[\"iter_num\"] = 10                             # the number of training iteration\n",
    "args[\"embed_vec_size\"] = 16                       # the dimension of embedding vectors\n",
    "args[\"global_batch_size\"] = 65536                 # the globally batchsize for all GPUs\n",
    "args[\"slot_num\"] = 3                              # the number of feature fields in this embedding layer\n",
    "args[\"max_vocabulary_size\"] = 30000\n",
    "args[\"vocabulary_range_per_slot\"] = [[0,10000],[10000,20000],[20000,30000]]\n",
    "# define model save path\n",
    "args[\"dense_model_path\"]          = \"naive_dnn_dense.model\"\n",
    "args[\"reshape_dense_model_path\"]  = \"naive_dnn_reshape_dense.model\"\n",
    "args[\"embedding_table_path\"]      = \"naive_dnn_sparse.model\"\n",
    "# define data type\n",
    "args[\"np_key_type\"]    = np.int64\n",
    "args[\"np_vector_type\"] = np.float32\n",
    "args[\"tf_key_type\"]    = tf.int64\n",
    "args[\"tf_vector_type\"] = tf.float32\n",
    "\n",
    "# GPU environment configuration for model training\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, range(args[\"gpu_num\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46808665-c08d-4c91-9adf-f16105058918",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "Generate mock datasets that meet the HPS input format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55c20ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_samples(num_samples, vocabulary_range_per_slot, key_dtype = args[\"np_key_type\"]):\n",
    "    \"\"\"\n",
    "    Data generator\n",
    "    \n",
    "    Returns a randomly generated set of values for keys and labels\n",
    "    \"\"\"\n",
    "    keys = list()\n",
    "    for vocab_range in vocabulary_range_per_slot:\n",
    "        keys_per_slot = np.random.randint(low=vocab_range[0], \n",
    "                                          high=vocab_range[1], \n",
    "                                          size=(num_samples, 1), \n",
    "                                          dtype=key_dtype)\n",
    "        keys.append(keys_per_slot)\n",
    "    keys = np.concatenate(np.array(keys), axis = 1)\n",
    "    labels = np.random.randint(low=0, high=2, size=(num_samples, 1))\n",
    "    return keys, labels\n",
    "\n",
    "def tf_dataset(keys, labels, batchsize):\n",
    "    \"\"\"\n",
    "    Slice tensor into batches\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((keys, labels))\n",
    "    dataset = dataset.batch(batchsize, drop_remainder=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b153bf-22ed-48df-baea-9915cac589f5",
   "metadata": {},
   "source": [
    "## Model construction, native TF DNN model for training\n",
    "We define the model graph for training with native TF layers, i.e., `tf.nn.embedding_lookup` and `tf.keras.layers.Dense`. Besides, the embedding weights are stored in `tf.Variable`. We can then train the model and extract the trained weights of the embedding table. As for the dense layers, they are saved as a separate model graph, which can be loaded directly during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c64f6260",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseModel(tf.keras.models.Model):\n",
    "    \"\"\"\n",
    "    Model with 1 input, 1 output and 3 fully-connected layer\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 init_tensors,\n",
    "                 slot_num,\n",
    "                 embed_vec_size,\n",
    "                 **kwargs):\n",
    "        super(DenseModel, self).__init__(**kwargs)\n",
    "        self.slot_num       = slot_num\n",
    "        self.embed_vec_size = embed_vec_size\n",
    "        self.init_tensors   = init_tensors\n",
    "        self.params         = tf.Variable(initial_value=tf.concat(self.init_tensors, axis=0))\n",
    "        \n",
    "        # define FC layers\n",
    "        self.fc_1 = tf.keras.layers.Dense(units=256, activation=None,\n",
    "                                                 kernel_initializer=\"ones\",\n",
    "                                                 bias_initializer=\"zeros\",\n",
    "                                                 name='fc_1')\n",
    "        self.fc_2 = tf.keras.layers.Dense(units=128, activation=None,\n",
    "                                                 kernel_initializer=\"ones\",\n",
    "                                                 bias_initializer=\"zeros\",\n",
    "                                                 name='fc_2')\n",
    "        self.fc_3 = tf.keras.layers.Dense(units=1, activation=None,\n",
    "                                                 kernel_initializer=\"ones\",\n",
    "                                                 bias_initializer=\"zeros\",\n",
    "                                                 name='fc_3')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embedding_vector = tf.nn.embedding_lookup(params=self.params, \n",
    "                                                  ids=inputs)\n",
    "        embedding_vector = tf.reshape(embedding_vector, \n",
    "                                      shape=[-1, self.slot_num * self.embed_vec_size])\n",
    "        fc1   = self.fc_1(embedding_vector)\n",
    "        fc2   = self.fc_2(fc1)\n",
    "        logit = self.fc_3(fc2)\n",
    "        return logit, embedding_vector\n",
    "\n",
    "    def summary(self):\n",
    "        inputs = tf.keras.Input(shape=(self.slot_num,), \n",
    "                                dtype=args[\"tf_key_type\"], \n",
    "                                name=\"input_dense\")\n",
    "        model  = tf.keras.models.Model(inputs=inputs, \n",
    "                                       outputs=self.call(inputs),\n",
    "                                       name='tf_model')\n",
    "        return model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c7e7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    \"\"\"\n",
    "    Define TF DNN model training process\n",
    "    \"\"\"\n",
    "    # initialize input tensor\n",
    "    init_tensors = np.ones(shape=[args[\"max_vocabulary_size\"], args[\"embed_vec_size\"]], \n",
    "                           dtype=args[\"np_vector_type\"])\n",
    "    \n",
    "    # model construction\n",
    "    model = DenseModel(init_tensors, args[\"slot_num\"], args[\"embed_vec_size\"])\n",
    "    model.summary()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    \n",
    "    # define model training steps\n",
    "    def _train_step(inputs, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logit, embedding_vector = model(inputs)\n",
    "            loss = loss_fn(labels, logit)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        return logit, embedding_vector, loss\n",
    "\n",
    "    # prepare dataset\n",
    "    keys, labels = generate_random_samples(args[\"global_batch_size\"] * args[\"iter_num\"], \n",
    "                                           args[\"vocabulary_range_per_slot\"],  \n",
    "                                           args[\"np_key_type\"])\n",
    "    dataset = tf_dataset(keys, labels, args[\"global_batch_size\"])\n",
    "    \n",
    "    # model training\n",
    "    for i, (id_tensors, labels) in enumerate(dataset):\n",
    "        _, embedding_vector, loss = _train_step(id_tensors, labels)\n",
    "        print(\"-\"*20, \"Step {}, loss: {}\".format(i, loss),  \"-\"*20)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b651759a-bcf9-4a00-a580-936db6625df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.compat.v1.nn.embedding_lookup), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(30000, 16) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "Model: \"tf_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_dense (InputLayer)    [(None, 3)]               0         \n",
      "                                                                 \n",
      " tf.compat.v1.nn.embedding_l  (None, 3, 16)            0         \n",
      " ookup (TFOpLambda)                                              \n",
      "                                                                 \n",
      " tf.reshape (TFOpLambda)     (None, 48)                0         \n",
      "                                                                 \n",
      " fc_1 (Dense)                (None, 256)               12544     \n",
      "                                                                 \n",
      " fc_2 (Dense)                (None, 128)               32896     \n",
      "                                                                 \n",
      " fc_3 (Dense)                (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45,569\n",
      "Trainable params: 45,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 03:27:01.518012: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-22 03:27:03.638472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30970 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2022-11-22 03:27:03.639636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30970 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:07:00.0, compute capability: 7.0\n",
      "2022-11-22 03:27:03.640571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 30970 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:0a:00.0, compute capability: 7.0\n",
      "2022-11-22 03:27:03.641476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30970 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:0b:00.0, compute capability: 7.0\n",
      "2022-11-22 03:27:04.888010: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x4f86a1a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-11-22 03:27:04.888045: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2022-11-22 03:27:04.888068: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2022-11-22 03:27:04.888072: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (2): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2022-11-22 03:27:04.888077: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (3): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2022-11-22 03:27:04.893308: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-11-22 03:27:05.018148: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fbdac28fee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fbdac28fee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "-------------------- Step 0, loss: 783168.0 --------------------\n",
      "-------------------- Step 1, loss: 518110.40625 --------------------\n",
      "-------------------- Step 2, loss: 327503.125 --------------------\n",
      "-------------------- Step 3, loss: 199153.59375 --------------------\n",
      "-------------------- Step 4, loss: 115212.203125 --------------------\n",
      "-------------------- Step 5, loss: 64676.38671875 --------------------\n",
      "-------------------- Step 6, loss: 34635.75390625 --------------------\n",
      "-------------------- Step 7, loss: 17304.05859375 --------------------\n",
      "-------------------- Step 8, loss: 8057.4736328125 --------------------\n",
      "-------------------- Step 9, loss: 3385.7255859375 --------------------\n"
     ]
    }
   ],
   "source": [
    "trained_model = train(args)\n",
    "\n",
    "# get trained model weights for HPS lookup\n",
    "weights_list  = trained_model.get_weights()\n",
    "embedding_weights = weights_list[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9f0026-5a4b-4905-9170-098d03301bb3",
   "metadata": {},
   "source": [
    "## Save dense model graph\n",
    "Separate the trained DNN model graph into two, embedding layer and dense model graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c99a56c-33e9-4ba8-aff6-57906a21ee91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_dense_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 48)]              0         \n",
      "                                                                 \n",
      " fc_1 (Dense)                (None, 256)               12544     \n",
      "                                                                 \n",
      " fc_2 (Dense)                (None, 128)               32896     \n",
      "                                                                 \n",
      " fc_3 (Dense)                (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45,569\n",
      "Trainable params: 45,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: naive_dnn_dense.model/assets\n"
     ]
    }
   ],
   "source": [
    "# save dense layers as a seperate model graph\n",
    "dense_model = tf.keras.models.Model(trained_model.get_layer(\"fc_1\").input, \n",
    "                                    trained_model.get_layer(\"fc_3\").output, \n",
    "                                    name='tf_dense_model')\n",
    "dense_model.summary()\n",
    "\n",
    "# saved dense model graph will be load directly in the Triton inference part\n",
    "dense_model.save(args[\"dense_model_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0975b3-07a2-4786-8942-f3c1061d82d2",
   "metadata": {},
   "source": [
    "## Reconstruct the dense model graph\n",
    "Add reshape layer to top of loaded dense model to meet input format of Triton Tensorflow backend. \n",
    "\n",
    "For a Triton input, a model that supports batching expects a batched input to have shape [batch-size], which means that the batch dimension fully describes the shape. For the inference API the equivalent shape [batch-size, 1] must be specified since each input must specify a non-empty dims [[link](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md#reshape)]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98d53b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeDenseModel(tf.keras.models.Model):\n",
    "    \"\"\"\n",
    "    Add a reshape layer on top of the loaded dense model,\n",
    "    to support Triton's tensorflow backend input format.\n",
    "    The output of HPS is 1-dimension [-1], while TF backend requires 2-dimensional input [batch_size, -1].\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 slot_num,\n",
    "                 embed_vec_size,\n",
    "                 dense_model_path,\n",
    "                 **kwargs):\n",
    "        super(ReshapeDenseModel, self).__init__(**kwargs)\n",
    "        self.slot_num = slot_num\n",
    "        self.embed_vec_size = embed_vec_size\n",
    "        self.dense_model = tf.keras.models.load_model(dense_model_path)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        reshaped_input_vector = tf.reshape(inputs, \n",
    "                                           shape=[-1, self.slot_num * self.embed_vec_size])\n",
    "        logit = self.dense_model(reshaped_input_vector)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3a299bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "reshape_dense_model = ReshapeDenseModel(args[\"slot_num\"], \n",
    "                                        args[\"embed_vec_size\"], \n",
    "                                        args[\"dense_model_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f8a29d5-40cb-4ed9-8ca2-7018198702f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For reshape dense model, input shape is:(96,) output shape is:(2, 1)\n"
     ]
    }
   ],
   "source": [
    "# test the new reshaped dense model\n",
    "input_test  = np.random.random((96)).astype(np.float32)\n",
    "output_test = reshape_dense_model(input_test)\n",
    "print(\"For reshape dense model, input shape is:{}\".format(input_test.shape), \n",
    "      \"output shape is:{}\".format(output_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7d6758f-8b44-4e2e-a564-78bf9b1cbc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: naive_dnn_reshape_dense.model/assets\n"
     ]
    }
   ],
   "source": [
    "# save reshaped dense model graph, \n",
    "reshape_dense_model.save(args[\"reshape_dense_model_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3808c5-9521-441f-ac7b-9c70365d16f2",
   "metadata": {},
   "source": [
    "## Construct HPS lookup model\n",
    "In order to leverage HPS to facilitate the embedding lookup part, we need to convert the model lookup part of the model into a [format supported by HPS](../../docs/architecture.md#hierarchical-parameter-server-input-format). \n",
    "In this case, `tf.nn.embedding_lookup` of native TF DNN model will be replaced by HPS. Embedding weights of dense models are reloaded and transferred to HPS (sparse models) in preparation for future inference deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51b189ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sparse_model(embeddings_weights, embedding_table_path, embedding_vec_size):\n",
    "    \"\"\"\n",
    "    Convert the lookup part of the model to a format supported by HPS (key-vector pair files),\n",
    "    the embedding weights of the trained dense model will be reloaded.\n",
    "    \n",
    "    Outputs(key-vector pair files) will be saved to defined sparse model path\n",
    "    \"\"\"\n",
    "    os.system(\"mkdir -p {}\".format(embedding_table_path))\n",
    "    \n",
    "    with open(\"{}/key\".format(embedding_table_path), 'wb') as key_file, \\\n",
    "        open(\"{}/emb_vector\".format(embedding_table_path), 'wb') as vec_file:\n",
    "        for key in range(embeddings_weights.shape[0]):\n",
    "            vec = embeddings_weights[key]\n",
    "            key_struct = struct.pack('q', key)\n",
    "            vec_struct = struct.pack(str(embedding_vec_size) + \"f\", *vec)\n",
    "            key_file.write(key_struct)\n",
    "            vec_file.write(vec_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "540ec43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_sparse_model(embedding_weights, \n",
    "                        args[\"embedding_table_path\"], \n",
    "                        args[\"embed_vec_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f869b6ab",
   "metadata": {},
   "source": [
    "## Convert dense model graph to ONNX\n",
    "We convert the dense TF SavedModel to ONNX, which will be used to build the TensorRT engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ced7a48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "loading onnx\n",
      "onnx model checked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 03:27:10.445160: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 4\n",
      "2022-11-22 03:27:10.445278: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2022-11-22 03:27:10.456497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30970 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2022-11-22 03:27:10.457164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30970 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:07:00.0, compute capability: 7.0\n",
      "2022-11-22 03:27:10.457812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 30970 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:0a:00.0, compute capability: 7.0\n",
      "2022-11-22 03:27:10.458465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30970 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:0b:00.0, compute capability: 7.0\n",
      "2022-11-22 03:27:10.547380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30970 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2022-11-22 03:27:10.548312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30970 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:07:00.0, compute capability: 7.0\n",
      "2022-11-22 03:27:10.549260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 30970 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:0a:00.0, compute capability: 7.0\n",
      "2022-11-22 03:27:10.550197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30970 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:0b:00.0, compute capability: 7.0\n",
      "2022-11-22 03:27:10.565411: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 4\n",
      "2022-11-22 03:27:10.565505: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2022-11-22 03:27:10.576447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30970 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2022-11-22 03:27:10.577100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30970 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:07:00.0, compute capability: 7.0\n",
      "2022-11-22 03:27:10.577736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 30970 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:0a:00.0, compute capability: 7.0\n",
      "2022-11-22 03:27:10.578563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30970 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:0b:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tf2onnx\n",
    "import onnx\n",
    "from tf2onnx import utils\n",
    "from tf2onnx.handler import tf_op\n",
    "\n",
    "BZ = 1024\n",
    "TF_MODEL_PATH = \"naive_dnn_dense.model\"\n",
    "ONNX_MODEL_PATH = \"naive_dnn_dense.onnx\"\n",
    "\n",
    "model = tf.keras.models.load_model(TF_MODEL_PATH)\n",
    "\n",
    "spec = [tf.TensorSpec(shape=(BZ, 3 * 16), dtype=tf.float32, name=\"input_1\")]\n",
    "\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=10)\n",
    "\n",
    "with open(ONNX_MODEL_PATH, \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(\"loading onnx\")\n",
    "onnx_model = onnx.load(ONNX_MODEL_PATH)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"onnx model checked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "699d244e",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Release the occupied GPU memory by TensorFlow and Keras\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960c8599",
   "metadata": {},
   "source": [
    "After finishing this notebook, please launch the container `nvcr.io/nvidia/ternsorrt:22.11-py3` to build the TensorRT engine from the ONNX model using the following command:\n",
    "\n",
    "```bash\n",
    "trtexec --onnx=naive_dnn_dense.onnx --saveEngine=naive_dnn_dense.trt\n",
    "```\n",
    "\n",
    "Then you can continue the notebooks [02_model_inference_hps_tf_ensemble.ipynb](02_model_inference_hps_tf_ensemble.ipynb) and [03_model_inference_hps_trt_ensemble.ipynb](02_model_inference_hps_trt_ensemble.ipynb) within the container `nvcr.io/nvidia/merlin/merlin-hugectr:22.11`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
